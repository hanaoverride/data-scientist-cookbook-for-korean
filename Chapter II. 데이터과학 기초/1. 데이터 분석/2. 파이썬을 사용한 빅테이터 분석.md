# 2. 데이터 분석의 첫걸음: Pandas로 시작하기

## 목차
- [2. 데이터 분석의 첫걸음: Pandas로 시작하기](#2-데이터-분석의-첫걸음-pandas로-시작하기)
  - [목차](#목차)
  - [1. 데이터 분석의 전체 그림](#1-데이터-분석의-전체-그림)
  - [2. Pandas: 데이터 분석가의 스위스 아미 나이프](#2-pandas-데이터-분석가의-스위스-아미-나이프)
    - [Series와 DataFrame: Pandas의 두 가지 핵심 구조](#series와-dataframe-pandas의-두-가지-핵심-구조)
  - [3. 데이터 정제: 가장 중요하지만 가장 힘든 일](#3-데이터-정제-가장-중요하지만-가장-힘든-일)
    - [결측치(Missing Data) 처리](#결측치missing-data-처리)
    - [이상치(Outlier) 처리: 올바른 방법](#이상치outlier-처리-올바른-방법)
  - [4. 데이터 탐색: 질문을 던지는 기술](#4-데이터-탐색-질문을-던지는-기술)
    - [데이터 선택과 필터링: `loc`, `iloc`, 그리고 불리언 인덱싱](#데이터-선택과-필터링-loc-iloc-그리고-불리언-인덱싱)
    - [정렬과 순위 매기기](#정렬과-순위-매기기)
    - [**분석의 꽃: `groupby`를 활용한 집계**](#분석의-꽃-groupby를-활용한-집계)

---

## 1. 데이터 분석의 전체 그림

데이터 분석은 정해진 프로세스를 따를 때 가장 효율적입니다.

1.  **문제 정의 (Question)** : 해결하려는 비즈니스 문제가 무엇인가? 어떤 질문에 답을 찾아야 하는가?
2.  **데이터 수집 (Gathering)** : 질문에 답하기 위해 필요한 데이터를 모은다. (API, DB 쿼리, 파일 등)
3.  **데이터 정제 (Wrangling/Cleaning)** : 데이터의 오류(결측치, 이상치)를 바로잡고 분석하기 좋은 형태로 가공한다. **분석 프로젝트 시간의 80%가 이 단계에 소요됩니다.**
4.  **탐색적 데이터 분석 (EDA)** : 데이터를 여러 관점에서 시각화하고 요약하며 패턴과 인사이트를 발견한다.
5.  **모델링 및 해석 (Modeling & Interpretation)** : 통계 모델이나 머신러닝을 사용해 예측하거나, 원인을 분석한다.
6.  **결과 공유 (Communication)** : 분석 결과를 이해관계자가 이해하기 쉽게 보고서나 대시보드로 전달한다.

`Pandas`는 이 과정에서 **3번(정제)** 과 **4번(탐색)**  단계의 핵심 도구입니다.

## 2. Pandas: 데이터 분석가의 스위스 아미 나이프

`pandas`는 파이썬으로 테이블 형태의 데이터를 쉽고 빠르게 다룰 수 있게 해주는 라이브러리입니다. NumPy를 기반으로 만들어져 강력한 성능을 자랑합니다.

### Series와 DataFrame: Pandas의 두 가지 핵심 구조

- **`Series`**: 인덱스(label)를 가진 1차원 배열. 하나의 열(column)이라고 생각하면 쉽습니다.
- **`DataFrame`**: 여러 개의 `Series`가 모인 2차원 테이블. 우리가 흔히 보는 엑셀 시트 같은 행과 열 구조입니다.

```python
import pandas as pd
import numpy as np

# DataFrame 생성: 딕셔너리를 활용하는 것이 가장 일반적입니다.
student_data = {
    '이름': ['철수', '영희', '민수', '지현'],
    '전공': ['경영', '컴공', '경영', '경제'],
    '나이': [22, 25, 22, 24],
    '학점': [3.5, 4.3, 3.8, 4.1]
}
df = pd.DataFrame(student_data)

print("학생 데이터프레임:")
print(df)
```

## 3. 데이터 정제: 가장 중요하지만 가장 힘든 일

"Garbage in, garbage out." (쓰레기를 넣으면, 쓰레기가 나온다.) 지저분한 데이터로 분석하면 잘못된 결론을 내릴 수밖에 없습니다.

### 결측치(Missing Data) 처리

데이터에 값이 비어있는 경우(`NaN`)를 결측치라고 합니다.

```python
# 일부러 결측치를 만들어 봅시다.
df_with_nan = df.copy()
df_with_nan.loc[1, '학점'] = np.nan
df_with_nan.loc[3, '나이'] = np.nan

print("\n결측치가 포함된 데이터프레임:")
print(df_with_nan)

# 결측치 확인
print("\n컬럼별 결측치 개수:")
print(df_with_nan.isnull().sum())

# 결측치 처리 방법
# 1. 결측치가 있는 행 전체를 제거 (데이터가 충분할 때)
df_dropped = df_with_nan.dropna()

# 2. 특정 값으로 채우기 (가장 흔한 방법)
# 나이는 평균으로, 학점은 0으로 채워보겠습니다.
age_mean = df_with_nan['나이'].mean()
df_filled = df_with_nan.fillna({'나이': age_mean, '학점': 0})
print("\n결측치 처리 후 데이터프레임:")
print(df_filled)
```

> **팁**:
> 결측치를 어떤 값으로 채울지는 분석 결과에 큰 영향을 미칩니다. 왜 평균값을 썼는지, 왜 0을 썼는지, 그 근거를 항상 설명할 수 있어야 합니다.

### 이상치(Outlier) 처리: 올바른 방법

**이상치**는 다른 데이터들과 동떨어진 극단적인 값입니다. 임의의 숫자로 판단하는 것은 주관적이며 위험합니다. 통계적인 방법을 사용해야 합니다. 대표적으로 **IQR(사분위수 범위) 규칙**이 있습니다.

- IQR = 3사분위수(Q3) - 1사분위수(Q1)
- 정상 범위: `Q1 - 1.5 * IQR` ~ `Q3 + 1.5 * IQR`

```python
# 이상치가 포함된 데이터 생성
scores = pd.Series([88, 92, 85, 95, 91, 1, 89, 93, 87])

# IQR 계산
Q1 = scores.quantile(0.25)
Q3 = scores.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"정상 데이터 범위: {lower_bound:.2f} ~ {upper_bound:.2f}")

# 이상치 필터링
outliers = scores[(scores < lower_bound) | (scores > upper_bound)]
print(f"발견된 이상치: \n{outliers}")
```

> **실무 관점**:
> 이상치를 발견하면 무조건 삭제해선 안 됩니다. 시스템 오류인지, 아니면 실제로 발생 가능한 극단적인 사례(예: VIP 고객의 구매액)인지 반드시 확인해야 합니다.

## 4. 데이터 탐색: 질문을 던지는 기술

깨끗해진 데이터를 여러 각도에서 살펴보며 질문에 대한 답을 찾아가는 과정입니다.

### 데이터 선택과 필터링: `loc`, `iloc`, 그리고 불리언 인덱싱

- `loc`: **라벨(이름)**  기반 인덱싱 (예: `df.loc[0, '이름']`)
- `iloc`: **위치(정수)**  기반 인덱싱 (예: `df.iloc[0, 0]`)
- **불리언 인덱싱**: **조건**을 사용한 필터링 (가장 많이 사용!)

```python
# 조건부 필터링 (불리언 인덱싱)
# 1. 경영학과 학생만 선택
major_cs = df[df['전공'] == '경영']
print("\n경영학과 학생:")
print(major_cs)

# 2. 23세 이상이면서 학점이 4.0 이상인 학생 선택
high_achievers = df[(df['나이'] >= 23) & (df['학점'] >= 4.0)]
print("\n우수 학생:")
print(high_achievers)
```

### 정렬과 순위 매기기

```python
# 학점(score)을 기준으로 내림차순 정렬
df_sorted = df.sort_values(by='학점', ascending=False)
print("\n학점순 정렬:")
print(df_sorted)
```

### **분석의 꽃: `groupby`를 활용한 집계**

`groupby`는 특정 컬럼을 기준으로 데이터를 그룹으로 묶고, 각 그룹에 대해 집계 함수(합계, 평균, 개수 등)를 적용하는, Pandas에서 가장 강력하고 중요한 기능입니다.

```python
# 전공별로 그룹을 묶어 분석해봅시다.
# 1. 전공별 학생 수는?
major_counts = df.groupby('전공')['이름'].count()
print("\n전공별 학생 수:")
print(major_counts)

# 2. 전공별 평균 나이와 평균 학점은?
major_stats = df.groupby('전공').agg({
    '나이': 'mean',
    '학점': 'mean'
}).round(2) # 소수점 2자리로 반올림

print("\n전공별 평균 스탯:")
print(major_stats)
```

> **팁**:
> `groupby`를 자유자재로 사용할 수 있다면, 당신은 주니어 데이터 분석가의 수준을 넘어선 것입니다. '어떤 기준으로 묶어서(groupby)', '무엇을 보고 싶은지(aggregation)'를 계속 생각하는 습관을 들이세요.