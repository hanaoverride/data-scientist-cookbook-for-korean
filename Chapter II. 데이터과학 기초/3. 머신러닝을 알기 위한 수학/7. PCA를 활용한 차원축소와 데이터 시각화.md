# 7. 주성분 분석(PCA): 데이터의 본질을 꿰뚫는 차원 축소

## 목차
- [7. 주성분 분석(PCA): 데이터의 본질을 꿰뚫는 차원 축소](#7-주성분-분석pca-데이터의-본질을-꿰뚫는-차원-축소)
  - [목차](#목차)
  - [1. PCA의 목표: 정보 손실을 최소화하며 차원 줄이기](#1-pca의-목표-정보-손실을-최소화하며-차원-줄이기)
  - [2. PCA의 핵심 아이디어: 분산이 가장 큰 방향을 찾아라](#2-pca의-핵심-아이디어-분산이-가장-큰-방향을-찾아라)
  - [3. 어떻게 그 방향을 찾는가?: 공분산 행렬과 고유값 분해](#3-어떻게-그-방향을-찾는가-공분산-행렬과-고유값-분해)
    - [PCA 알고리즘 요약](#pca-알고리즘-요약)
  - [4. 실전 예제: PCA로 3차원 데이터를 2차원 평면에 시각화하기](#4-실전-예제-pca로-3차원-데이터를-2차원-평면에-시각화하기)
  - [5. PCA의 또 다른 얼굴: Eigenface](#5-pca의-또-다른-얼굴-eigenface)

---

## 1. PCA의 목표: 정보 손실을 최소화하며 차원 줄이기

상상해봅시다. 3차원 공간에 흩뿌려진 데이터 포인트들이 있습니다. 이 데이터들을 정보 손실을 최소화하면서 2차원 평면에 표현하려면 어떻게 해야 할까요? 마치 여러 각도에서 그림자를 비춰볼 때, 데이터 포인트들이 가장 잘 구분되도록(그림자가 가장 넓게 퍼지도록) 평면을 위치시키는 것과 같습니다.

PCA의 목표는 바로 이것입니다: **데이터의 분산(Variance)을 최대한 보존하는 새로운 축(주성분)들을 찾아, 원본 데이터를 이 새로운 축에 투영(projection)하여 차원을 축소하는 것.**

## 2. PCA의 핵심 아이디어: 분산이 가장 큰 방향을 찾아라

데이터의 '정보'는 '변화' 또는 '차이'에 있습니다. 모든 데이터 포인트가 같은 값을 가진다면 아무 정보도 없습니다. 데이터 과학에서 이러한 변화량을 측정하는 가장 대표적인 지표가 바로 **분산(Variance)**  입니다.

PCA는 데이터가 **가장 넓게 퍼져있는 방향**, 즉 **분산이 가장 큰 방향**을 첫 번째 **주성분(Principal Component, PC1)**  으로 찾습니다. 그 다음, 첫 번째 주성분과 수직인 방향 중에서 분산이 가장 큰 방향을 두 번째 주성분(PC2)으로 찾습니다. 이 과정을 반복하여 새로운 좌표축들을 만듭니다.

이 새로운 좌표축들은 데이터의 정보를 가장 효율적으로 요약합니다. 단 몇 개의 주성분만으로도 원본 데이터의 분산(정보) 대부분을 설명할 수 있게 됩니다.

## 3. 어떻게 그 방향을 찾는가?: 공분산 행렬과 고유값 분해

이 마법 같은 '방향'은 어떻게 찾을까요? 놀랍게도, 그 해답은 **공분산 행렬(Covariance Matrix)**  의 **고유값 분해**에 있습니다.

- **공분산 행렬**: 데이터의 각 특성들이 서로 어떻게 함께 변하는지를 나타내는 행렬입니다. 대각선에는 각 특성의 분산이, 그 외에는 특성 간의 공분산이 위치합니다. 이 행렬은 데이터의 전체적인 '형태'와 '방향성'에 대한 정보를 담고 있습니다.

- **공분산 행렬의 고유값 분해**:
  - **고유벡터(Eigenvectors)** : 공분산 행렬의 고유벡터는 바로 **데이터의 분산이 가장 큰 방향**, 즉 **주성분**을 나타냅니다.
  - **고유값(Eigenvalues)** : 각 고유벡터에 해당하는 고유값은 그 **주성분 방향으로의 분산의 크기**, 즉 해당 **주성분의 중요도**를 나타냅니다.

> **핵심**: PCA는 결국 **데이터의 공분산 행렬을 고유값 분해하여, 가장 중요한 축(고유값이 가장 큰 고유벡터)을 찾는 과정**입니다.

### PCA 알고리즘 요약
1.  **데이터 표준화**: 각 특성의 스케일이 다르므로, 평균을 0, 분산을 1로 맞춥니다. (매우 중요!)
2.  **공분산 행렬 계산**: 표준화된 데이터의 공분산 행렬을 구합니다.
3.  **고유값 분해**: 공분산 행렬의 고유값과 고유벡터를 계산합니다.
4.  **주성분 선택**: 고유값이 큰 순서대로 원하는 개수(`k`)의 고유벡터(주성분)를 선택합니다.
5.  **데이터 변환**: 원본 데이터를 선택된 `k`개의 주성분 축으로 투영하여 `k`차원의 새로운 데이터를 얻습니다.

## 4. 실전 예제: PCA로 3차원 데이터를 2차원 평면에 시각화하기

Scikit-learn의 `PCA`를 사용하면 이 모든 과정을 단 몇 줄의 코드로 수행할 수 있습니다. 붓꽃(iris) 데이터의 4개 특성 중 3개를 사용하여 3D 공간에 시각화하고, 이를 PCA를 통해 2D로 축소해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. 데이터 로드 및 표준화
iris = load_iris()
X = iris.data
y = iris.target
X_scaled = StandardScaler().fit_transform(X)

# 2. PCA 적용 (2개의 주성분 선택)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 3. 시각화
plt.figure(figsize=(8, 6))
colors = ['navy', 'turquoise', 'darkorange']
lw = 2

for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=lw,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

print(f"설명된 분산의 비율: {pca.explained_variance_ratio_}")
print(f"두 주성분으로 설명되는 총 분산: {sum(pca.explained_variance_ratio_):.2f}")
```
> **인사이트**: 4차원의 복잡한 데이터가 단 2개의 주성분(2차원 평면)만으로도 품종별로 잘 구분되는 것을 볼 수 있습니다. 또한, `explained_variance_ratio_`를 통해 두 개의 주성분이 원본 데이터의 분산(정보)을 약 96%나 설명하고 있음을 알 수 있습니다.

## 5. PCA의 또 다른 얼굴: Eigenface

PCA는 영상 인식 분야에서도 **Eigenface**라는 이름으로 활약합니다. 수많은 얼굴 이미지를 PCA로 분석하면, 얼굴을 구성하는 주요한 특징(눈, 코, 입의 형태 등)들이 주성분(고유벡터)으로 추출됩니다. 이 주성분 벡터들을 시각화하면 사람 얼굴처럼 보이기 때문에 'Eigenface'라고 부릅니다.

새로운 얼굴 이미지가 들어왔을 때, 이 Eigenface들에 얼마나 가깝게 표현되는지를 계산하여 누구의 얼굴인지 인식할 수 있습니다. 이는 고차원의 이미지 데이터를 저차원의 '얼굴 특징 공간'으로 압축하여 효율적으로 처리하는 것입니다.