# 5. 고유값과 고유벡터: 행렬의 숨겨진 축과 힘을 찾아서

## 목차
- [5. 고유값과 고유벡터: 행렬의 숨겨진 축과 힘을 찾아서](#5-고유값과-고유벡터-행렬의-숨겨진-축과-힘을-찾아서)
  - [목차](#목차)
  - [1. 고유값과 고유벡터의 직관적 의미](#1-고유값과-고유벡터의-직관적-의미)
    - [고유벡터: 변환에도 방향이 변하지 않는 '축'](#고유벡터-변환에도-방향이-변하지-않는-축)
    - [고유값: 그 '축' 방향의 영향력 (스케일)](#고유값-그-축-방향의-영향력-스케일)
  - [2. 머신러닝에서의 활용: 주성분 분석(PCA)의 핵심 원리](#2-머신러닝에서의-활용-주성분-분석pca의-핵심-원리)
  - [3. 행렬의 대각화: 행렬을 본질적인 요소로 분해하기](#3-행렬의-대각화-행렬을-본질적인-요소로-분해하기)
    - [대각화의 의미: 복잡한 변환을 '회전 + 스케일링 + 역회전'으로 분해](#대각화의-의미-복잡한-변환을-회전--스케일링--역회전으로-분해)
  - [4. 대칭 행렬과 고유값 분해](#4-대칭-행렬과-고유값-분해)

---

## 1. 고유값과 고유벡터의 직관적 의미

어떤 행렬 `A`가 선형변환(공간을 바꾸는 함수)을 의미할 때, 대부분의 벡터는 이 변환을 거치면서 방향과 크기가 모두 바뀝니다. 하지만 그 중 아주 특별한 벡터들이 있습니다.

### 고유벡터: 변환에도 방향이 변하지 않는 '축'

**고유벡터(Eigenvector)**  는 행렬 `A`로 선형변환을 해도, 그 **방향이 변하지 않는** 벡터입니다. 변환 후에도 원래 벡터와 평행하게 같은 직선 위에 놓여있습니다. 이 벡터들은 해당 변환의 **'고유한 축(axis)'** 역할을 합니다.

### 고유값: 그 '축' 방향의 영향력 (스케일)

**고유값(Eigenvalue)**  은 바로 그 고유벡터가 변환될 때, **크기가 얼마나 변하는지를 나타내는 스케일 값**입니다.
- 고유값이 2라면, 해당 고유벡터는 변환 후 방향은 그대로이고 크기만 2배가 됩니다.
- 고유값이 0.5라면, 크기가 절반으로 줄어듭니다.
- 고유값이 음수라면, 방향이 정반대로 바뀝니다.

수식으로는 `Av = λv` 로 표현됩니다.
- `v`: 고유벡터 (방향이 변하지 않는 축)
- `λ`: 고유값 (그 축 방향으로의 스케일 값)
- `A`: 선형변환 행렬

```python
import numpy as np
import matplotlib.pyplot as plt

# 변환 행렬 A
A = np.array([
    [3, 1],
    [1, 3]
])

# A의 고유값과 고유벡터 계산
eigenvalues, eigenvectors = np.linalg.eig(A)
v1 = eigenvectors[:, 0] # 첫 번째 고유벡터
lambda1 = eigenvalues[0] # 첫 번째 고유값

# 고유벡터 변환 전/후 시각화
v1_transformed = A @ v1

plt.figure(figsize=(6,6))
# 원본 고유벡터 (파란색)
plt.arrow(0, 0, v1[0], v1[1], head_width=0.1, head_length=0.1, fc='blue', ec='blue', label='Eigenvector')
# 변환된 고유벡터 (빨간색)
plt.arrow(0, 0, v1_transformed[0], v1_transformed[1], head_width=0.1, head_length=0.1, fc='red', ec='red', label='Transformed (A @ v)')
plt.xlim(-1, 5)
plt.ylim(-1, 5)
plt.axhline(0, color='grey', lw=0.5)
plt.axvline(0, color='grey', lw=0.5)
plt.grid()
plt.legend()
plt.title("고유벡터는 변환 후에도 방향이 변하지 않는다")
plt.show()

print(f"고유벡터 v1: {v1.round(2)}")
print(f"고유값 λ1: {lambda1.round(2)}")
print(f"변환된 벡터 Av1: {v1_transformed.round(2)}")
print(f"고유값*고유벡터 λ1*v1: {(lambda1 * v1).round(2)}")
print("Av1 과 λ1*v1 은 동일하다!")
```

## 2. 머신러닝에서의 활용: 주성분 분석(PCA)의 핵심 원리

고유값과 고유벡터가 가장 화려하게 사용되는 분야는 바로 **주성분 분석(PCA)**  입니다. PCA는 데이터의 **공분산 행렬(Covariance Matrix)**  이라는 특별한 행렬의 고유값과 고유벡터를 찾습니다.

- **공분산 행렬의 고유벡터**: 데이터가 **가장 넓게 퍼져있는(분산이 가장 큰) 방향**을 나타냅니다. 이 방향이 바로 데이터의 **주성분(Principal Component)**  입니다.
- **공분산 행렬의 고유값**: 해당 주성분 방향으로 데이터가 **얼마나 넓게 퍼져있는지(분산의 크기)** , 즉 그 **주성분의 중요도(설명력)**  를 나타냅니다.

PCA는 고유값이 큰 순서대로 몇 개의 주성분(고유벡터)만 선택하여, 데이터의 핵심 정보를 대부분 유지하면서 차원을 획기적으로 줄일 수 있게 해줍니다.

## 3. 행렬의 대각화: 행렬을 본질적인 요소로 분해하기

**대각화(Diagonalization)**  는 어떤 정방행렬 `A`를, 그 행렬의 고유벡터로 이루어진 행렬 `P`와 고유값으로 이루어진 대각행렬 `D`의 곱으로 분해하는 과정입니다: `A = PDP⁻¹`

- `P`: `A`의 고유벡터들을 열로 갖는 행렬
- `D`: `A`의 고유값들을 대각원소로 갖는 대각행렬
- `P⁻¹`: `P`의 역행렬

### 대각화의 의미: 복잡한 변환을 '회전 + 스케일링 + 역회전'으로 분해

대각화는 복잡한 선형변환 `A`를 세 단계의 단순한 변환으로 나누어 이해하는 과정입니다.
1.  **`P⁻¹` (좌표계 변환)** : 표준 좌표계의 데이터를, `A`의 고유벡터들을 축으로 하는 **새로운 좌표계**로 옮깁니다.
2.  **`D` (스케일링)** : 새로운 좌표계에서, 각 축(고유벡터 방향)으로 **고유값만큼 단순하게 스케일링**합니다.
3.  **`P` (원래 좌표계로 복귀)** : 스케일링된 데이터를 다시 원래의 표준 좌표계로 되돌립니다.

> **핵심**: 대각화는 복잡해 보이는 변환 `A`의 본질이, 결국 **'고유벡터 방향으로 고유값만큼 늘려주는' 단순한 작업**임을 보여줍니다. 이는 행렬의 거듭제곱(`Aⁿ = PDⁿP⁻¹`) 등을 매우 쉽게 계산할 수 있게 해줘, 마르코프 연쇄 같은 모델 분석에 매우 유용합니다.

## 4. 대칭 행렬과 고유값 분해

데이터 과학에서 자주 만나는 **대칭 행렬(Symmetric Matrix, `A = Aᵀ`)** , 특히 공분산 행렬은 매우 특별하고 좋은 성질을 가집니다.

- 고유값이 항상 **실수**입니다.
- 서로 다른 고유값에 대응하는 고유벡터들은 항상 **서로 직교(orthogonal)**  합니다.

이 덕분에 대칭 행렬은 항상 **직교 행렬 `Q`** 로 대각화할 수 있습니다: `A = QDQᵀ`
(직교 행렬은 `Q⁻¹ = Qᵀ` 이므로 계산이 훨씬 간편합니다.)

이것을 **고유값 분해(Eigen Decomposition)**  라고 부르며, 행렬을 그 본질인 '축(Q)'과 '영향력(D)'으로 완벽하게 분해하는 과정입니다.
