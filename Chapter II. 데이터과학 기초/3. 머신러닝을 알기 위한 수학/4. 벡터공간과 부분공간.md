# 4. 벡터공간과 부분공간 ✨

## 목차 📑

### 1. 벡터 개념의 시작
- [1.1 벡터의 정의와 기하학적 의미](#벡터의-정의와-기하학적-의미-) 🏹
- [1.2 벡터의 연산과 성질](#벡터의-연산과-성질-) ➕

### 2. 벡터공간과 Rⁿ
- [2.1 벡터공간의 개념](#벡터공간의-개념-) 🌌
- [2.2 부분공간의 개념](#부분공간의-개념-) 🏛️
- [2.3 Rⁿ의 정의와 연산](#rⁿ의-정의와-연산-) 🔢
- [2.4 벡터 표기법](#벡터-표기법-) ✍️

### 3. 벡터의 내적과 노름
- [3.1 내적과 벡터의 크기](#내적과-벡터의-크기-) 📏
- [3.2 거리와 노름의 정의](#거리와-노름의-정의-) 📐
- [3.3 내적의 성질과 연산법칙](#내적의-성질과-연산법칙-) ⚖️
- [3.4 주요 부등식과 정리](#주요-부등식과-정리-) 🧮
- [3.5 벡터 사이의 각과 수직성](#벡터-사이의-각과-수직성-) ⦜

---

## 벡터의 정의와 기하학적 의미 🏹

**설명**  
**벡터**는 크기와 방향을 동시에 가지는 수학적 객체로, 물리학에서는 속도, 힘, 가속도 등 다양한 현상을 표현하는 데 사용된다. 데이터 사이언스에서는 **데이터 포인트나 피처(feature)의 집합**을 나타내는 핵심 도구다. 예를 들어, 고객의 나이, 소득, 구매이력을 하나의 3차원 벡터로 표현할 수 있다.

벡터는 일반적으로 화살표로 시각화하며, 시작점과 끝점, 그리고 길이로 각각 위치와 크기를 나타낸다. 중요한 점은 시작점의 절대적 위치가 아니라 **변위(displacement)**  자체가 벡터의 본질이라는 것이다.

> **팁**  
> "실무에서 벡터는 단순한 숫자 배열이 아닙니다. 고객 한 명, 문서 하나, 이미지 한 장이 모두 고차원 벡터공간의 '점'으로 표현됩니다. 이 관점을 이해하면 데이터 간의 유사도, 클러스터링, 분류 문제를 기하학적으로 사고할 수 있게 됩니다. 벡터를 '데이터의 좌표'로 인식하는 것이 머신러닝 이해의 출발점입니다."

**예시**
```python
# 2차원 평면에서 벡터를 시각화하는 예시
import matplotlib.pyplot as plt

start = [0, 0]
vector = [3, 2]

plt.quiver(*start, *vector, angles='xy', scale_units='xy', scale=1, color='blue')
plt.xlim(-1, 5)
plt.ylim(-1, 4)
plt.grid()
plt.title("2차원 벡터 시각화")
plt.xlabel("Feature 1 (예: 나이)")
plt.ylabel("Feature 2 (예: 소득)")
plt.show()
```

**주의사항**  
벡터는 방향성을 가지므로, 동일한 크기라도 방향이 다르면 서로 다른 벡터로 간주한다. 평행이동으로 겹칠 수 있으면 같은 벡터로 본다.

---

## 벡터의 연산과 성질 ➕

**설명**  
벡터는 **덧셈**과 **스칼라 곱셈** 연산이 정의된다. 이 두 연산은 **선형 결합(Linear Combination)** 의 기초가 되며, 머신러닝의 거의 모든 알고리즘에서 핵심적인 역할을 한다. 두 벡터의 덧셈은 평행사변형 법칙 또는 꼬리-머리 연결법으로 시각화할 수 있다. 스칼라 곱셈은 벡터의 크기를 변화시키되 방향은 유지한다(음수면 반대 방향).

> **팁**  
> "선형 결합 `c₁v₁ + c₂v₂ + ...`은 머신러닝의 DNA입니다. 선형 회귀의 예측값, 뉴럴 네트워크의 각 레이어, PCA의 주성분 모두 벡터들의 선형 결합입니다. 이 기본 연산을 깊이 이해하면, 복잡해 보이는 알고리즘들이 결국 '가중합'의 변형임을 깨닫게 될 것입니다."

**예시**
```python
# 벡터 덧셈과 스칼라 곱셈 예시
import numpy as np

v1 = np.array([2, 1])
v2 = np.array([1, 3])
sum_vec = v1 + v2        # 벡터 덧셈
scaled_vec = 2 * v1      # 스칼라 곱셈

print("벡터 덧셈:", sum_vec)      # [3 4] - 두 고객 데이터의 평균적 특성
print("스칼라 곱셈:", scaled_vec) # [4 2] - 특성 값의 스케일링
```

**주의사항**  
벡터의 덧셈은 교환법칙과 결합법칙이 성립한다. 스칼라 곱셈은 실수와 벡터의 곱으로, 크기와 방향에 영향을 준다.

---

## 벡터공간의 개념 🌌

**설명**  
**벡터공간**은 벡터와 스칼라(실수) 사이의 두 연산(덧셈, 스칼라 곱셈)이 정의되고, 다음 8가지 공리를 만족하는 집합이다:

**벡터공간의 8가지 공리** (V에 속한 임의의 벡터 u, v, w와 스칼라 c, d에 대해):
1. **덧셈 교환법칙**: u + v = v + u
2. **덧셈 결합법칙**: (u + v) + w = u + (v + w)  
3. **덧셈 항등원**: u + **0** = u를 만족하는 영벡터 **0**이 존재
4. **덧셈 역원**: u + (-u) = **0**을 만족하는 -u가 존재
5. **스칼라 곱셈 분배법칙 (벡터)** : c(u + v) = cu + cv
6. **스칼라 곱셈 분배법칙 (스칼라)** : (c + d)u = cu + du
7. **스칼라 곱셈 결합법칙**: c(du) = (cd)u
8. **스칼라 곱셈 항등원**: 1u = u

> **팁**  
> "이 공리들 덕분에 우리가 아는 대수적 조작이 벡터에서도 성립합니다. Rⁿ뿐만 아니라 함수 공간, 확률 분포 공간도 벡터공간이 될 수 있습니다. 이 추상화 덕분에 이미지, 텍스트, 음성 등 모든 데이터를 '벡터'라는 통일된 프레임워크로 분석할 수 있습니다. 데이터 사이언스는 본질적으로 다양한 데이터를 벡터공간으로 임베딩하여 분석하는 학문입니다."

**예시**
```python
# 3차원 벡터공간에서 벡터 덧셈과 스칼라 곱셈
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 0, -1])
scalar = 3

sum_ab = a + b
scaled_a = scalar * a

print("벡터 덧셈:", sum_ab)      # [5 2 2]
print("스칼라 곱셈:", scaled_a)  # [3 6 9] - 특성 값의 증폭
```

**주의사항**  
벡터공간의 원소는 반드시 같은 차원을 가져야 하며, 연산 결과도 동일한 차원의 벡터가 되어야 한다.

---

## 부분공간의 개념 🏛️

**설명**
**부분공간(Subspace)** 은 더 큰 벡터공간 V의 부분집합이면서, 동시에 그 자체로도 벡터공간인 집합 H를 의미한다. 부분공간이 되기 위한 조건은 다음 세 가지다:

1. **영벡터 포함**: **0** ∈ H
2. **덧셈에 대해 닫힘**: u, v ∈ H ⟹ u + v ∈ H  
3. **스칼라 곱셈에 대해 닫힘**: u ∈ H, c ∈ ℝ ⟹ cu ∈ H

예를 들어, R³에서 원점을 지나는 평면이나 직선은 모두 R³의 부분공간이다.

> **팁**  
> "부분공간은 **차원 축소(Dimensionality Reduction)** 의 핵심 개념입니다. 고차원 데이터가 실제로는 저차원 부분공간 근처에 분포하는 경우가 많습니다. **PCA(주성분 분석)** 는 데이터를 가장 잘 설명하는 부분공간을 찾는 기법입니다. 또한 **선형 회귀**에서 예측값들이 이루는 공간도 피처 공간의 부분공간입니다. '차원의 저주'를 극복하는 핵심 아이디어가 바로 부분공간입니다."

**예시**
```python
# R³에서 xy-평면(z=0)이 부분공간임을 확인
import numpy as np

# H = {(x, y, 0) | x, y ∈ ℝ} (xy-평면)
v1 = np.array([1, 2, 0])  # H에 속함
v2 = np.array([3, -1, 0]) # H에 속함
zero = np.array([0, 0, 0]) # 영벡터, H에 속함

# 덧셈에 대해 닫혀있는지 확인
sum_v = v1 + v2
print(f"v1 + v2 = {sum_v}")  # [4 1 0], z=0이므로 H에 속함

# 스칼라 곱셈에 대해 닫혀있는지 확인  
scaled_v1 = 2.5 * v1
print(f"2.5 * v1 = {scaled_v1}")  # [2.5 5. 0.], z=0이므로 H에 속함
```

**주의사항**
원점을 지나지 않는 직선이나 평면은 영벡터를 포함하지 않으므로 부분공간이 될 수 없다.

---

## Rⁿ의 정의와 연산 🔢

**설명**  
**Rⁿ**은 n개의 실수로 이루어진 순서쌍(튜플)들의 집합이다. R²는 2차원 평면, R³는 3차원 공간을 의미하며, 일반적으로 Rⁿ은 n차원 벡터공간을 나타낸다. 데이터 사이언스에서는 n개의 피처를 가진 데이터를 표현하는 n차원 공간으로 해석된다.

> **팁**  
> "Rⁿ은 가장 직관적인 벡터공간이지만, 여기에만 매몰되면 안 됩니다. 벡터공간의 추상적 개념을 이해하면 함수 공간, 확률 분포 공간 등 더 복잡한 대상도 다룰 수 있습니다. 모든 것을 Rⁿ의 일반화로 생각하는 훈련을 하세요. 텍스트는 단어 공간의 벡터, 이미지는 픽셀 공간의 벡터로 표현됩니다."

**예시**
```python
# 4차원 벡터공간에서의 연산 예시
import numpy as np

x = np.array([1, 2, 3, 4])
y = np.array([0, -1, 2, 1])
alpha = 0.5

sum_xy = x + y
scaled_y = alpha * y

print("벡터 덧셈:", sum_xy)      # [1 1 5 5]
print("스칼라 곱셈:", scaled_y)  # [ 0.  -0.5  1.   0.5]
```

**주의사항**  
덧셈과 스칼라 곱셈은 각각 Rⁿ 내에서 닫혀 있어야 하며, 연산 결과가 항상 Rⁿ의 원소가 되어야 한다.

---

## 벡터 표기법 ✍️

**설명**  
벡터는 일반적으로 소문자 굵은체 또는 화살표로 표기한다. 행벡터는 (x₁, x₂, ..., xₙ) 형태, 열벡터는 세로로 나열된 형태로 나타낸다. 전치 연산(`.T` 또는 `⊤`)을 통해 행벡터와 열벡터를 변환할 수 있다.

> **팁**  
> "NumPy에서 1차원 배열은 행/열 구분이 모호합니다. `v.shape`가 `(n,)`로 나타나죠. 이는 편리하지만 행렬 연산에서 예상치 못한 오류를 일으킬 수 있습니다. `v.reshape(-1, 1)` (열벡터) 또는 `v.reshape(1, -1)` (행벡터)로 명시적으로 차원을 지정하는 습관을 들이세요. 코드의 안정성이 크게 향상됩니다."

**예시**
```python
import numpy as np

v = np.array([5, -2, 7])
v_row = v.reshape(1, -1)          # 명시적 행벡터
v_col = v.reshape(-1, 1)          # 명시적 열벡터

print("1D 배열:", v, "Shape:", v.shape)      # (3,)
print("행벡터:", v_row, "Shape:", v_row.shape) # (1, 3)  
print("열벡터:\n", v_col, "Shape:", v_col.shape) # (3, 1)
```

**주의사항**  
벡터의 표기 방식에 따라 연산 결과가 달라질 수 있으므로, 행/열 벡터 구분에 유의해야 한다.

---

## 내적과 벡터의 크기 📏

**설명**  
**내적(Inner product)** 은 두 벡터의 대응 원소를 곱한 뒤 모두 더하는 연산이다. 내적을 통해 벡터의 크기(노름)와 두 벡터 사이의 각도를 계산할 수 있다. 벡터의 크기는 자기 자신과의 내적의 제곱근으로 정의된다: `||v|| = sqrt(⟨v, v⟩)`.

> **팁**  
> "내적의 본질은 '투영(Projection)'과 '유사도(Similarity)'입니다. 추천 시스템에서 사용자 벡터와 아이템 벡터의 내적으로 선호도를 예측하고, 코사인 유사도는 내적을 정규화한 것입니다. Word2Vec도 단어 벡터 간의 내적으로 의미적 유사도를 측정합니다. 내적은 '얼마나 같은 방향인가?'를 수치화하는 도구입니다."

**예시**
```python
import numpy as np

u = np.array([2, -1, 4])
v = np.array([1, 3, 0])

dot_product = np.dot(u, v)      # 내적
norm_u = np.linalg.norm(u)      # 벡터의 크기

print("내적:", dot_product)     # 2*1 + (-1)*3 + 4*0 = -1 (음수 = 둔각)
print("벡터의 크기:", norm_u)   # sqrt(2^2 + (-1)^2 + 4^2) = sqrt(21)
```

**주의사항**  
내적은 두 벡터의 차원이 같을 때만 정의된다. 내적 결과가 0이면 두 벡터는 서로 수직이다.

---

## 거리와 노름의 정의 📐

**설명**  
**노름(Norm)** 은 벡터의 크기를 측정하는 함수이며, **거리(Metric)** 는 두 점(벡터) 사이의 간격을 나타낸다. 대표적인 노름으로 L₁, L₂, L∞ 노름이 있으며, L₂ 노름은 유클리드 거리와 동일하다.

- **L₁ 노름 (맨해튼)** : `||v||₁ = Σ|vᵢ|` - 변화량의 절대적 크기
- **L₂ 노름 (유클리드)** : `||v||₂ = sqrt(Σvᵢ²)` - 직선 거리  
- **L∞ 노름 (최대값)** : `||v||∞ = max|vᵢ|` - 가장 큰 성분

> **팁**  
> "노름은 **정규화(Regularization)** 의 핵심입니다. L₁ 정규화(Lasso)는 불필요한 피처를 0으로 만들어 **피처 선택** 효과를 가지고, L₂ 정규화(Ridge)는 가중치를 전반적으로 작게 만들어 **과적합을 방지**합니다. 거리 함수 선택도 중요합니다. 이미지에서는 L₂, 텍스트에서는 코사인 거리, 추천시스템에서는 맨해튼 거리를 자주 사용합니다."

**예시**
```python
import numpy as np

a = np.array([1, -2, 3])
b = np.array([4, 0, -1])

l1_norm = np.sum(np.abs(a))
l2_norm = np.linalg.norm(a)
linf_norm = np.max(np.abs(a))

euclidean_distance = np.linalg.norm(a - b)

print("L1 노름:", l1_norm)             # 6 - 특성 변화량의 총합
print("L2 노름:", l2_norm)             # sqrt(14) - 원점으로부터 거리
print("L∞ 노름:", linf_norm)           # 3 - 가장 극단적인 특성값
print("유클리드 거리:", euclidean_distance) # 두 데이터 포인트간 거리
```

**주의사항**  
노름과 거리는 유사하지만, 노름은 벡터의 크기, 거리는 두 벡터 사이의 간격을 의미한다. p값에 따라 다양한 거리/노름이 정의된다.

---

## 내적의 성질과 연산법칙 ⚖️

**설명**  
내적은 다음과 같은 성질을 가진다:
- **선형성**: ⟨a + b, c⟩ = ⟨a, c⟩ + ⟨b, c⟩
- **대칭성**: ⟨a, b⟩ = ⟨b, a⟩  
- **양의 정부호성**: ⟨a, a⟩ ≥ 0, ⟨a, a⟩ = 0 ⇔ a = 0

이러한 성질들로부터 피타고라스 정리, 평행사변형 법칙 등이 유도된다.

> **팁**  
> "선형성은 복잡한 계산을 간단한 요소로 분해할 수 있게 해주는 강력한 성질입니다. **커널 트릭(Kernel Trick)** 은 내적의 성질을 활용해 고차원 공간에서의 내적을 직접 계산하지 않고도 구할 수 있게 해줍니다. SVM의 핵심 아이디어죠. 기본 성질에 대한 깊은 이해가 고급 기법의 토대가 됩니다."

**예시**
```python
import numpy as np

x = np.array([2, 1])
y = np.array([-1, 3])

# 피타고라스 정리 검증
left = np.linalg.norm(x + y)** 2
right = np.linalg.norm(x)** 2 + np.linalg.norm(y)** 2 + 2 * np.dot(x, y)

print("좌변:", left)    # ||x+y||²
print("우변:", right)   # ||x||² + ||y||² + 2⟨x,y⟩ (평행사변형 법칙)
```

**주의사항**  
내적의 성질을 활용할 때, 벡터의 차원과 연산 순서에 주의해야 한다.

---

## 주요 부등식과 정리 🧮

**설명**  
- **Cauchy-Schwarz 부등식**: 임의의 벡터 v, w에 대해 `|⟨v, w⟩| ≤ ||v||·||w||`가 성립한다. 등호는 두 벡터가 일직선상에 있을 때만 성립한다.
- **삼각부등식**: `||v + w|| ≤ ||v|| + ||w||`가 항상 성립한다.
- **피타고라스 정리**: 두 벡터가 수직일 때, `||v + w||² = ||v||² + ||w||²`가 성립한다.

> **팁**  
> "Cauchy-Schwarz 부등식은 **상관계수가 -1과 1 사이에 있는 이유**를 설명합니다. 삼각부등식은 '직접 가는 것이 돌아가는 것보다 짧다'는 거리의 직관을 수학적으로 보장합니다. 이런 기본 부등식들이 머신러닝 알고리즘의 수렴성과 안정성을 증명하는 데 핵심적인 역할을 합니다."

**예시**
```python
import numpy as np

v = np.array([1, 2, 2])
w = np.array([2, -1, 0])

# Cauchy-Schwarz 부등식 검증
inner = np.abs(np.dot(v, w))
product_norms = np.linalg.norm(v) * np.linalg.norm(w)

print("내적 절댓값:", inner)
print("노름 곱:", product_norms)
print("부등식 성립:", inner <= product_norms)  # True
```

**주의사항**  
Cauchy-Schwarz 부등식은 벡터 공간에서 매우 중요한 불평등이며, 벡터의 각도 정의 등 다양한 곳에 활용된다.

---

## 벡터 사이의 각과 수직성 ⦜

**설명**  
두 벡터 사이의 **각도**는 내적을 이용해 정의된다:  
`cos θ = ⟨v, w⟩ / (||v||·||w||)`  
이때 θ는 [0, π] 범위의 유일한 값이다.  
특히, 내적이 0이면 두 벡터는 서로 **수직(직교)** 이다.

> **팁**  
> "수직성은 '독립성'의 기하학적 표현입니다. **PCA에서 주성분들은 서로 수직**이며, 이는 각 성분이 독립적인 분산 방향을 나타냄을 의미합니다. **선형 회귀의 정규방정식**도 오차벡터가 피처공간과 수직이 되는 지점을 찾는 것입니다. '수직'이 보이면 '최적화'나 '독립성'을 떠올리세요."

**예시**
```python
import numpy as np

a = np.array([1, 1])     # 45도 방향
b = np.array([-1, 1])    # 135도 방향 (90도 차이)

cos_theta = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
theta_rad = np.arccos(np.clip(cos_theta, -1, 1))  # 수치 오차 방지
theta_deg = np.degrees(theta_rad)

print("내적:", np.dot(a, b))      # 0 (수직)
print("cosθ:", cos_theta)        # 0
print("각도(도):", theta_deg)     # 90.0
```

**주의사항**  
벡터의 크기가 0인 경우 각도 계산이 불가능하므로, 반드시 두 벡터가 영벡터가 아닌지 확인해야 한다.

