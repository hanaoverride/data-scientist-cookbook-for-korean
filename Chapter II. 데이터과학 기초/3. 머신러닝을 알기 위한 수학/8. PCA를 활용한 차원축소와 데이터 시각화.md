# 8. PCA를 활용한 차원축소와 데이터 시각화

## 목차 📑

### 1. PCA 개념
- [1.1 PCA란 무엇인가?](#pca란-무엇인가-) 🧩
- [1.2 결합 확률과 공분산](#결합-확률과-공분산-) 📊

### 2. 주성분 분석 이론
- [2.1 분산과 공분산 행렬](#분산과-공분산-행렬-) 🧮
- [2.2 주성분 분석(PCA) 원리](#주성분-분석pca-원리-) 🔍
- [2.3 주성분 분석 알고리즘](#주성분-분석-알고리즘-) ⚙️

### 3. PCA 실습 및 예제
- [3.1 데이터 예시와 공분산 계산](#데이터-예시와-공분산-계산-) 📝
- [3.2 주성분 방향과 차원 축소](#주성분-방향과-차원-축소-) 📉

### 4. PCA 응용
- [4.1 Eigenface와 영상 인식](#eigenface와-영상-인식-) 🖼️

---

## PCA란 무엇인가? 🧩

**주성분 분석(PCA, Principal Component Analysis)** 는 고차원 데이터를 저차원 공간으로 변환하여 **차원 축소**와 **특징 추출**을 수행하는 대표적인 통계적 기법이다. PCA의 주요 목적은 데이터의 **분산(variance)** 을 최대한 보존하는 새로운 축(주성분)을 찾아, 원본 데이터의 복잡성을 줄이고 계산 효율성을 높이는 데 있다. 이 과정에서 **노이즈 제거**와 **중요 특징 추출**이 가능하며, 데이터 시각화와 특징 선택에도 활용된다.

PCA는 데이터의 분산이 가장 큰 방향을 따라 **주성분 벡터**를 정의하고, 이 벡터를 기준으로 데이터를 투영하여 차원을 축소한다. 이를 통해 데이터의 구조와 패턴을 더 명확하게 파악할 수 있다.

```python
# PCA를 통한 2차원 데이터의 1차원 축소 예시
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 임의의 2차원 데이터 생성
np.random.seed(1)
X = np.dot(np.random.rand(2, 2), np.random.randn(2, 100)).T

# PCA 적용 (1차원으로 축소)
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)

print("축소 전 데이터 shape:", X.shape)
print("축소 후 데이터 shape:", X_pca.shape)
```

**출력 예시**:
```
축소 전 데이터 shape: (100, 2)
축소 후 데이터 shape: (100, 1)
```

> **⚠️ 주의사항**: PCA는 데이터의 분산이 큰 방향을 우선적으로 선택하므로, 모든 경우에 중요한 정보가 보존된다고 단정할 수 없다. 도메인 지식과 함께 해석해야 한다.

---

## 결합 확률과 공분산 📊

PCA의 이론적 기반에는 **확률 변수**, **결합 확률 분포**, **기대값**, **분산** 및 **공분산** 개념이 포함된다. 여러 개의 특성(예: 키, 몸무게, 시험 점수 등)을 가진 데이터셋에서 각 특성은 확률 변수로 간주할 수 있다. 이들 변수 간의 **관계**와 **공동 분포**를 이해하는 것이 중요하다.

**결합 확률 분포**는 두 개 이상의 확률 변수가 동시에 특정 값을 가질 확률을 나타낸다. 예를 들어, 학생들의 키와 몸무게가 각각 특정 구간에 속할 확률을 결합하여 나타낼 수 있다.

**공분산**은 두 변수 간의 선형 관계의 방향과 강도를 나타내는 값이다. 공분산이 양수면 두 변수가 함께 증가하거나 감소하고, 음수면 한 변수가 증가할 때 다른 변수는 감소하는 경향이 있다.

```python
# 두 변수(키, 몸무게)의 공분산 계산 예시
import numpy as np

height = np.array([160, 165, 170, 175, 180])
weight = np.array([50, 55, 60, 65, 70])

cov_matrix = np.cov(height, weight)
print("공분산 행렬:\n", cov_matrix)
```

**출력 예시**:
```
공분산 행렬:
[[62.5 62.5]
 [62.5 62.5]]
```

> **주의사항**: 공분산 값은 단위에 따라 크기가 달라지므로, 변수의 표준화가 필요할 수 있다. 또한, 공분산만으로는 관계의 강도를 절대적으로 비교하기 어렵다.

---

## 분산과 공분산 행렬 🧮

**분산(Variance)** 은 한 변수의 값이 평균을 중심으로 얼마나 퍼져 있는지를 나타내는 척도이다. **공분산(Covariance)** 은 두 변수의 동시 변동성을 측정한다. 여러 변수의 분산과 공분산을 모두 포함하는 **공분산 행렬(Covariance Matrix)** 은 PCA에서 핵심적인 역할을 한다.

공분산 행렬은 대각선에는 각 변수의 분산, 비대각선에는 변수 간의 공분산이 위치한다. 이 행렬은 데이터의 분포와 변수 간 상관관계를 요약한다.

```python
# 3개 변수(키, 몸무게, 점수)의 공분산 행렬 계산 예시
import numpy as np

data = np.array([
    [172, 65, 3.8],
    [168, 59, 3.2],
    [175, 72, 3.5],
    [163, 54, 3.9],
    [170, 68, 3.1]
])

cov_matrix = np.cov(data, rowvar=False)
print("공분산 행렬:\n", cov_matrix)
```

**출력 예시**:
```
공분산 행렬:
[[ 22.3  25.2  -0.11]
 [ 25.2  34.7  -0.18]
 [ -0.11 -0.18  0.13]]
```

> **주의사항**: 공분산 행렬은 반드시 대칭 행렬이며, 데이터가 충분히 많지 않으면 신뢰도가 낮아질 수 있다.

---

## 주성분 분석(PCA) 원리 🔍

PCA는 공분산 행렬의 **고유값 분해** 또는 **특이값 분해(SVD)** 를 통해 주성분 방향을 찾는다. 각 주성분은 데이터의 분산을 최대한 설명하는 새로운 축이다. 첫 번째 주성분은 가장 큰 분산 방향, 두 번째 주성분은 그 다음으로 큰 분산 방향을 나타낸다.

주성분은 서로 직교(orthogonal)하며, 데이터의 차원을 축소할 때 상위 몇 개의 주성분만 선택하여 정보를 최대한 보존한다.

```python
# 3차원 데이터를 2차원으로 축소하는 PCA 예시
from sklearn.decomposition import PCA
import numpy as np

data = np.array([
    [172, 65, 3.8],
    [168, 59, 3.2],
    [175, 72, 3.5],
    [163, 54, 3.9],
    [170, 68, 3.1]
])

pca = PCA(n_components=2)
reduced = pca.fit_transform(data)
print("축소된 데이터 shape:", reduced.shape)
print("주성분 벡터:\n", pca.components_)
```

**출력 예시**:
```
축소된 데이터 shape: (5, 2)
주성분 벡터:
[[ 0.79  0.61  0.02]
 [ 0.05 -0.02  0.99]]
```

> **주의사항**: 주성분 벡터는 데이터의 스케일에 영향을 받으므로, PCA 적용 전 표준화가 필요하다.

---

## 주성분 분석 알고리즘 ⚙️

PCA의 기본 알고리즘은 다음과 같다.

1. **평균 중심화**: 각 변수별 평균을 계산하고, 데이터에서 평균을 뺀다.
2. **공분산 행렬 계산**: 중심화된 데이터의 공분산 행렬을 구한다.
3. **고유값 분해 또는 SVD**: 공분산 행렬을 고유값 분해하여 고유값과 고유벡터(또는 SVD로 특이값과 특이벡터)를 구한다.
4. **주성분 선택**: 고유값이 큰 순서대로 원하는 개수만큼의 주성분을 선택한다.
5. **데이터 투영**: 원본 데이터를 선택한 주성분 방향으로 투영하여 차원을 축소한다.

```python
# PCA 알고리즘 단계별 구현 예시
import numpy as np

# 임의의 데이터 생성
X = np.array([
    [2.5, 2.4],
    [0.5, 0.7],
    [2.2, 2.9],
    [1.9, 2.2],
    [3.1, 3.0]
])

# 1. 평균 중심화
mean_vec = np.mean(X, axis=0)
X_centered = X - mean_vec

# 2. 공분산 행렬 계산
cov_mat = np.cov(X_centered, rowvar=False)

# 3. 고유값 분해
eig_vals, eig_vecs = np.linalg.eigh(cov_mat)

# 4. 주성분 선택 (가장 큰 고유값의 벡터)
principal_vec = eig_vecs[:, np.argmax(eig_vals)]

# 5. 데이터 투영
X_reduced = X_centered @ principal_vec

print("투영된 데이터 shape:", X_reduced.shape)
```

**출력 예시**:
```
투영된 데이터 shape: (5,)
```

> **주의사항**: SVD를 활용하면 수치적으로 더 안정적인 결과를 얻을 수 있다. 고유값의 크기는 각 주성분이 설명하는 분산의 크기를 의미한다.

---

## 데이터 예시와 공분산 계산 📝

PCA를 적용하기 전에, 실제 데이터셋에서 각 변수의 평균, 분산, 공분산을 계산하는 과정이 필요하다. 예를 들어, 학생들의 키, 몸무게, 성적 데이터를 사용하여 각 변수의 통계량과 공분산 행렬을 구할 수 있다.

```python
import pandas as pd
import numpy as np

# 임의의 학생 데이터 생성
df = pd.DataFrame({
    'height': [170, 165, 174, 169, 155, 172, 166, 168],
    'weight': [60, 55, 75, 67, 49, 63, 58, 61],
    'score': [4.1, 3.0, 2.8, 2.9, 3.1, 3.6, 3.7, 4.0]
})

# 평균, 분산, 공분산 계산
means = df.mean()
variances = df.var()
cov_matrix = df.cov()

print("평균:\n", means)
print("\n분산:\n", variances)
print("\n공분산 행렬:\n", cov_matrix)
```

**출력 예시**:
```
평균:
height    167.375
weight     61.000
score       3.400
dtype: float64

분산:
height    33.696429
weight    60.857143
score      0.262857
dtype: float64

공분산 행렬:
         height    weight     score
height  33.696429  39.428571  0.371429
weight  39.428571  60.857143  0.942857
score    0.371429   0.942857  0.262857
```

> **주의사항**: 데이터의 단위가 다를 경우, 표준화(평균 0, 분산 1로 변환)를 먼저 수행하는 것이 바람직하다.

---

## 주성분 방향과 차원 축소 📉

PCA의 핵심은 데이터의 분산이 가장 큰 방향(주성분 방향)을 찾아, 데이터를 그 방향으로 투영하는 것이다. 예를 들어, 언어 성적(국어, 영어, 고전)을 하나의 **언어능력** 축으로 묶어 차원을 축소할 수 있다. 이 과정에서 원본 데이터의 특성을 최대한 보존하면서, 불필요한 정보(노이즈)는 줄일 수 있다.

```python
# 3차원 데이터를 1차원으로 축소하는 예시
from sklearn.decomposition import PCA
import numpy as np

data = np.array([
    [85, 90, 88],   # 국어, 영어, 고전
    [78, 85, 80],
    [92, 95, 94],
    [70, 75, 72],
    [88, 89, 90]
])

pca = PCA(n_components=1)
lang_ability = pca.fit_transform(data)
print("언어능력(1차원)으로 축소된 값:\n", lang_ability.flatten())
```

**출력 예시**:
```
언어능력(1차원)으로 축소된 값:
[ 2.1 -5.3  7.8 -13.2  8.6]
```

> **주의사항**: 차원 축소 후 해석은 주성분 벡터의 계수(가중치)에 따라 달라지므로, 각 변수의 기여도를 확인해야 한다.

---

## Eigenface와 영상 인식 🖼️

PCA는 **영상 인식** 분야에서 **Eigenface** 기법으로 널리 활용된다. 얼굴 이미지를 벡터로 변환한 후, PCA를 적용하여 주요 특징(주성분)을 추출한다. 이때 각 주성분 벡터는 얼굴의 대표적인 패턴을 나타내며, 이를 **Eigenface**라고 부른다.

영상 인식 시스템에서는 여러 장의 얼굴 이미지를 PCA로 분석하여, 새로운 얼굴이 기존 데이터와 얼마나 유사한지 판단한다. 이 과정에서 데이터의 차원이 크게 줄어들어 계산 효율성이 높아진다.

```python
# 흑백 얼굴 이미지(예: 64x64)를 1차원 벡터로 변환 후 PCA 적용 예시
import numpy as np
from sklearn.decomposition import PCA

# 임의의 10장 64x64 흑백 이미지 (0~255) 생성
images = np.random.randint(0, 256, (10, 64, 64))
X = images.reshape(10, -1)  # (10, 4096)

# PCA 적용 (주성분 3개 추출)
pca = PCA(n_components=3)
faces_pca = pca.fit_transform(X)

print("Eigenface shape:", pca.components_.shape)
print("투영된 얼굴 데이터 shape:", faces_pca.shape)
```

**출력 예시**:
```
Eigenface shape: (3, 4096)
투영된 얼굴 데이터 shape: (10, 3)
```

> **주의사항**: Eigenface 기반 인식은 조명, 표정, 각도 변화에 민감할 수 있으므로, 사전 전처리와 추가적인 특징 추출 기법과 함께 사용해야 한다.