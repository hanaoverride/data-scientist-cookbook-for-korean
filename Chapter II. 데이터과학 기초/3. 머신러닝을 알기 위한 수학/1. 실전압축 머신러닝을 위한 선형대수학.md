# 1. 머신러닝을 위한 선형대수학: 데이터를 공간으로, 연산을 변환으로 이해하기

## 목차
- [1. 머신러닝을 위한 선형대수학: 데이터를 공간으로, 연산을 변환으로 이해하기](#1-머신러닝을-위한-선형대수학-데이터를-공간으로-연산을-변환으로-이해하기)
  - [목차](#목차)
  - [1. 왜 선형대수학을 배워야 하는가?](#1-왜-선형대수학을-배워야-하는가)
  - [2. 선형대수학의 눈으로 데이터 바라보기](#2-선형대수학의-눈으로-데이터-바라보기)
    - [데이터 포인트는 '벡터(Vector)'다: 공간 속의 한 점](#데이터-포인트는-벡터vector다-공간-속의-한-점)
    - [데이터셋은 '행렬(Matrix)'이다: 점들의 집합](#데이터셋은-행렬matrix이다-점들의-집합)
  - [3. 머신러닝 모델은 '행렬 연산'이다: 공간의 변환](#3-머신러닝-모델은-행렬-연산이다-공간의-변환)
    - [예측의 본질: 행렬 곱으로 데이터를 움직이기](#예측의-본질-행렬-곱으로-데이터를-움직이기)
  - [4. 차원 축소: 더 단순한 공간에서 데이터 바라보기](#4-차원-축소-더-단순한-공간에서-데이터-바라보기)
  - [5. 앞으로 우리가 배울 것들: 선형대수학 핵심 로드맵](#5-앞으로-우리가-배울-것들-선형대수학-핵심-로드맵)

---

## 1. 왜 선형대수학을 배워야 하는가?

컴퓨터는 이미지, 텍스트, 소리 같은 복잡한 데이터를 직접 이해하지 못합니다. 컴퓨터가 이해하는 유일한 언어는 **숫자**입니다. 선형대수학은 바로 이 다양한 현실 세계의 데이터를 컴퓨터가 이해할 수 있는 **숫자의 배열(벡터와 행렬)**  로 표현하고, 이를 효율적으로 조작하는 규칙을 제공합니다.

즉, 선형대수학은 **데이터를 숫자로 표현하고, 모델의 예측 과정을 연산으로 설명하는 머신러닝의 기본 언어**입니다.

## 2. 선형대수학의 눈으로 데이터 바라보기

### 데이터 포인트는 '벡터(Vector)'다: 공간 속의 한 점

하나의 데이터 포인트는 여러 개의 특성(feature)으로 이루어집니다. 예를 들어, 붓꽃 데이터 하나는 [꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비]라는 4개의 숫자로 구성됩니다.

선형대수학에서는 이 숫자들의 리스트를 **벡터(Vector)**  라고 부릅니다. 4개의 특성을 가졌다면 4차원 공간 속의 **한 점** 또는 원점에서의 **방향과 크기를 가진 화살표**로 상상할 수 있습니다.

```python
import numpy as np

# 붓꽃 데이터 하나를 4차원 벡터로 표현
# [꽃받침 길이=5.1, 꽃받침 너비=3.5, 꽃잎 길이=1.4, 꽃잎 너비=0.2]
iris_vector = np.array([5.1, 3.5, 1.4, 0.2])

print(f"붓꽃 데이터 벡터: {iris_vector}")
print(f"벡터의 차원 (특성의 개수): {iris_vector.shape[0]}")
```

### 데이터셋은 '행렬(Matrix)'이다: 점들의 집합

데이터셋은 여러 개의 데이터 포인트(벡터)들이 모여있는 것입니다. 이 벡터들을 차곡차곡 쌓으면 바로 **행렬(Matrix)**  이 됩니다. 행렬의 각 행은 하나의 데이터 포인트를, 각 열은 하나의 특성을 나타냅s니다.

```python
# 3개의 붓꽃 데이터(벡터)가 모여 3x4 행렬을 이룸
iris_matrix = np.array([
    [5.1, 3.5, 1.4, 0.2], # 데이터 포인트 1 (벡터 1)
    [4.9, 3.0, 1.4, 0.2], # 데이터 포인트 2 (벡터 2)
    [4.7, 3.2, 1.3, 0.2]  # 데이터 포인트 3 (벡터 3)
])

print("붓꽃 데이터 행렬:\n", iris_matrix)
print(f"행렬의 크기 (데이터 개수, 특성 개수): {iris_matrix.shape}")
```
> **핵심**: 머신러닝에 데이터를 입력한다는 것은, 결국 이 **행렬**을 모델에 넣어주는 것과 같습니다.

## 3. 머신러닝 모델은 '행렬 연산'이다: 공간의 변환

머신러닝 모델이 하는 일은 입력된 데이터(행렬)를 우리가 원하는 출력(예측값)으로 **변환(Transformation)**  시키는 것입니다. 그리고 이 변환 과정의 핵심이 바로 **행렬 연산**, 특히 **행렬 곱셈**입니다.

모델의 **가중치(weights)**  는 또 다른 행렬로 표현됩니다. 입력 행렬에 가중치 행렬을 곱하는 것은, 입력 데이터가 있는 공간을 '회전'시키거나 '늘리거나 줄여서' 정답에 더 가까운 새로운 공간으로 옮기는 과정으로 이해할 수 있습니다.

### 예측의 본질: 행렬 곱으로 데이터를 움직이기

선형 회귀 모델 `y = Xw + b` 를 선형대수학의 관점에서 봅시다.

- `X`: 입력 데이터 행렬 (데이터 포인트들이 있는 원래 공간)
- `w`: 모델이 학습한 가중치 벡터/행렬 (데이터를 어떻게 변환시킬지에 대한 정보)
- `y`: 예측 결과 벡터 (변환된 공간에서의 데이터 위치)

```python
# 2개의 특성을 가진 3개의 데이터 포인트
X = np.array([
    [1, 2], # 데이터 1
    [3, 4], # 데이터 2
    [5, 6]  # 데이터 3
])

# 모델이 학습한 가중치 (2x1 행렬)
w = np.array([[0.5],
              [-0.2]])

# 행렬 곱으로 예측값 계산
# (3, 2) 행렬 @ (2, 1) 행렬 -> (3, 1) 행렬
y_pred = X @ w

print("입력 데이터 (3x2 행렬):\n", X)
print("\n가중치 (2x1 행렬):\n", w)
print("\n예측 결과 (3x1 행렬):\n", y_pred)
```
> **인사이트**: 머신러닝의 '학습'이란, 예측 결과(`y_pred`)가 실제 정답(`y_true`)과 가장 가까워지도록 만드는 최적의 변환 규칙, 즉 **가중치 행렬 `w`를 찾아내는 과정**입니다.

## 4. 차원 축소: 더 단순한 공간에서 데이터 바라보기

데이터의 특성(차원)이 너무 많으면 분석이 어렵고 모델 성능이 저하될 수 있습니다. **차원 축소(Dimensionality Reduction)**  는 데이터의 본질적인 정보는 최대한 유지하면서, 더 낮은 차원의 공간으로 데이터를 옮기는 기술입니다.

**주성분 분석(PCA)**  과 같은 기법은 선형대수학의 **고유값(Eigenvalue)과 고유벡터(Eigenvector)**  개념을 사용하여, 데이터가 가장 넓게 퍼져있는(분산이 가장 큰) 방향(축)을 찾아내고, 그 축을 중심으로 데이터를 재표현하여 차원을 줄입니다.

## 5. 앞으로 우리가 배울 것들: 선형대수학 핵심 로드맵

이러한 머신러닝의 동작 원리를 이해하기 위해, 우리는 다음과 같은 선형대수학의 핵심 개념들을 학습할 것입니다.

1.  **벡터와 행렬**: 데이터 표현의 기본 단위와 연산 규칙
2.  **선형 조합과 생성 공간(Span)** : 벡터들로 어떤 공간을 만들 수 있는가?
3.  **행렬 곱의 의미**: 데이터를 다른 공간으로 변환하는 방법
4.  **선형 독립과 랭크(Rank)** : 데이터가 얼마나 '새로운' 정보를 담고 있는가?
5.  **고유값과 고유벡터**: 데이터를 가장 잘 설명하는 핵심 축(방향) 찾기
6.  **특이값 분해(SVD)** : 행렬을 가장 중요한 정보들로 분해하는 기술 (차원 축소, 추천 시스템의 핵심 원리)