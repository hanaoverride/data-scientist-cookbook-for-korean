# 1. 실전압축 머신러닝을 위한 선형대수학 📚

## 목차 📑

### 1. 선형대수학의 역할과 데이터 표현
- [1.1 머신러닝에서의 선형대수학](#머신러닝에서의-선형대수학-) 🧠
- [1.2 데이터의 행렬 및 벡터 표현](#데이터의-행렬-및-벡터-표현-) 🖼️
- [1.3 모델과 행렬 연산](#모델과-행렬-연산-) 🔢
- [1.4 차원 축소와 데이터 효율화](#차원-축소와-데이터-효율화-) 📉

### 2. 강의 목표 및 학습 방법
- [2.1 강의 목표](#강의-목표-) 🎯
- [2.2 학습 방법](#학습-방법-) 📖

### 3. 강의 커리큘럼
- [3.1 선형대수학의 기초](#선형대수학의-기초-) 🏗️
- [3.2 선형대수학 심화](#선형대수학-심화-) 🔬
- [3.3 머신러닝 응용](#머신러닝-응용-) 🤖
- [3.4 다양한 응용](#다양한-응용-) 🌐

---

## 머신러닝에서의 선형대수학 🧠

**설명**  
**선형대수학**은 머신러닝과 딥러닝에서 핵심적인 역할을 한다. 데이터는 주로 **행렬**이나 **벡터**의 형태로 표현되며, 이러한 구조를 효과적으로 다루기 위해서는 선형대수의 개념이 필수적이다. 예를 들어, 이미지 데이터는 픽셀 값이 행렬로 저장되고, 텍스트 데이터는 단어의 벡터로 변환되어 처리된다. 머신러닝 모델의 수식, 연산, 최적화 과정에서도 선형대수는 기본 도구로 사용된다.

**예시**

```python
# 3x3 이미지(흑백) 데이터를 행렬로 표현
import numpy as np

image_matrix = np.array([
    [120, 180, 200],
    [ 90, 150, 170],
    [ 60, 110, 140]
])
print("이미지 행렬:\n", image_matrix)

# 4차원 단어 벡터 예시
sentence_vector = np.array([1, 0, 1, 1])  # 예: "AI is fun" -> [1, 0, 1, 1]
print("문장 벡터:", sentence_vector)
```

**주의사항**  
데이터의 구조에 따라 적절한 행렬 또는 벡터 표현을 선택해야 하며, 차원이 잘못 맞춰지면 연산 오류가 발생할 수 있다.

---

## 데이터의 행렬 및 벡터 표현 🖼️

**설명**  
이미지, 텍스트, 표 등 다양한 데이터는 **행렬** 또는 **벡터**로 변환하여 머신러닝 모델에 입력된다. 예를 들어, 컬러 이미지는 각 픽셀의 RGB 값을 가지는 3차원 행렬로 표현된다. 텍스트는 단어별로 벡터화하여 수치 데이터로 변환된다.

**예시**

```python
# RGB 이미지(2x2 픽셀, 3채널) 예시
rgb_image = np.array([
    [[255, 0, 0], [0, 255, 0]],
    [[0, 0, 255], [255, 255, 0]]
])
print("RGB 이미지 행렬:\n", rgb_image)

# 5개의 단어로 구성된 문장 벡터화 예시
words = ["deep", "learning", "is", "very", "powerful"]
word_vector = np.array([1, 1, 1, 1, 1])  # 각 단어가 한 번씩 등장
print("문장 벡터:", word_vector)
```

**주의사항**  
이미지의 경우 채널(RGB 등)과 크기(행, 열)를 명확히 구분해야 하며, 텍스트 벡터화 시 단어 순서와 차원 수에 유의해야 한다.

---

## 모델과 행렬 연산 🔢

**설명**  
많은 머신러닝 모델은 **행렬 연산**을 기반으로 한다. 대표적으로 **선형 회귀**, **로지스틱 회귀**, **서포트 벡터 머신** 등은 입력 데이터와 가중치 벡터의 곱셈, 전치, 역행렬 등의 연산을 통해 예측을 수행한다. 이러한 연산은 모델의 학습과 추론 과정에서 반복적으로 사용된다.

**예시**

```python
# 선형 회귀 예측: y = Xw + b
X = np.array([[1, 2], [3, 4], [5, 6]])  # 입력 데이터 (3개 샘플, 2개 특성)
w = np.array([0.5, -0.2])               # 가중치 벡터
b = 1.0                                 # 편향

y_pred = X @ w + b
print("예측 결과:", y_pred)
```

**주의사항**  
행렬 곱셈 시 차원이 일치해야 하며, 역행렬 계산은 행렬이 정방행렬이고 가역적일 때만 가능하다.

---

## 차원 축소와 데이터 효율화 📉

**설명**  
**차원 축소**는 고차원 데이터를 저차원으로 변환하여 연산 효율을 높이고, 노이즈를 줄이는 데 사용된다. 대표적인 기법으로 **주성분 분석(PCA)**, **특이값 분해(SVD)** 등이 있으며, 데이터의 본질적인 특성을 최대한 보존하면서 불필요한 정보를 제거한다.

**예시**

```python
from sklearn.decomposition import PCA

# 5차원 데이터를 2차원으로 축소
data = np.random.rand(10, 5)
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data)
print("축소된 데이터:\n", reduced_data)
```

**주의사항**  
차원 축소 시 정보 손실이 발생할 수 있으므로, 축소 전후의 데이터 분포와 설명력을 반드시 확인해야 한다.

---

## 학습 목표 🎯

**설명**  
본 학습의 목표는 머신러닝에 필수적인 **선형대수**, **확률이론**, **미분 및 최적화**의 핵심 내용을 이해하는 것이다. 각 이론은 머신러닝 모델의 원리와 구현에 직접적으로 연결되며, 실무와 연구 모두에 필수적이다.

**예시**

```python
# 확률 이론: 조건부 확률 계산 예시
def conditional_probability(event_a, event_b, prob_a, prob_b, prob_a_and_b):
    return prob_a_and_b / prob_b

# 미분 이론: 단순 함수의 도함수 계산
def derivative(f, x, h=1e-5):
    return (f(x + h) - f(x)) / h
```

**주의사항**  
각 이론의 모든 내용을 한 번에 완벽히 이해하려 하지 말고, 반복 학습과 실습을 통해 점진적으로 익히는 것이 효과적이다.

---

## 학습 방법 📖

**설명**  
학습자는 처음부터 모든 개념을 완벽히 이해하려고 하기보다는, **지속적인 반복 학습**과 **실제 예제 중심의 학습**을 통해 점진적으로 이론을 습득하는 것이 중요하다. 각 이론은 구체적인 예시와 함께 설명되며, 필요시 증명도 함께 제공된다.

**예시**

```python
# 반복 학습 예시: 벡터 내적 연습
a = np.array([2, 3])
b = np.array([4, 1])
dot_product = np.dot(a, b)
print("벡터 내적 결과:", dot_product)
```

**주의사항**  
처음에는 이론의 전체 구조를 파악하고, 이후 세부 내용을 반복적으로 복습하는 것이 효과적이다.

---

## 머신러닝 응용 🤖

**설명**  
선형대수학은 **데이터 표현**, **특징 추출**, **차원 축소(PCA)**, **확률론적 모델링(나이브 베이즈, 마르코프 과정 등)**, **최적화(경사 하강법, 라그랑주 승수법)** 등 다양한 머신러닝 응용에 활용된다. 미분 및 편미분, 테일러 급수, 극값 탐색 등도 포함된다.

**예시**

```python
# PCA를 이용한 특징 추출 예시
from sklearn.decomposition import PCA

sample_data = np.random.rand(8, 4)
pca = PCA(n_components=2)
features = pca.fit_transform(sample_data)
print("추출된 특징:\n", features)

# 경사 하강법 단순 예시
def gradient_descent_step(w, grad, lr=0.1):
    return w - lr * grad

w = 2.0
grad = 0.5
w_new = gradient_descent_step(w, grad)
print("업데이트된 가중치:", w_new)
```

**주의사항**  
최적화 알고리즘의 수렴 조건과 학습률 설정에 주의해야 하며, 차원 축소 시 정보 손실을 고려해야 한다.

---

## 다양한 응용 🌐

**설명**  
신경망의 기본 구조, 행렬 연산, 딥러닝의 역사, 역전파 알고리즘 등 실제 인공지능 시스템에서 선형대수학의 다양한 응용 사례를 다룬다. 특히, 신경망의 학습 과정에서 **행렬 연산**과 **편미분**이 핵심적으로 사용된다.

**예시**

```python
# 신경망의 행렬 연산 예시 (단일 은닉층)
X = np.array([[0.2, 0.8]])
W1 = np.array([[0.5, -0.3], [0.8, 0.2]])
b1 = np.array([0.1, -0.1])
hidden = np.dot(X, W1) + b1
print("은닉층 출력:", hidden)

# 역전파에서의 편미분 예시
def simple_backprop(y_true, y_pred):
    return 2 * (y_pred - y_true)

loss_grad = simple_backprop(1.0, 0.7)
print("손실 함수의 미분 결과:", loss_grad)
```

**주의사항**  
신경망 연산에서 행렬의 크기와 데이터 배치 차원에 유의해야 하며, 역전파 구현 시 미분 규칙을 정확히 적용해야 한다.

