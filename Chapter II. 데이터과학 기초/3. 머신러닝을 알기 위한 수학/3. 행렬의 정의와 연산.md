# 3. 행렬 연산: 데이터를 변환하고 모델을 학습시키는 언어


## 목차
- [3. 행렬 연산: 데이터를 변환하고 모델을 학습시키는 언어](#3-행렬-연산-데이터를-변환하고-모델을-학습시키는-언어)
  - [목차](#목차)
  - [1. 기본 연산: 데이터의 스케일링과 결합](#1-기본-연산-데이터의-스케일링과-결합)
    - [덧셈/뺄셈과 스칼라 곱](#덧셈뺄셈과-스칼라-곱)
  - [2. 행렬 곱셈: 머신러닝의 심장](#2-행렬-곱셈-머신러닝의-심장)
    - [행렬 곱셈의 의미: 선형 변환의 합성](#행렬-곱셈의-의미-선형-변환의-합성)
    - [내적(Dot Product)의 의미: 유사도와 투영](#내적dot-product의-의미-유사도와-투영)
  - [3. 전치 행렬: 데이터의 관점 바꾸기](#3-전치-행렬-데이터의-관점-바꾸기)
  - [4. 단위 행렬과 역행렬: 변환을 취소하는 연산](#4-단위-행렬과-역행렬-변환을-취소하는-연산)
    - [역행렬의 활용: 선형 회귀의 해 구하기](#역행렬의-활용-선형-회귀의-해-구하기)
  - [5. 행렬식(Determinant): 변환 시 공간의 부피 변화율](#5-행렬식determinant-변환-시-공간의-부피-변화율)

---

## 1. 기본 연산: 데이터의 스케일링과 결합

### 덧셈/뺄셈과 스칼라 곱

- **스칼라 곱**: 행렬에 특정 숫자를 곱하는 것은, 데이터 공간 전체를 균일하게 **확대 또는 축소**하는 것과 같습니다.
- **덧셈/뺄셈**: 두 행렬을 더하거나 빼는 것은, 데이터 포인트들을 특정 방향으로 **집단 이동**시키는 것과 같습니다.

이들은 데이터 전처리 과정에서 스케일을 조정하거나 특정 값을 일괄적으로 보정할 때 사용됩니다.

```python
import numpy as np

# 2개의 2차원 데이터 포인트
A = np.array([
    [1, 2],
    [3, 4]
])

# 스칼라 곱: 모든 데이터를 원점으로부터 2배 멀리 이동
scaled_A = 2 * A
print("스케일링된 데이터:\n", scaled_A)

# 덧셈: 모든 데이터를 [10, 20] 방향으로 평행 이동
B = np.array([
    [10, 20],
    [10, 20]
])
moved_A = A + B
print("\n평행 이동된 데이터:\n", moved_A)
```

## 2. 행렬 곱셈: 머신러닝의 심장

행렬 곱셈은 선형대수학에서 가장 중요하고 강력한 연산입니다. 단순한 원소별 곱이 아니라, **'선형 변환(Linear Transformation)'** 이라는 기하학적 의미를 담고 있습니다.

### 행렬 곱셈의 의미: 선형 변환의 합성

행렬을 곱한다는 것은 **데이터 공간을 변환하는 행위를 연달아 적용**하는 것과 같습니다.

- `입력 데이터 행렬 X`에 `가중치 행렬 W`를 곱하는 것 (`X @ W`)은,
- `X`라는 데이터 포인트들을 `W`라는 규칙에 따라 새로운 위치로 '회전'시키고 '크기'를 조절하여 옮기는 과정입니다.
- 이것이 바로 **머신러닝 모델의 예측(prediction) 과정 그 자체**입니다.

```python
# 2차원 데이터 포인트
point = np.array([1, 0])

# 45도 회전 변환 행렬
angle = np.pi / 4
rotation_matrix = np.array([
    [np.cos(angle), -np.sin(angle)],
    [np.sin(angle),  np.cos(angle)]
])

# 행렬 곱으로 점을 45도 회전
rotated_point = point @ rotation_matrix.T # NumPy에서는 벡터를 행으로 다루므로 전치

print(f"원본 점: {point}")
print(f"45도 회전 행렬:\n{rotation_matrix}")
print(f"회전된 점: {rotated_point.round(2)}")
```

### 내적(Dot Product)의 의미: 유사도와 투영

벡터 간의 내적은 두 벡터가 **얼마나 비슷한 방향을 가리키는가(유사도)**  를 측정하는 도구입니다.
- 두 벡터가 같은 방향이면 내적 값은 최대가 됩니다.
- 두 벡터가 90도(직교)를 이루면 내적 값은 0이 됩니다 (관련 없음).
- 두 벡터가 반대 방향이면 내적 값은 최소가 됩니다.

또한, 한 벡터를 다른 벡터에 **투영(projection)**  시켜 그림자의 길이를 구하는 데도 사용됩니다. 이는 특정 성분을 추출하는 데 매우 유용합니다.

> **머신러닝 관점**:
> 추천 시스템에서 사용자 벡터와 아이템 벡터의 내적을 계산하여, 두 벡터가 얼마나 유사한지를 바탕으로 아이템을 추천해 줄 수 있습니다.

## 3. 전치 행렬: 데이터의 관점 바꾸기

**전치(Transpose)**  는 행렬의 행과 열을 맞바꾸는 간단한 연산이지만, 데이터의 관점을 바꾸는 중요한 역할을 합니다.
- `(샘플 수, 특성 수)` 모양의 데이터 행렬을 전치하면 `(특성 수, 샘플 수)` 모양이 됩니다.
- 이는 연산의 편의를 위해, 또는 데이터의 공분산 행렬을 계산하는 등 특성 간의 관계를 분석할 때 필수적으로 사용됩니다.

```python
# (샘플 3개, 특성 2개)
data = np.array([
    [160, 60], # 사람1: 키, 몸무게
    [170, 70], # 사람2
    [180, 80]  # 사람3
])

# 전치를 통해 (특성 2개, 샘플 3개)로 관점 변경
data_transposed = data.T
print("전치된 데이터:\n", data_transposed)
# 이제 첫 행은 모든 사람의 '키' 정보, 두 번째 행은 '몸무게' 정보가 됨
```

## 4. 단위 행렬과 역행렬: 변환을 취소하는 연산

- **단위 행렬(Identity Matrix)** : 곱셈에서의 숫자 '1'과 같은 역할. 어떤 행렬에 곱해도 자기 자신을 그대로 돌려줍니다. 즉, **아무것도 바꾸지 않는 변환**입니다.

- **역행렬(Inverse Matrix)** : 어떤 행렬 `A`가 수행한 변환을 **정확히 거꾸로 되돌리는 변환**입니다. `A`로 회전시켰다면, `A의 역행렬`은 원래 위치로 되돌리는 회전입니다. `A @ A_inverse`는 결국 아무것도 하지 않은 것과 같으므로 단위 행렬이 됩니다.

### 역행렬의 활용: 선형 회귀의 해 구하기

선형 회귀 모델은 `y = Xw` 라는 식으로 표현됩니다 (편의상 편향 생략). 여기서 우리가 구하고 싶은 것은 데이터 `X`와 정답 `y`를 가장 잘 설명하는 최적의 가중치 `w`입니다.

양변에 `X`의 역행렬(정확히는 의사역행렬)을 곱하면, `w`에 대해 식을 풀 수 있습니다.
`X_inverse @ y = (X_inverse @ X) @ w`
`X_inverse @ y = I @ w`
`w = X_inverse @ y`

> **핵심**: 역행렬은 이처럼 특정 변환의 효과를 없애고, 방정식의 해를 구하는 데 결정적인 역할을 합니다.

```python
# y = Xw 에서 w 구하기
X = np.array([[1, 2], [2, 3]])
y = np.array([3, 5])

# 역행렬을 이용해 w 계산
X_inv = np.linalg.inv(X)
w = X_inv @ y

print(f"최적의 가중치 w: {w}")
# 확인: X @ w 가 y와 같은지?
print(f"검증 (X @ w): {X @ w}")
```

## 5. 행렬식(Determinant): 변환 시 공간의 부피 변화율

**행렬식**은 행렬이 공간을 얼마나 '넓히거나' '좁히는지'를 나타내는 스칼라 값입니다.
- 행렬식의 절대값 > 1: 변환 후 공간의 부피가 늘어남
- 행렬식의 절대값 < 1: 변환 후 공간의 부피가 줄어듦
- **행렬식 = 0**: 변환 후 공간이 납작하게 찌그러져 부피가 0이 됨 (예: 2차원 평면이 1차원 직선으로 변환). 이 경우, 원래대로 되돌릴 방법이 없으므로 **역행렬이 존재하지 않습니다.**

> **머신러닝 관점**:
> 행렬식이 0에 가깝다는 것은, 해당 변환이 데이터의 정보를 심각하게 손실시킨다는 의미입니다. 또한, 여러 특성들이 서로 독립적이지 않고 강하게 겹칠 때(다중공선성) 행렬식이 0에 가까워지며, 이는 모델의 불안정성을 야기할 수 있습니다.