# 5. 선형변환 📐

## 목차 📑

### 1. 행렬의 기본 개념
- [1.1 행렬의 세 가지 관점](#행렬의-세-가지-관점-) 🔍
- [1.2 행렬의 연산과 벡터공간](#행렬의-연산과-벡터공간-) ➕

### 2. 선형변환과 함수로서의 행렬
- [2.1 선형변환의 정의](#선형변환의-정의-) 🔄
- [2.2 선형변환의 성질](#선형변환의-성질-) 🧩
- [2.3 신경망에서의 선형변환](#신경망에서의-선형변환-) 🧠

### 3. 행렬의 Rank(랭크)
- [3.1 Rank의 정의와 계산](#rank의-정의와-계산-) 📏
- [3.2 부분 행렬식을 이용한 Rank 계산](#부분-행렬식을-이용한-rank-계산-) 🧮

### 4. 행렬식(Determinant)
- [4.1 행렬식의 정의와 계산법](#행렬식의-정의와-계산법-) 🔢
- [4.2 행렬식의 성질](#행렬식의-성질-) ⚠️
- [4.3 행렬식과 역행렬](#행렬식과-역행렬-) 🔁

### 5. 특수한 행렬
- [5.1 전치행렬](#전치행렬-) 🔃
- [5.2 대각행렬, 단위행렬, 대칭/반대칭행렬](#대각행렬-단위행렬-대칭반대칭행렬-) 🟦
- [5.3 직교행렬](#직교행렬-) 🟩

---

## 행렬의 세 가지 관점 🔍

### 설명

**행렬**은 선형대수학에서 매우 중요한 개념으로, 다양한 시각에서 해석할 수 있다.  
첫째, 행렬은 **벡터의 확장**으로 볼 수 있다. 여러 개의 벡터를 행 또는 열로 나열하여 새로운 연산을 정의할 수 있다.  
둘째, 행렬은 **벡터공간을 연결하는 함수**로 해석된다. 이때 행렬 곱셈은 함수의 합성과 유사하며, 역함수 개념은 **역행렬**과 대응된다.  
셋째, 행렬 자체의 변형(예: 행 연산, 열 연산 등)을 통해 원하는 성질을 갖는 행렬로 바꿀 수 있다. 이는 선형대수에서 매우 자주 활용된다.

### 예시

```python
# 2x3 행렬과 2x1 벡터의 연산 예시
import numpy as np

A = np.array([[2, 0, 1],
              [1, 3, 2]])
v = np.array([[1],
              [4],
              [2]])

result = np.dot(A, v)
print(result)
# 출력: [[6], [15]]
```

### 주의사항

- **행렬 연산**에서 크기가 맞지 않는 벡터나 행렬은 연산이 불가능하다.
- 행렬의 해석 방법에 따라 문제 해결 방식이 달라질 수 있다.

---

## 행렬의 연산과 벡터공간 ➕

### 설명

행렬은 벡터공간의 성질을 확장하여 **덧셈**과 **스칼라 곱**이 정의된다.  
행렬의 덧셈은 같은 크기의 행렬끼리만 가능하며, 각 원소별로 더한다.  
스칼라 곱은 실수(또는 복소수)와 행렬의 곱으로, 각 원소에 스칼라를 곱한다.  
이러한 연산은 벡터공간의 공리(교환법칙, 결합법칙, 항등원, 역원 등)를 만족한다.

### 예시

```python
import numpy as np

M1 = np.array([[1, 2], [3, 4]])
M2 = np.array([[5, 6], [7, 8]])
k = 2

add_result = M1 + M2
scalar_result = k * M1

print("덧셈 결과:\n", add_result)
print("스칼라 곱 결과:\n", scalar_result)
```

### 주의사항

- **행렬의 크기**가 다르면 덧셈이 불가능하다.
- 행렬 연산의 기본 성질(교환법칙, 결합법칙 등)을 반드시 이해해야 한다.

---

## 선형변환의 정의 🔄

### 설명

**선형변환**은 한 벡터공간에서 다른 벡터공간으로의 함수로, 벡터 연산(덧셈, 스칼라 곱)을 보존한다.  
즉, 임의의 벡터 x, y와 스칼라 α에 대해  
`f(x + y) = f(x) + f(y)`  
`f(αx) = αf(x)`  
를 만족하는 함수이다.  
행렬은 이러한 선형변환을 구체적으로 표현하는 도구이다.

### 예시

```python
import numpy as np

# 2차원 벡터에 대한 선형변환(회전)
theta = np.pi / 4  # 45도 회전
rotation_matrix = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)]
])
vector = np.array([1, 0])

transformed = np.dot(rotation_matrix, vector)
print("변환 결과:", transformed)
```

### 주의사항

- 모든 함수가 선형변환이 되는 것은 아니며, 반드시 **벡터 연산 보존** 조건을 만족해야 한다.
- 선형변환은 행렬 곱셈으로 표현할 수 있다.

---

## 선형변환의 성질 🧩

### 설명

선형변환은 다음과 같은 성질을 가진다.
- 선형변환의 합성은 행렬 곱셈과 같다.
- 선형변환의 **영공간**(null space)과 **치역**(range)은 각각 부분공간을 이룬다.
- 선형변환의 차원 공식:  
  `n = dim(N(A)) + dim(Range(A))`  
  여기서 N(A)는 영공간, Range(A)는 치역이다.

### 예시

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
x = np.array([0, 1])
y = np.array([1, 1])

# 선형성 확인
left = np.dot(A, x + y)
right = np.dot(A, x) + np.dot(A, y)
print("선형성 확인:", np.allclose(left, right))
```

### 주의사항

- 선형변환의 영공간과 치역은 모두 **부분공간**임을 기억해야 한다.
- 차원 공식은 선형대수의 핵심 정리 중 하나이다.

---

## 신경망에서의 선형변환 🧠

### 설명

신경망에서 **선형변환**은 입력 벡터에 가중치 행렬과 편향 벡터를 적용하여 다음 계층으로 전달하는 연산이다.  
이 과정은 행렬 곱셈과 벡터 덧셈으로 표현된다.

### 예시

```python
import numpy as np

# 입력 벡터, 가중치 행렬, 편향 벡터
input_vec = np.array([0.5, 1.2])
weights = np.array([[0.8, -0.3], [0.2, 0.5]])
bias = np.array([0.1, -0.2])

output = np.dot(weights, input_vec) + bias
print("신경망 선형변환 결과:", output)
```

### 주의사항

- 입력 벡터와 가중치 행렬의 크기가 맞아야 연산이 가능하다.
- 편향 벡터는 각 출력 노드에 더해진다.

---

## Rank의 정의와 계산 📏

### 설명

**Rank(랭크)**는 행렬의 열 또는 행 벡터 중 **일차독립**인 벡터의 최대 개수이다.  
Rank는 선형방정식의 해의 존재성과 밀접한 관련이 있다.  
행렬의 Rank는 행 또는 열 벡터의 생성 공간의 차원과 같다.

### 예시

```python
import numpy as np

M = np.array([[1, 2, 3],
              [2, 4, 6],
              [3, 6, 9]])

rank = np.linalg.matrix_rank(M)
print("행렬의 Rank:", rank)  # 출력: 1
```

### 주의사항

- Rank가 0이면 모든 벡터가 0벡터임을 의미한다.
- Rank는 행과 열 중 어느 쪽을 기준으로 해도 동일하다.

---

## 부분 행렬식을 이용한 Rank 계산 🧮

### 설명

Rank를 계산할 때 **부분 행렬식**을 활용할 수 있다.  
n×m 행렬에서 k×k 부분 행렬식이 0이 아닌 최대 k를 찾으면, 그 값이 Rank이다.  
부분 행렬식이 모두 0이면, 그보다 작은 크기의 부분 행렬식을 확인한다.

### 예시

```python
import numpy as np

M = np.array([[2, 4, 1],
              [0, 0, 0],
              [1, 2, 0]])

# 2x2 부분 행렬식 확인
submatrix = M[:2, :2]
det = np.linalg.det(submatrix)
print("2x2 부분 행렬식:", det)
```

### 주의사항

- 부분 행렬식이 0이 아닌 가장 큰 크기를 찾는 것이 중요하다.
- 실수 오차로 인해 행렬식이 0에 매우 가까운 경우 주의해야 한다.

---

## 행렬식의 정의와 계산법 🔢

### 설명

**행렬식(Determinant)**은 정방행렬(행과 열의 수가 같은 행렬)에 대해 정의되는 값으로, 선형방정식의 해의 존재, 역행렬의 존재 등과 밀접한 관련이 있다.  
2×2 행렬의 행렬식은 `ad - bc`로 계산한다.  
3×3 이상의 행렬은 **여러 행과 열을 제거한 부분 행렬식(소행렬식)**을 이용하여 재귀적으로 계산한다.

### 예시

```python
import numpy as np

M2 = np.array([[4, 2],
               [3, 1]])
det2 = np.linalg.det(M2)
print("2x2 행렬식:", det2)

M3 = np.array([[1, 2, 3],
               [0, 1, 4],
               [5, 6, 0]])
det3 = np.linalg.det(M3)
print("3x3 행렬식:", det3)
```

### 주의사항

- 행렬식은 정방행렬에서만 정의된다.
- 3×3 이상의 행렬식 계산은 **사루스 법칙**이나 **여인자 전개**를 사용할 수 있다.

---

## 행렬식의 성질 ⚠️

### 설명

행렬식은 다음과 같은 중요한 성질을 가진다.
- `det(A) = det(A.T)` (전치행렬의 행렬식은 원래 행렬과 같다)
- 두 행(또는 열)이 같거나 일차종속이면 행렬식은 0이다.
- `det(kA) = k^n * det(A)` (n×n 행렬에서 k는 스칼라)
- `det(AB) = det(A) * det(B)`
- 행렬이 **정칙행렬**(역행렬 존재)이면 행렬식이 0이 아니다.

### 예시

```python
import numpy as np

A = np.array([[2, 1], [3, 4]])
B = np.array([[1, 0], [0, 1]])
print("det(A):", np.linalg.det(A))
print("det(B):", np.linalg.det(B))
print("det(AB):", np.linalg.det(np.dot(A, B)))
print("det(A)*det(B):", np.linalg.det(A) * np.linalg.det(B))
```

### 주의사항

- 행렬식이 0이면 역행렬이 존재하지 않는다.
- 행렬의 두 행(또는 열)이 같거나 일차종속이면 반드시 행렬식이 0이 된다.

---

## 행렬식과 역행렬 🔁

### 설명

정방행렬의 **역행렬**은 행렬식이 0이 아닐 때에만 존재한다.  
역행렬은 행렬식과 **여인자 행렬**을 이용해 구할 수 있다.  
2×2 행렬의 역행렬은 다음과 같이 계산된다.

### 예시

```python
import numpy as np

M = np.array([[3, 2],
              [1, 4]])
det = np.linalg.det(M)
if det != 0:
    inv = np.linalg.inv(M)
    print("역행렬:\n", inv)
else:
    print("역행렬이 존재하지 않음")
```

### 주의사항

- 행렬식이 0이면 역행렬이 존재하지 않는다.
- 역행렬 계산 시 행렬식이 분모에 들어가므로, 0에 가까운 값일 때 수치적 불안정성이 발생할 수 있다.

---

## 전치행렬 🔃

### 설명

**전치행렬(Transposed matrix)**은 행과 열을 서로 바꾼 행렬이다.  
즉, 원래 행렬의 (i, j) 원소가 전치행렬에서는 (j, i) 위치에 온다.  
전치 연산은 `A.T` 또는 `A.transpose()`로 수행한다.

### 예시

```python
import numpy as np

M = np.array([[1, 2, 3],
              [4, 5, 6]])
M_T = M.T
print("전치행렬:\n", M_T)
```

### 주의사항

- 전치행렬의 전치는 원래 행렬과 같다.
- `(AB).T = B.T @ A.T`임을 기억해야 한다.

---

## 대각행렬, 단위행렬, 대칭/반대칭행렬 🟦

### 설명

- **대각행렬(Diagonal matrix)**: 주대각선 이외의 모든 원소가 0인 정방행렬이다.
- **단위행렬(Identity matrix)**: 대각행렬 중 주대각선의 원소가 모두 1인 행렬로, 행렬 곱셈의 항등원 역할을 한다.
- **대칭행렬(Symmetric matrix)**: 전치행렬과 자기 자신이 같은 행렬(`A = A.T`)이다.
- **반대칭행렬(Antisymmetric matrix)**: 전치행렬과 부호가 반대인 행렬(`A = -A.T`)이다.

### 예시

```python
import numpy as np

diag = np.diag([2, 5, 7])
identity = np.eye(3)
symmetric = np.array([[1, 2], [2, 3]])
antisymmetric = np.array([[0, 4], [-4, 0]])

print("대각행렬:\n", diag)
print("단위행렬:\n", identity)
print("대칭행렬:\n", symmetric)
print("반대칭행렬:\n", antisymmetric)
```

### 주의사항

- 반대칭행렬의 대각선 원소는 항상 0이다.
- 단위행렬은 행렬 곱셈에서 항등원 역할을 한다.

---

## 직교행렬 🟩

### 설명

**직교행렬(Orthogonal matrix)**은 전치행렬과 자기 자신을 곱했을 때 단위행렬이 되는 정방행렬이다.  
즉, `Q.T @ Q = I`를 만족한다.  
직교행렬은 벡터의 **길이**와 **각도**를 보존하는 변환(예: 회전, 반사 등)을 나타낸다.

### 예시

```python
import numpy as np

theta = np.pi / 3  # 60도 회전
Q = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)]
])

# 직교성 확인
print("Q.T @ Q:\n", np.dot(Q.T, Q))
```

### 주의사항

- 직교행렬의 역행렬은 전치행렬과 같다.
- 직교행렬은 항상 행렬식이 ±1이다.
- 직교행렬은 벡터의 **길이**와 **내적**을 보존한다.
