# 9. 확률이론과 머신러닝 응용

## 목차 📑

### 1. 확률이론
- [1.1 표본공간과 사건](#표본공간과-사건-) 🎲
- [1.2 확률의 공리와 계산](#확률의-공리와-계산-) 📐
- [1.3 조건부 확률과 베이즈 정리](#조건부-확률과-베이즈-정리-) 🔄
- [1.4 전확률 공식](#전확률-공식-) 🧮

### 2. 확률변수
- [2.1 확률변수와 상태공간](#확률변수와-상태공간-) 📏
- [2.2 확률함수와 분포](#확률함수와-분포-) 📊
- [2.3 기대값과 분산](#기대값과-분산-) 📈

### 3. 결합확률분포
- [3.1 결합확률질량함수와 주변확률](#결합확률질량함수와-주변확률-) 🔗
- [3.2 결합확률밀도함수와 다변수 확장](#결합확률밀도함수와-다변수-확장-) 🌐

### 4. 나이브 베이즈 분류기와 마르코프 결정과정
- [4.1 확률변수의 독립과 조건부 독립](#확률변수의-독립과-조건부-독립-) 🔓
- [4.2 나이브 베이즈 분류기](#나이브-베이즈-분류기-) 🏷️
- [4.3 마르코프 결정과정](#마르코프-결정과정-) 🔄

---

## 표본공간과 사건 🎲

**설명**  
확률이론에서 **표본공간**은 확률실험의 모든 가능한 결과의 집합을 의미한다. **사건**은 표본공간의 부분집합으로, 특정 결과들의 집합이다. 예를 들어, 주사위를 한 번 던지는 실험의 표본공간은 {1, 2, 3, 4, 5, 6}이고, "짝수가 나오는 사건"은 {2, 4, 6}과 같이 표현된다.

**예시**

```python
# 동전을 세 번 던지는 실험의 표본공간 생성
import itertools

outcomes = list(itertools.product(['앞', '뒤'], repeat=3))
print("표본공간:", outcomes)
# 출력 예시: [('앞', '앞', '앞'), ('앞', '앞', '뒤'), ..., ('뒤', '뒤', '뒤')]
```

**주의사항**  
사건은 표본공간의 부분집합이므로, 표본공간을 명확히 정의한 후 사건을 설정해야 한다.

---

## 확률의 공리와 계산 📐

**설명**  
확률은 표본공간의 각 사건에 대해 0과 1 사이의 값을 할당하는 **집합함수**이다. 확률의 기본 공리는 다음과 같다:
1. 모든 사건 A에 대해 0 ≤ P(A) ≤ 1
2. 전체 표본공간 S에 대해 P(S) = 1
3. 서로 배반인 사건 A, B에 대해 P(A ∪ B) = P(A) + P(B)
4. 무한히 많은 서로 배반인 사건의 합에 대해서도 확률의 덧셈이 성립한다.

**예시**

```python
# 1~10까지의 숫자 중 3의 배수가 나올 확률 계산
sample_space = set(range(1, 11))
event = {3, 6, 9}
prob = len(event) / len(sample_space)
print(f"3의 배수가 나올 확률: {prob:.2f}")
# 출력: 3의 배수가 나올 확률: 0.30
```

**주의사항**  
확률의 합이 1이 되도록 사건을 정의해야 하며, 중복되는 사건을 합산할 때는 배반 여부를 반드시 확인해야 한다.

---

## 조건부 확률과 베이즈 정리 🔄

**설명**  
**조건부 확률**은 어떤 사건이 이미 발생했다는 조건 하에서 다른 사건이 발생할 확률을 의미한다. **베이즈 정리**는 조건부 확률을 서로 바꿔 계산할 수 있는 공식이다.

- 조건부 확률: P(B|A) = P(A ∩ B) / P(A)
- 베이즈 정리: P(A|B) = [P(A) × P(B|A)] / P(B)

**예시**

```python
# 가상의 데이터: 학생 100명 중 40명이 수학 동아리, 25명이 과학 동아리, 10명이 두 동아리 모두 소속
total = 100
math = 40
science = 25
both = 10

# 수학 동아리 학생 중 과학 동아리 학생일 확률
p_both = both / total
p_math = math / total
p_science_given_math = p_both / p_math
print(f"수학 동아리 학생 중 과학 동아리 학생일 확률: {p_science_given_math:.2f}")
# 출력: 0.25
```

**주의사항**  
분모가 0이 되는 경우(조건 사건의 확률이 0)는 조건부 확률을 정의할 수 없다.

---

## 전확률 공식 🧮

**설명**  
**전확률 공식**은 표본공간이 여러 부분으로 분할될 때, 전체 사건의 확률을 각 분할에 대한 조건부 확률과 분할의 확률의 곱의 합으로 나타내는 공식이다.

P(B) = Σ P(Ai) × P(B|Ai)

**예시**

```python
# 세 개의 상자에 각각 빨간공과 파란공이 다르게 들어있을 때, 임의로 상자를 선택해 빨간공을 뽑을 확률
boxes = [
    {'red': 2, 'blue': 2},
    {'red': 3, 'blue': 1},
    {'red': 1, 'blue': 3}
]
prob_box = 1 / 3
prob_red = sum((box['red'] / (box['red'] + box['blue'])) * prob_box for box in boxes)
print(f"빨간공을 뽑을 확률: {prob_red:.2f}")
# 출력: 0.44
```

**주의사항**  
분할된 사건들은 서로 배반이어야 하며, 모든 경우의 수를 빠짐없이 고려해야 한다.

---

## 확률변수와 상태공간 📏

**설명**  
**확률변수**는 표본공간의 각 원소에 실수 값을 대응시키는 함수이다. 확률변수는 **이산형**(유한 또는 셀 수 있는 값)과 **연속형**(구간 내의 모든 값)으로 나뉜다. 확률변수의 치역을 **상태공간**이라 한다.

**예시**

```python
# 학생 5명의 키를 구간별로 분류하는 확률변수 예시
students = ['A', 'B', 'C', 'D', 'E']
heights = [158, 162, 170, 175, 180]
def height_category(h):
    if h < 160:
        return '150-160'
    elif h < 170:
        return '160-170'
    elif h < 180:
        return '170-180'
    else:
        return '180+'
categories = [height_category(h) for h in heights]
print("키 구간:", categories)
# 출력: ['150-160', '160-170', '170-180', '170-180', '180+']
```

**주의사항**  
확률변수의 값은 반드시 상태공간 내에 존재해야 하며, 구간 분할 시 중복이나 누락이 없도록 주의해야 한다.

---

## 확률함수와 분포 📊

**설명**  
이산 확률변수의 경우 **확률질량함수(pmf)**, 연속 확률변수의 경우 **확률밀도함수(pdf)** 로 확률을 정의한다.  
- pmf: 각 상태공간의 값에 확률을 할당
- pdf: 구간 내에서 확률의 밀도를 정의하며, 특정 값에서의 확률은 0

**예시**

```python
# 주사위 한 번 던지기: 확률질량함수
pmf = {i: 1/6 for i in range(1, 7)}
print("주사위 pmf:", pmf)

# 연속 확률변수 예시: 0~1 구간에서 균등분포
def pdf_uniform(x):
    return 1 if 0 <= x <= 1 else 0
print("pdf(0.5):", pdf_uniform(0.5))
```

**주의사항**  
pmf의 모든 확률의 합은 1이어야 하며, pdf의 경우 전체 구간에 대해 적분한 값이 1이어야 한다.

---

## 기대값과 분산 📈

**설명**  
**기대값**은 확률변수가 가질 수 있는 값에 그 확률을 곱해 모두 더한 값이다. **분산**은 확률변수의 값이 평균에서 얼마나 퍼져 있는지를 나타내는 척도이다.

- 이산형: E[X] = Σ x * P(X=x)
- 연속형: E[X] = ∫ x * f(x) dx
- 분산: Var(X) = E[(X - μ)²] = E[X²] - (E[X])²

**예시**

```python
# 1등 1명(1000원), 2등 2명(500원), 3등 3명(200원), 꽝 4명(0원)인 복권의 기대값
prizes = [1000, 500, 200, 0]
counts = [1, 2, 3, 4]
total = sum(counts)
probs = [c / total for c in counts]
expected = sum(p * v for p, v in zip(probs, prizes))
print(f"기대값: {expected}원")
# 출력: 기대값: 260.0원
```

**주의사항**  
분산 계산 시 기대값의 제곱과 기대값의 제곱의 차이를 혼동하지 않도록 주의해야 한다.

---

## 결합확률질량함수와 주변확률 🔗

**설명**  
두 개 이상의 확률변수의 결합분포는 **결합확률질량함수(jpmf)** 또는 **결합확률밀도함수(jpdf)** 로 표현한다.  
**주변확률**은 결합분포에서 한 변수에 대해 다른 변수의 값을 모두 합산(또는 적분)하여 얻는다.

**예시**

```python
# 두 개의 주사위를 던져 합이 7이 되는 확률 계산
outcomes = [(i, j) for i in range(1, 7) for j in range(1, 7)]
event = [pair for pair in outcomes if sum(pair) == 7]
prob = len(event) / len(outcomes)
print(f"합이 7이 될 확률: {prob:.2f}")
# 출력: 0.17
```

**주의사항**  
결합확률분포의 모든 값의 합은 1이어야 하며, 주변확률을 구할 때는 해당 변수 이외의 모든 값을 합산해야 한다.

---

## 결합확률밀도함수와 다변수 확장 🌐

**설명**  
연속 확률변수의 결합분포는 **결합확률밀도함수(jpdf)** 로 정의한다. 다변수의 경우, 각 변수에 대해 적분(또는 합산)하여 주변확률밀도함수를 구할 수 있다.

**예시**

```python
# 2차원 정규분포의 결합확률밀도함수 예시
import numpy as np
def bivariate_normal_pdf(x, y, mu_x=0, mu_y=0, sigma_x=1, sigma_y=1, rho=0):
    z = ((x - mu_x)**2 / sigma_x**2) + ((y - mu_y)**2 / sigma_y**2) - (2 * rho * (x - mu_x) * (y - mu_y) / (sigma_x * sigma_y))
    denom = 2 * np.pi * sigma_x * sigma_y * np.sqrt(1 - rho**2)
    return np.exp(-z / (2 * (1 - rho**2))) / denom
print(f"결합확률밀도함수(0,0): {bivariate_normal_pdf(0,0):.3f}")
```

**주의사항**  
다변수 결합분포에서 각 변수의 독립 여부에 따라 결합확률밀도함수의 형태가 달라진다.

---

## 확률변수의 독립과 조건부 독립 🔓

**설명**  
두 확률변수 X, Y가 **독립**이면, 결합확률은 각 확률의 곱으로 표현된다:  
P(X = x, Y = y) = P(X = x) × P(Y = y)  
**조건부 독립**은 제3의 변수 Z가 주어졌을 때 X와 Y가 서로 독립임을 의미한다:  
P(X, Y | Z) = P(X | Z) × P(Y | Z)

**예시**

```python
# 독립 사건 예시: 동전과 주사위 동시 실험
coin = ['앞', '뒤']
dice = [1, 2, 3, 4, 5, 6]
prob_coin = 1 / len(coin)
prob_dice = 1 / len(dice)
prob_joint = prob_coin * prob_dice
print(f"동전과 주사위가 동시에 특정 결과가 나올 확률: {prob_joint:.2f}")
# 출력: 0.08
```

**주의사항**  
조건부 독립은 반드시 조건이 주어진 상황에서만 성립하며, 무조건적 독립과 혼동하지 않도록 한다.

---

## 나이브 베이즈 분류기 🏷️

**설명**  
**나이브 베이즈 분류기**는 각 특성(feature)이 서로 독립이라는 가정 하에 베이즈 정리를 이용해 분류 확률을 계산하는 지도학습 알고리즘이다.  
분류하고자 하는 클래스 y와 특성 x₁, x₂, ..., xₙ에 대해  
P(y | x₁, ..., xₙ) ∝ P(y) × Π P(xᵢ | y)

**예시**

```python
# 가상의 데이터: 날씨와 활동 여부
data = [
    {'날씨': '맑음', '온도': '높음', '습도': '보통', '활동': '예'},
    {'날씨': '비', '온도': '낮음', '습도': '높음', '활동': '아니오'},
    {'날씨': '맑음', '온도': '낮음', '습도': '보통', '활동': '예'},
    {'날씨': '흐림', '온도': '높음', '습도': '높음', '활동': '예'},
]

# 새로운 데이터: 맑음, 높음, 보통 → '예'일 확률 계산
p_yes = 3/4
p_weather_given_yes = 2/3
p_temp_given_yes = 2/3
p_humidity_given_yes = 2/3
prob = p_yes * p_weather_given_yes * p_temp_given_yes * p_humidity_given_yes
print(f"활동=예로 분류될 확률(정규화 전): {prob:.2f}")
```

**주의사항**  
특성 간 독립 가정이 실제 데이터에 부합하지 않을 수 있으며, 데이터가 희소할 때 확률이 0이 되는 문제(제로 확률 문제)에 주의해야 한다.

---

## 마르코프 결정과정 🔄

**설명**  
**마르코프 결정과정(MDP)** 는 현재 상태가 미래 상태에 영향을 미치지만, 과거 상태에는 영향을 받지 않는 **마르코프 성질**을 가진 확률적 과정이다.  
1차 마르코프 체인에서는 다음 상태의 확률이 오직 현재 상태에만 의존한다.

**예시**

```python
# 날씨 상태 전이 확률 예시
states = ['맑음', '비', '안개']
transition = {
    '맑음': {'맑음': 0.7, '비': 0.2, '안개': 0.1},
    '비': {'맑음': 0.3, '비': 0.5, '안개': 0.2},
    '안개': {'맑음': 0.4, '비': 0.3, '안개': 0.3}
}
# 오늘 맑음, 내일 비, 모레 맑음일 확률
prob = transition['맑음']['비'] * transition['비']['맑음']
print(f"오늘 맑음→내일 비→모레 맑음 확률: {prob:.2f}")
# 출력: 0.06
```

**주의사항**  
상태 전이 확률의 합이 1이 되어야 하며, n차 마르코프 체인의 경우 필요한 통계량이 기하급수적으로 증가한다.
