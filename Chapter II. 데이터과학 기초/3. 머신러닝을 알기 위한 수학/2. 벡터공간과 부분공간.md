# 2. 벡터공간과 부분공간 ✨

## 목차 📑

### 1. 벡터 개념의 시작
- [1.1 벡터의 정의와 기하학적 의미](#벡터의-정의와-기하학적-의미-) 🏹
- [1.2 벡터의 연산과 성질](#벡터의-연산과-성질-) ➕

### 2. 벡터공간과 Rⁿ
- [2.1 벡터공간의 개념](#벡터공간의-개념-) 🌌
- [2.2 부분공간의 개념](#부분공간의-개념-) 🏛️
- [2.3 Rⁿ의 정의와 연산](#rⁿ의-정의와-연산-) 🔢
- [2.4 벡터 표기법](#벡터-표기법-) ✍️

### 3. 벡터공간의 핵심 개념
- [3.1 Span: 주어진 재료로 만들 수 있는 세상의 범위](#span-주어진-재료로-만들-수-있는-세상의-범위)
- [3.2 기저(Basis): 세상을 바라보는 기준 (좌표계)](#기저basis-세상을-바라보는-기준-좌표계)
- [3.3 직교성(Orthogonality): 서로에게 간섭하지 않는 정보](#직교성orthogonality-서로에게-간섭하지-않는-정보)
- [3.4 그람-슈미트 과정: 이상적인 좌표계 만들기](#그람-슈미트-과정-이상적인-좌표계-만들기)

### 4. 벡터의 내적과 노름
- [4.1 내적과 벡터의 크기](#내적과-벡터의-크기-) 📏
- [4.2 거리와 노름의 정의](#거리와-노름의-정의-) 📐
- [4.3 내적의 성질과 연산법칙](#내적의-성질과-연산법칙-) ⚖️
- [4.4 주요 부등식과 정리](#주요-부등식과-정리-) 🧮
- [4.5 벡터 사이의 각과 수직성](#벡터-사이의-각과-수직성-) ⦜

---

## 1. 벡터 개념의 시작

### 1.1 벡터의 정의와 기하학적 의미 🏹

**설명**  
**벡터**는 크기와 방향을 동시에 가지는 수학적 객체로, 물리학에서는 속도, 힘, 가속도 등 다양한 현상을 표현하는 데 사용됩니다. 데이터 사이언스에서는 **데이터 포인트나 피처(feature)의 집합**을 나타내는 핵심 도구입니다. 예를 들어, 고객의 나이, 소득, 구매이력을 하나의 3차원 벡터로 표현할 수 있습니다.

벡터는 일반적으로 화살표로 시각화하며 시작점과 끝점, 그리고 길이로 각각 위치와 크기를 나타냅니다. 중요한 점은 시작점의 절대적 위치가 아니라 **변위(displacement)**  자체가 벡터의 본질이라는 것입니다.

> **팁**  
> "실무에서 벡터는 단순한 숫자 배열이 아닙니다. 고객 한 명, 문서 하나, 이미지 한 장이 모두 고차원 벡터공간의 '점'으로 표현됩니다. 이 관점을 이해하면 데이터 간의 유사도, 클러스터링, 분류 문제를 기하학적으로 사고할 수 있게 됩니다. ��터를 '데이터의 좌표'로 인식하는 것이 머신러닝 이해의 출발점입니다."

**예시**
```python
# 2차원 평면에서 벡터를 시각화하는 예시
import matplotlib.pyplot as plt

start = [0, 0]
vector = [3, 2]

plt.quiver(*start, *vector, angles='xy', scale_units='xy', scale=1, color='blue')
plt.xlim(-1, 5)
plt.ylim(-1, 4)
plt.grid()
plt.title("2차원 벡터 시각화")
plt.xlabel("Feature 1 (예: 나이)")
plt.ylabel("Feature 2 (예: 소득)")
plt.show()
```

**주의사항**  
벡터는 방향성을 가지므로, 동일한 크기라도 방향이 다르면 서로 다른 벡터로 간주합니다. 평행이동으로 겹칠 수 있으면 같은 벡터로 봅니다.

---

### 1.2 벡터의 연산과 성질 ➕

**설명**  
벡터는 **덧셈**과 **스칼라 곱셈** 연산이 정의됩니다. 이 두 연산은 **선형 결합(Linear Combination)**의 기초가 되며, 머신러닝의 거의 모든 알고리즘에서 핵심적인 역할을 합니다. 두 벡터의 덧셈은 평행사변형 법칙 또는 꼬리-머리 연결법으로 시각화할 수 있습니다. 스칼라 곱셈은 벡터의 크기를 변화시키되 방향은 유지합니다(음수면 반대 방향).

> **팁**  
> "선형 결합 `c₁v₁ + c₂v₂ + ...`은 머신러닝의 DNA와 같습니다. 선형 회귀의 예측값, 뉴럴 네트워크의 각 레이어, PCA의 주성분 모두 벡터들의 선형 결합입니다. 이 기본 연산을 깊이 이해하면, 복잡해 보이는 알고리즘들이 결국 '가중합'의 변형임을 깨닫게 될 것입니다."

**예시**
```python
# 벡터 덧셈과 스칼라 곱셈 예시
import numpy as np

v1 = np.array([2, 1])
v2 = np.array([1, 3])
sum_vec = v1 + v2        # 벡터 덧셈
scaled_vec = 2 * v1      # 스칼라 곱셈

print("벡터 덧셈:", sum_vec)      # [3 4] - 두 고객 데이터의 평균적 특성
print("스칼라 곱셈:", scaled_vec) # [4 2] - 특성 값의 스케일링
```

**주의사항**  
벡터의 덧셈은 교환법칙과 결합법칙이 성립합니다. 스칼라 곱셈은 실수와 벡터의 곱으로, 크기와 방향에 영향을 줍니다.

---

## 2. 벡터공간과 Rⁿ

### 2.1 벡터공간의 개념 🌌

**설명**  
**벡터공간**은 벡터와 스칼라(실수) 사이의 두 연산(덧셈, 스칼라 곱셈)이 정의되고, 다음 8가지 공리를 만족하는 집합입니다:

**벡터공간의 8가지 공리** (V에 속한 임의의 벡터 u, v, w와 스칼라 c, d에 대해):
1. **덧셈 교환법칙**: u + v = v + u
2. **덧셈 결합법칙**: (u + v) + w = u + (v + w)  
3. **덧셈 항등원**: u + **0** = u를 만족하는 영벡터 **0**이 존재합니다.
4. **덧셈 역원**: u + (-u) = **0**을 만족하는 -u가 존재합니다.
5. **스칼라 곱셈 분배법칙 (벡터)** : c(u + v) = cu + cv
6. **스칼라 곱셈 분배법칙 (스칼라)** : (c + d)u = cu + du
7. **스칼라 곱셈 결합법칙**: c(du) = (cd)u
8. **스칼라 곱셈 항등원**: 1u = u

> **팁**  
> "이 공리들 덕분에 우리가 아는 대수적 조작이 벡터에서도 성립합니다. Rⁿ뿐만 아니라 함수 공간, 확률 분포 공간도 벡터공간이 될 수 있습니다. 이 추상화 덕분에 이미지, 텍스트, 음성 등 모든 데이터를 '벡터'라는 통일된 프레임워크로 분석할 수 있습니다. 데이터 사이언스는 본질적으로 다양한 데이터를 벡터공간으로 임베딩하여 분석하는 학문입니다."

---

### 2.2 부분공간의 개념 🏛️

**설명**
**부분공간(Subspace)**은 더 큰 벡터공간 V의 부분집합이면서, 동시에 그 자체로도 벡터공간인 집합 H를 의미합니다. 부분공간이 되기 위한 조건은 다음 세 가지입니다:

1. **영벡터 포함**: **0** ∈ H
2. **덧셈에 대해 닫힘**: u, v ∈ H ⟹ u + v ∈ H  
3. **스칼라 곱셈에 대해 닫힘**: u ∈ H, c ∈ ℝ ⟹ cu ∈ H

예를 들어, R³에서 원점을 지나는 평면이나 직선은 모두 R³의 부분공간입니다.

> **팁**  
> "부분공간은 **차원 축소(Dimensionality Reduction)**의 핵심 개념입니다. 고차원 데이터가 실제로는 저차원 부분공간 근처에 분포하는 경우가 많습니다. **PCA(주성분 분석)**는 데이터를 가장 잘 설명하는 부분공간을 찾는 기법입니다. 또한 **선형 회귀**에서 예측값들이 이루는 공간도 피처 공간의 부분공간입니다. '차원의 저주'를 극복하는 핵심 아이디어가 바로 부분공간입니다."

---

### 2.3 Rⁿ의 정의와 연산 🔢

**설명**  
**Rⁿ**은 n개의 실수로 이루어진 순서쌍(튜플)들의 집합입니다. R²는 2차원 평면, R³는 3차원 공간을 의미하며, 일반적으로 Rⁿ은 n차원 벡터공간을 나타냅니다. 데이터 사이언스에서는 n개의 피처를 가진 데이터를 표현하는 n차원 공간으로 해석됩니다.

> **팁**  
> "Rⁿ은 가장 직관적인 벡터공간이지만, 여기에만 매몰되면 안 됩니다. 벡터공간의 추상적 개념을 이해하면 함수 공간, 확률 분포 공간 등 더 복잡한 대상도 다룰 수 있습니다. 모든 것을 Rⁿ의 일반화로 생각하는 훈련을 하십시오. 텍스트는 단어 공간의 벡터, 이미지는 픽셀 공간의 벡터로 표현됩니다."

---

### 2.4 벡터 표기법 ✍️

**설명**  
벡터는 일반적으로 소문자 굵은체 또는 화살표로 표기합니다. 행벡터는 (x₁, x₂, ..., xₙ) 형태, 열벡터는 세로로 나열된 형태로 나타냅니다. 전치 연산(`.T` 또는 `⊤`)을 통해 행벡터와 열벡터를 변환할 수 있습니다.

> **팁**  
> "NumPy에서 1차원 배열은 행/열 구분이 모호합니다. `v.shape`가 `(n,)`로 나타나기 때문입니다. 이는 편리하지만 행렬 연산에서 예상치 못한 오류를 일으킬 수 있습니다. `v.reshape(-1, 1)` (열벡터) 또는 `v.reshape(1, -1)` (행벡터)로 명시적으로 차원을 지정하는 습관을 들이는 것이 좋습니다. 코드의 안정성이 크게 향상됩니다."

---

## 3. 벡터공간의 핵심 개념

### 3.1 Span: 주어진 재료로 만들 수 있는 세상의 범위

**생성(Span)**이란, 주어진 벡터(재료)들의 **일차결합(레시피)**으로 만들 수 있는 모든 결과물의 집합을 말합니다.

- 2차원 공간에서, 서로 다른 방향의 벡터 2개(`v1`, `v2`)가 있다면, 이 둘의 Span은 2차원 평면 전체가 됩니다. `a*v1 + b*v2` 조합으로 평면 위의 어떤 점이든 만들어낼 수 있기 때문입니다.
- 만약 두 벡터가 같은 방향이라면(일차종속), 그 둘의 Span은 하나의 직선에 불과합니다. 아무�� 조합해도 직선을 벗어날 수 없기 때문입니다.

> **머신러닝 관점**:
> 데이터의 특성(feature) 벡터들이 Span하는 공간은 **해당 모델이 만들어낼 수 있는 예측의 한계**를 결정합니다. 좋은 특성들을 사용해야 더 넓고 풍부한 예측 공간을 만들 수 있습니다.

---

### 3.2 기저(Basis): 세상을 바라보는 기준 (좌표계)

**기저(Basis)**는 어떤 벡터 공간을 Span하는, 즉 그 공간 전체를 만들어낼 수 있는 **최소한의 재료(벡터)들의 집합**입니다. 이 재료들은 서로 독립적이어야 합니다(일차독립).

쉽게 말해, 기저는 세상을 측정하는 **'좌표계'**입니다. 우리가 흔히 쓰는 직각 좌표계(`(1,0)`, `(0,1)`)는 수많은 기저 중 하나일 뿐이며, 세상을 바라보는 기준은 얼마든지 바뀔 수 있습니다.

> **머신러닝 관점**:
> **차원 축소(PCA)**의 본질은 기존의 복잡한 좌표계(기저)를, 데이터를 가장 잘 설명하는 **새로운 좌표계(기저)로 바꾸는 것**입니다. 새로운 좌표계에서는 단 몇 개의 축만으로도 데이터의 핵심 정보를 대부분 표현할 수 있습니다.

---

### 3.3 직교성(Orthogonality): 서로에게 간섭하지 않는 정보

**직교(Orthogonal)**한다는 것은 두 벡터가 서로 **90도**를 이룬다는 의미입니다. 기하학적으로는 '수직'이지만, 데이터 과학에서는 **'상관없음' 또는 '독립적'**이라는 의미로 해석할 수 있습니다.

- 키와 몸무게 정보는 어느 정도 상관관계가 있지만 (직교하지 않음)
- 키와 좋아하는 색깔 정보는 서로 아무 관련이 없습니다 (직교함)

직교하는 벡터(정보)들은 서로에게 아무런 영향을 주지 않기 때문에, 데이터를 분석하고 해석하기 매우 편리합니다. **정규직교기저(Orthonormal Basis)**는 모든 기저 벡터들이 서로 직교하고, 각 벡터의 크기(길이)가 1인, 가장 이상적이고 다루기 쉬운 좌표계입니다.

---

### 3.4 그람-슈미트 과정: 이상적인 좌표계 만들기

**그람-슈미트 직교화(Gram-Schmidt Orthogonalization)**는 서로 독립이기만 한 일반적인 기저(좌표계)를, 다루기 쉬운 **직교기저(Orthogonal Basis)**로 바꿔주는 매우 유용한 알고리즘입니다.

과정은 간단합니다.
1. 첫 번째 벡터는 그대로 둡니다.
2. 두 번째 벡터에서, 첫 번째 벡터와 겹치는(상관있는) 성분을 빼서 수직으로 만듭니다.
3. 세 번째 벡터에서, 첫 번째와 두 번째 벡터와 겹치는 성분들을 모두 빼서 수직으로 만듭니다.
4. 이 과정을 모든 벡터에 대해 반복합니다.

```python
# 그람-슈미트 과정으로 일반 기저를 직교 기저로 변환
def gram_schmidt(vectors):
    basis = []
    for v in vectors:
        w = v - sum(np.dot(v, b) / np.dot(b, b) * b for b in basis)
        if not np.allclose(w, 0):
            basis.append(w)
    return basis

# 일반적인 기저 (서로 수직이 아님)
general_basis = [np.array([3, 1]), np.array([2, 2])]

# 직교 기저로 변환
orthogonal_basis = gram_schmidt(general_basis)

print("일반 기저:\n", general_basis)
print("\n직교 기저로 변환:\n", orthogonal_basis)
# 변환된 두 벡터의 내적이 0에 가까운지 확인
print(f"\n직교성 확인 (내적): {np.dot(orthogonal_basis[0], orthogonal_basis[1]):.2f}")
```

> **머신러닝 관점**:
> 그람-슈미트 과정과 같은 직교화 기법은 데이터의 특성들 간의 상관관계를 제거하여, 각 특성이 독립적으로 모델에 기여하도록 만드는 **특성 공학(Feature Engineering)**에 활용될 수 있습니다.

---

## 4. 벡터의 내적과 노름

### 4.1 내적과 벡터의 크기 📏

**설명**  
**내적(Inner product)**은 두 벡터의 대응 원소를 곱한 뒤 모두 더하는 연산입니다. 내적을 통해 벡터의 크기(노름)와 두 벡터 사이의 각도를 계산할 �� 있습니다. 벡터의 크기는 자기 자신과의 내적의 제곱근으로 정의됩니다: `||v|| = sqrt(⟨v, v⟩)`.

> **팁**  
> "내적의 본질은 '투영(Projection)'과 '유사도(Similarity)'입니다. 추천 시스템에서 사용자 벡터와 아이템 벡터의 내적으로 선호도를 예측하고, 코사인 유사도는 내적을 정규화한 것입니다. Word2Vec도 단어 벡터 간의 내적으로 의미적 유사도를 측정합니다. 내적은 '얼마나 같은 방향인가?'를 수치화하는 도구입니다."

---

### 4.2 거리와 노름의 정의 📐

**설명**  
**노름(Norm)**은 벡터의 크기를 측정하는 함수이며, **거리(Metric)**는 두 점(벡터) 사이의 간격을 나타냅니다. 대표적인 노름으로 L₁, L₂, L∞ 노름이 있으며, L₂ 노름은 유클리드 거리와 동일합니다.

- **L₁ 노름 (맨해튼)** : `||v||₁ = Σ|vᵢ|` - 변화량의 절대적 크기
- **L₂ 노름 (유클리드)** : `||v||₂ = sqrt(Σvᵢ²)` - 직선 거리  
- **L∞ 노름 (최대값)** : `||v||∞ = max|vᵢ|` - 가장 큰 성분

> **팁**  
> "노름은 **정규화(Regularization)**의 핵심입니다. L₁ 정규화(Lasso)는 불필요한 피처를 0으로 만들어 **피처 선택** 효과를 가지고, L₂ 정규화(Ridge)는 가중치를 전반적으로 ���게 만들어 **과적합을 방지**합니다. 거리 함수 선택도 중요합니다. 이미지에서는 L₂, 텍스트에서는 코사인 거리, 추천시스템에서는 맨해튼 거리를 자주 사용합니다."

---

### 4.3 내적의 성질과 연산법칙 ⚖️

**설명**  
내적은 다음과 같은 성질을 가집니다:
- **선형성**: ⟨a + b, c⟩ = ⟨a, c⟩ + ⟨b, c⟩
- **대칭성**: ⟨a, b⟩ = ⟨b, a⟩  
- **양의 정부호성**: ⟨a, a⟩ ≥ 0, ⟨a, a⟩ = 0 ⇔ a = 0

이러한 성질들로부터 피타고라스 정리, 평행사변형 법칙 등이 유도됩니다.

> **팁**  
> "선형성은 복잡한 계산을 간단한 요소로 분해할 수 있게 해주는 강력한 성질입니다. **커널 트릭(Kernel Trick)**은 내적의 성질을 활용해 고차원 공간에서의 내적을 직접 계산하지 않고도 구할 수 있게 해줍니다. SVM의 핵심 아이디어입니다. 기본 성질에 대한 깊은 이해가 고급 기법의 토대가 됩니다."

---

### 4.4 주요 부등식과 정리 🧮

**설명**  
- **Cauchy-Schwarz 부등식**: 임의의 벡터 v, w에 대해 `|⟨v, w⟩| ≤ ||v||·||w||`가 성립합니다. 등호는 두 벡터가 일직선상에 있을 때만 성립합니다.
- **삼각부등식**: `||v + w|| ≤ ||v|| + ||w||`가 항상 성립합니다.
- **피타고라스 정리**: 두 벡터가 수직일 때, `||v + w||² = ||v||² + ||w||²`가 성립합니다.

> **팁**  
> "Cauchy-Schwarz 부등식은 **상관계수가 -1과 1 사이에 있는 이유**를 설명합니다. 삼각부등식은 '직접 가는 것이 돌아가는 것보다 짧다'는 거리의 직관을 수학적으로 보장합니다. 이런 기본 부등식들이 머신러닝 알고리즘의 수렴성과 안정성을 증명하는 데 핵심적인 역할을 합니다."

---

### 4.5 벡터 사이의 각과 수직성 ⦜

**설명**  
두 벡터 사이의 **각도**는 내적을 이용해 정의됩니다:  
`cos θ = ⟨v, w⟩ / (||v||·||w||)`  
이때 θ는 [0, π] 범위의 유일한 값입니다.  
특히, 내적이 0이면 두 벡터는 서로 **수직(직교)**입니다.

> **팁**  
> "수직성은 '독립성'의 기하학적 표현입니다. **PCA에서 주성분들은 서로 수직**이며, 이는 각 성분이 독립적인 분산 방향을 나타냄을 의미합니다. **선형 회귀의 정규방정식**도 오차벡터가 피처공간과 수직이 되는 지점을 찾는 것입니다. '수직'이 보이면 '최적화'나 '독립성'을 떠올리십시오."