# 2. 벡터 공간: 데이터가 살아가는 세상

## 목차
- [2. 벡터 공간: 데이터가 살아가는 세상](#2-벡터-공간-데이터가-살아가는-세상)
  - [목차](#목차)
  - [1. 벡터 공간과 부분공간: 데이터의 놀이터](#1-벡터-공간과-부분공간-데이터의-놀이터)
    - [Span: 주어진 재료로 만들 수 있는 세상의 범위](#span-주어진-재료로-만들-수-있는-세상의-범위)
  - [2. 기저(Basis): 세상을 바라보는 기준 (좌표계)](#2-기저basis-세상을-바라보는-기준-좌표계)
    - [왜 좋은 기저가 필요한가?](#왜-좋은-기저가-필요한가)
  - [3. 직교성(Orthogonality): 서로에게 간섭하지 않는 정보](#3-직교성orthogonality-서로에게-간섭하지-않는-정보)
    - [정규직교기저: 가장 이상적인 좌표계](#정규직교기저-가장-이상적인-좌표계)
    - [그람-슈미트 과정: 일반적인 좌표계를 이상적인 좌표계로 바꾸는 마법](#그람-슈미트-과정-일반적인-좌표계를-이상적인-좌표계로-바꾸는-마법)

---

## 1. 벡터 공간과 부분공간: 데이터의 놀이터

**벡터 공간(Vector Space)**  은 간단히 말해 **데이터가 존재할 수 있는 세상의 범위**입니다. 2개의 특성을 가진 데이터는 2차원 평면(벡터 공간)에 살고, 100개의 특성을 가진 데이터는 100차원 공간에 삽니다.

**부분공간(Subspace)**  은 이 전체 세상 속에서, **그 자체로 하나의 완결된 세상을 이루는 더 작은 공간**입니다. 예를 들어, 3차원 공간 속에서 원점을 지나는 평면이나 직선은 부분공간입니다. 이 평면/직선 위의 점들끼리 더하거나 스칼라 배를 해도 여전히 그 평면/직선 위에 존재하기 때문입니다.

> **머신러닝 관점**:
> 데이터셋의 모든 데이터 포인트는 보통 전체 벡터 공간을 꽉 채우지 못하고, 더 낮은 차원의 **부분공간** 근처에 분포하는 경우가 많습니다. 모델은 바로 이 부분공간의 구조를 학습하여 데이터의 패턴을 파악합니다.

### Span: 주어진 재료로 만들 수 있는 세상의 범위

**생성(Span)**  이란, 주어진 벡터(재료)들의 **일차결합(레시피)**  으로 만들 수 있는 모든 결과물의 집합입니다.

- 2차원 공간에서, 서로 다른 방향의 벡터 2개(`v1`, `v2`)가 있다면, 이 둘의 Span은 2차원 평면 전체가 됩니다. `a*v1 + b*v2` 조합으로 평면 위의 어떤 점이든 만들어낼 수 있기 때문입니다.
- 만약 두 벡터가 같은 방향이라면(일차종속), 그 둘의 Span은 하나의 직선에 불과합니다. 아무리 조합해도 직선을 벗어날 수 없죠.

> **머신러닝 관점**:
> 데이터의 특성(feature) 벡터들이 Span하는 공간은 **해당 모델이 만들어낼 수 있는 예측의 한계**를 결정합니다. 좋은 특성들을 사용해야 더 넓고 풍부한 예측 공간을 만들 수 있습니다.

```python
import numpy as np

# v1, v2는 서로 다른 방향 -> 2차원 평면 전체를 Span
v1 = np.array([1, 0])
v2 = np.array([0, 1])
# a*v1 + b*v2 로 [3, 5] 만들기
a, b = 3, 5
point1 = a*v1 + b*v2
print(f"[1,0], [0,1] 벡터로 만든 점: {point1}")


# v3, v4는 같은 방향 -> 직선밖에 만들지 못함
v3 = np.array([1, 2])
v4 = np.array([-2, -4]) # v4 = -2 * v3
# c*v3 + d*v4 로 [3, 5]를 만들 수 있을까? -> 불가능
# 결과는 항상 (c-2d, 2c-4d) 형태로, y=2x 직선 위에만 존재
point2 = 3*v3 + 5*v4
print(f"[1,2], [-2,-4] 벡터로 만든 점: {point2}, y/x = {point2[1]/point2[0]}")
```

## 2. 기저(Basis): 세상을 바라보는 기준 (좌표계)

**기저(Basis)**  는 어떤 벡터 공간을 Span하는, 즉 그 공간 전체를 만들어낼 수 있는 **최소한의 재료(벡터)들의 집합**입니다. 이 재료들은 서로 독립적이어야 합니다(일차독립).

쉽게 말해, 기저는 세상을 측정하는 **'좌표계'** 입니다. 우리가 흔히 쓰는 직각 좌표계(`(1,0)`, `(0,1)`)는 수많은 기저 중 하나일 뿐, 세상을 바라보는 기준은 얼마든지 바뀔 수 있습니다.

> **머신러닝 관점**:
> **차원 축소(PCA)**  의 본질은 기존의 복잡한 좌표계(기저)를, 데이터를 가장 잘 설명하는 **새로운 좌표계(기저)로 바꾸는 것**입니다. 새로운 좌표계에서는 단 몇 개의 축만으로도 데이터의 핵심 정보를 대부분 표현할 수 있습니다.

### 왜 좋은 기저가 필요한가?

좋은 기저(좌표계)를 사용하면 데이터의 구조를 더 단순하고 명확하게 표현할 수 있습니다. 이는 모델의 학습 효율과 성능 향상으로 직결됩니다.

## 3. 직교성(Orthogonality): 서로에게 간섭하지 않는 정보

**직교(Orthogonal)**  한다는 것은 두 벡터가 서로 **90도**를 이룬다는 의미입니다. 기하학적으로는 '수직'이지만, 데이터 과학에서는 **'상관없음' 또는 '독립적'** 이라는 의미로 해석할 수 있습니다.

- 키와 몸무게 정보는 어느 정도 상관관계가 있지만 (직교하지 않음)
- 키와 좋아하는 색깔 정보는 서로 아무 관련이 없습니다 (직교함)

직교하는 벡터(정보)들은 서로에게 아무런 영향을 주지 않기 때문에, 데이터를 분석하고 해석하기 매우 편리합니다.

### 정규직교기저: 가장 이상적인 좌표계

**정규직교기저(Orthonormal Basis)**  는 모든 기저 벡터들이 서로 직교하고, 각 벡터의 크기(길이)가 1인, 가장 이상적이고 다루기 쉬운 좌표계입니다.

- **장점**: 계산이 매우 단순해지고, 정보의 중복이 없으며, 각 축(특성)이 데이터에 얼마나 기여하는지 명확히 알 수 있습니다.

### 그람-슈미트 과정: 일반적인 좌표계를 이상적인 좌표계로 바꾸는 마법

**그람-슈미트 직교화(Gram-Schmidt Orthogonalization)**  는 서로 독립이기만 한 일반적인 기저(좌표계)를, 다루기 쉬운 **직교기저(Orthogonal Basis)**  로 바꿔주는 매우 유용한 알고리즘입니다.

과정은 간단합니다.
1. 첫 번째 벡터는 그대로 둡니다.
2. 두 번째 벡터에서, 첫 번째 벡터와 겹치는(상관있는) 성분을 빼서 수직으로 만듭니다.
3. 세 번째 벡터에서, 첫 번째와 두 번째 벡터와 겹치는 성분들을 모두 빼서 수직으로 만듭니다.
4. 이 과정을 모든 벡터에 대해 반복합니다.

```python
# 그람-슈미트 과정으로 일반 기저를 직교 기저로 변환
def gram_schmidt(vectors):
    basis = []
    for v in vectors:
        w = v - sum(np.dot(v, b) / np.dot(b, b) * b for b in basis)
        if not np.allclose(w, 0):
            basis.append(w)
    return basis

# 일반적인 기저 (서로 수직이 아님)
general_basis = [np.array([3, 1]), np.array([2, 2])]

# 직교 기저로 변환
orthogonal_basis = gram_schmidt(general_basis)

print("일반 기저:\n", general_basis)
print("\n직교 기저로 변환:\n", orthogonal_basis)
# 변환된 두 벡터의 내적이 0에 가까운지 확인
print(f"\n직교성 확인 (내적): {np.dot(orthogonal_basis[0], orthogonal_basis[1]):.2f}")
```

> **머신러닝 관점**:
> 그람-슈미트 과정과 같은 직교화 기법은 데이터의 특성들 간의 상관관계를 제거하여, 각 특성이 독립적으로 모델에 기여하도록 만드는 **특성 공학(Feature Engineering)**  에 활용될 수 있습니다.