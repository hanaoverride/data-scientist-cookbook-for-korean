# 8. 확률론: 불확실성의 과학과 머신러닝

## 목차
- [8. 확률론: 불확실성의 과학과 머신러닝](#8-확률론-불확실성의-과학과-머신러닝)
  - [목차](#목차)
  - [1. 왜 확률론이 필요한가?: 불확실성 모델링](#1-왜-확률론이-필요한가-불확실성-모델링)
  - [2. 조건부 확률: 새로운 정보를 얻었을 때](#2-조건부-확률-새로운-정보를-얻었을-때)
  - [3. 베이즈 정리: 머신러닝의 추론 방식](#3-베이즈-정리-머신러닝의-추론-방식)
    - [실전 예제: 스팸 메일 필터는 어떻게 작동하는가?](#실전-예제-스팸-메일-필터는-어떻게-작동하는가)
  - [4. 확률변수와 확률분포: 데이터의 패턴을 설명하는 모델](#4-확률변수와-확률분포-데이터의-패턴을-설명하는-모델)
    - [기대값과 분산: 분포의 핵심 요약](#기대값과-분산-분포의-핵심-요약)
  - [5. 나이브 베이즈 분류기: 단순하지만 강력한 확률 모델](#5-나이브-베이즈-분류기-단순하지만-강력한-확률-모델)

---

## 1. 왜 확률론이 필요한가?: 불확실성 모델링

머신러닝은 데이터를 기반으로 미래를 예측하거나 숨겨진 패턴을 찾습니다. 하지만 데이터에는 항상 노이즈가 섞여있고, 우리가 관찰하지 못하는 수많은 요인들이 결과에 영향을 미칩니다. 따라서 모델의 예측은 항상 불확실성을 내포합니다.

확률론은 이 불확실성을 다루는 체계적인 방법을 제공합니다.
- **사건(Event)** : '내일 비가 온다', '고객이 이탈한다' 등 우리가 관심 있는 현상
- **확률(Probability)** : 각 사건이 일어날 가능성을 0과 1 사이의 숫자로 표현

머신러닝 모델의 최종 출력값이 클래스(예: '스팸', '정상')가 아니라, 각 클래스에 속할 **확률** (예: '스팸일 확률 80%, 정상일 확률 20%')로 나오는 이유가 바로 이 때문입니다.

## 2. 조건부 확률: 새로운 정보를 얻었을 때

**조건부 확률(Conditional Probability)**  은 "어떤 사건 A가 일어났다는 조건 하에, 다른 사건 B가 일어날 확률"을 의미하며, `P(B|A)`로 표기합니다.

이 개념은 새로운 정보를 얻었을 때, 우리의 믿음(확률)을 어떻게 업데이트해야 하는지를 알려주기 때문에 매우 중요합니다.

- `P(구매)`: 어떤 고객이 우리 제품을 구매할 확률
- `P(구매 | 광고 클릭)`: 그 고객이 '광고를 클릭했다는 새로운 정보'가 주어졌을 때, 제품을 구매할 확률

당연히 `P(구매 | 광고 클릭)`은 `P(구매)`보다 높을 것입니다. 이처럼 조건부 확률은 새로운 정보를 바탕으로 더 정확한 추론을 가능하게 합니다.

## 3. 베이즈 정리: 머신러닝의 추론 방식

**베이즈 정리(Bayes' Theorem)**  는 조건부 확률을 이용하여 **사후 확률(Posterior)**  을 계산하는, 머신러닝 추론의 핵심적인 정리입니다.

`P(A|B) = P(B|A) * P(A) / P(B)`

- `P(A)`: **사전 확률(Prior)** . B라는 정보가 없을 때의 A에 대한 믿음.
- `P(A|B)`: **사후 확률 (Posterior)** . B라는 정보를 얻은 후의 A에 대한 믿음.
- `P(B|A)`: **가능도(Likelihood)** . A가 사실일 때, B라는 정보가 관찰될 확률.

베이즈 정리는 "나의 기존 믿음(Prior)과 새로운 증거(Likelihood)를 결합하여, 더 합리적인 새로운 믿음(Posterior)을 만든다"는 논리적인 추론 과정을 수학적으로 표현한 것입니다.

한편 베이즈 정리의 식을 변형하면 다음과 같이 표현할 수도 있습니다:

`P(A|B) * P(B) = P(B|A) * P(A)`

이 식은 "A가 주어졌을 때 B가 일어날 확률과, B가 주어졌을 때 A가 일어날 확률의 관계"를 보여줍니다. 즉, A와 B 사이의 상호작용을 이해하는 데 유용합니다.

### 실전 예제: 스팸 메일 필터는 어떻게 작동하는가?

'무료'라는 단어가 포함된 메일을 받았다고 가정해봅시다. 이 메일이 스팸일 확률, 즉 `P(스팸 | '무료')`는 베이즈 정리를 이용해 계산할 수 있습니다.

`P(스팸 | '무료') = P('무료' | 스팸) * P(스팸) / P('무료')`

- `P(스팸)`: 전체 메일 중 스팸 메일의 비율 (사전 확률)
- `P('무료' | 스팸)`: 스팸 메일 중에서 '무료'라는 단어가 포함될 확률 (가능도)
- `P('무료')`: 전체 메일 중에서 '무료'라는 단어가 포함될 확률

우리는 과거의 메일 데이터를 통해 우변의 확률들을 모두 계산할 수 있고, 이를 통해 좌변의 '이 메일이 스팸일 확률'을 추론할 수 있습니다.

## 4. 확률변수와 확률분포: 데이터의 패턴을 설명하는 모델

**확률변수(Random Variable)**  는 특정 확률로 발생하는 각각의 결과에 숫자 값을 부여한 것입니다. 예를 들어, '동전 던지기'라는 실험에서 '앞면=1, 뒷면=0'으로 숫자를 부여하면, 동전 던지기의 결과는 확률변수가 됩니다.

**확률분포(Probability Distribution)**  는 이 확률변수가 가질 수 있는 모든 값과, 그 값이 나타날 확률을 함께 나타낸 것입니다. 데이터가 어떤 패턴으로 분포하는지를 수학적으로 모델링한 것입니다.
- **이산 확률분포**: 주사위 던지기처럼 결과가 정수로 딱딱 떨어지는 경우 (예: 베르누이 분포, 이항분포, 포아송 분포)
- **연속 확률분포**: 키, 몸무게처럼 결과가 특정 범위 안의 어떤 값이든 될 수 있는 경우 (예: 정규분포, 균등분포)

> **머신러닝 관점**:
> 데이터의 특성(feature)들은 각각 특정 확률분포를 따르는 확률변수로 간주할 수 있습니다. 데이터의 분포를 잘 설명하는 확률분포 모델을 가정하면, 더 정교한 예측이 가능해집니다.

### 기대값과 분산: 분포의 핵심 요약

- **기대값(Expected Value)** : 확률분포의 '무게중심'. 해당 확률변수가 평균적으로 어떤 값을 가질 것인지에 대한 예측입니다.
- **분산(Variance)** : 데이터가 기대값으로부터 얼마나 넓게 퍼져있는지를 나타냅니다. 분산이 크다는 것은 불확실성이 크다는 의미입니다.

## 5. 나이브 베이즈 분류기: 단순하지만 강력한 확률 모델

**나이브 베이즈 분류기(Naive Bayes Classifier)**  는 베이즈 정리에 기반한 간단하면서도 매우 효과적인 분류 알고리즘입니다.

이름에 '나이브(Naive, 순진한)'가 붙은 이유는, 모든 특성(feature)들이 서로 **조건부 독립(conditionally independent)**  이라고 '순진하게' 가정하기 때문입니다. 즉, '스팸'이라는 조건 하에서는, 메일에 '무료'라는 단어가 포함될 확률과 '세일'이라는 단어가 포함될 확률이 서로에게 영향을 주지 않는다고 가정하는 것입니다.

이 가정 덕분에 계산이 매우 간단해지고, 데이터가 적을 때도 준수한 성능을 보입니다. 스팸 필터, 텍스트 분류, 의료 진단 등 다양한 분야에서 활용됩니다.