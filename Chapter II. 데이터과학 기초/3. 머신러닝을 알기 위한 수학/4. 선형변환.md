# 4. 행렬의 진짜 역할: 공간을 바꾸는 함수, 선형변환

## 목차
- [4. 행렬의 진짜 역할: 공간을 바꾸는 함수, 선형변환](#4-행렬의-진짜-역할-공간을-바꾸는-함수-선형변환)
  - [목차](#목차)
  - [1. 선형변환: 행렬의 동적인 역할](#1-선형변환-행렬의-동적인-역할)
    - [선형성의 두 가지 조건](#선형성의-두-가지-조건)
  - [2. 선형변환의 결과와 소실: 치역과 영공간](#2-선형변환의-결과와-소실-치역과-영공간)
    - [치역 (Column Space): 변환 후 데이터가 도착하는 세상](#치역-column-space-변환-후-데이터가-도착하는-세상)
    - [영공간 (Null Space): 변환 후 0으로 붕괴되는 벡터들](#영공간-null-space-변환-후-0으로-붕괴되는-벡터들)
  - [3. 랭크(Rank): 변환된 공간의 진짜 차원](#3-랭크rank-변환된-공간의-진짜-차원)
    - [Full Rank vs. Rank Deficient](#full-rank-vs-rank-deficient)
  - [4. 특별한 변환을 하는 특수 행렬들](#4-특별한-변환을-하는-특수-행렬들)
    - [대각 행렬 (Diagonal Matrix): 각 축을 독립적으로 스케일링](#대각-행렬-diagonal-matrix-각-축을-독립적으로-스케일링)
    - [직교 행렬 (Orthogonal Matrix): 공간을 왜곡 없이 회전](#직교-행렬-orthogonal-matrix-공간을-왜곡-없이-회전)

---

## 1. 선형변환: 행렬의 동적인 역할

**선형변환**은 벡터를 입력받아 다른 벡터로 출력하는 '함수'입니다. 그리고 모든 선형변환은 **행렬 곱셈**으로 표현할 수 있습니다. 즉, **`행렬 A`는 '벡터를 입력받아 `Ax`라는 새로운 벡터로 변환하는 함수'** 라고 볼 수 있는것입니다.

예를 들어, 2차원 공간의 모든 벡터를 90도 회전시키는 행위는 하나의 선형변환이며, 이는 특정 2x2 행렬을 곱하는 것으로 표현할 수 있습니다. 과거 기하와 벡터 교육과정에서는 cos, sin을 이용해 회전 행렬을 직접 계산하는 식으로 표현했는데, 그것 또한 선형변환의 일종입니다.

### 선형성의 두 가지 조건

어떤 변환(함수 `f`)이 선형변환이 되려면, 다음 두 가지 규칙을 만족해야 합니다.
1.  `f(v + w) = f(v) + f(w)` (각각을 변환한 뒤 더한 것과, 먼저 더한 뒤 변환한 결과가 같다)
2.  `f(c * v) = c * f(v)` (벡터를 c배 한 뒤 변환한 것과, 변환한 뒤 c배 한 결과가 같다)

이 조건은 변환 후에도 **공간의 격자선이 평행하고 균일한 상태를 유지**함을 의미합니다. 신경망의 각 레이어에서 일어나는 행렬 곱셈이 바로 이 선형변환에 해당합니다.

## 2. 선형변환의 결과와 소실: 치역과 영공간

행렬이라는 함수가 공간 전체를 변환시킬 때, 그 결과를 살펴보는 것은 매우 중요합니다.

### 치역 (Column Space): 변환 후 데이터가 도착하는 세상

**치역(Column Space 또는 Range)**  은 입력 공간의 모든 벡터들이 변환 후 **도착할 수 있는 모든 결과 벡터들의 집합**입니다. 즉, **변환의 결과로 만들어지는 공간**입니다.

치역은 행렬의 **열(column) 벡터**들이 생성(Span)하는 공간과 같습니다. 그래서 Column Space라고 부릅니다.

> **머신러닝 관점**:
> 모델의 예측값들은 항상 이 **치역** 안에 존재합니다. 만약 실제 정답이 모델의 치역 바깥에 있다면, 그 모델은 절대로 정답을 맞출 수 없습니다. 좋은 모델은 실제 정답이 존재하는 공간을 최대한 포함하는 넓은 치역을 가져야 합니다.

### 영공간 (Null Space): 변환 후 0으로 붕괴되는 벡터들

**영공간(Null Space)**  은 변환 후 **결과가 0벡터가 되어버리는 입력 벡터들의 집합**입니다. 즉, `Ax = 0`을 만족하는 모든 `x`들의 공간입니다.

영공간에 속한 벡터들은 변환 과정에서 모든 정보를 잃고 원점(0)으로 붕괴됩니다.

> **머신러닝 관점**:
> 영공간이 크다는 것은, 서로 다른 입력 데이터임에도 불구하고 모델이 동일한 예측(0)을 내놓는 경우가 많다는 의미입니다. 이는 모델이 데이터의 차이를 구분해내지 못하고 정보를 손실하고 있음을 시사합니다.

## 3. 랭크(Rank): 변환된 공간의 진짜 차원

**랭크(Rank)**  는 선형변환 후 결과 공간, 즉 **치역(Column Space)의 차원**을 의미합니다.

- 3x3 행렬의 랭크가 3이라면, 3차원 공간을 변환해도 여전히 3차원 공간을 만듭니다. (정보 손실 없음)
- 3x3 행렬의 랭크가 2라면, 3차원 공간을 변환했더니 2차원 평면으로 찌그러졌다는 의미입니다.
- 3x3 행렬의 랭크가 1이라면, 3차원 공간이 1차원 직선으로 붕괴되었다는 의미입니다.

결국, 랭크는 **"이 행렬(변환)이 얼마나 많은 차원의 정보를 보존하는가?"**  를 나타내는 가장 중요한 지표입니다.

```python
import numpy as np

# 랭크 2 행렬: 3차원 공간을 2차원 평면으로 변환
A = np.array([
    [1, 0, 1],
    [0, 1, 1],
    [0, 0, 0] # 이 행 때문에 3차원 정보를 모두 표현 못함
])

# 랭크 1 행렬: 모든 열이 서로의 배수 -> 1차원 직선으로 변환
B = np.array([
    [1, 2, 3],
    [2, 4, 6],
    [3, 6, 9]
])

print(f"행렬 A의 랭크: {np.linalg.matrix_rank(A)}")
print(f"행렬 B의 랭크: {np.linalg.matrix_rank(B)}")
```

### Full Rank vs. Rank Deficient

- **Full Rank**: 행렬이 가질 수 있는 최대 랭크를 갖는 경우. 정보 손실이 최소화되는 가장 이상적인 상태입니다.
- **Rank Deficient**: 랭크가 최대 랭크보다 작은 경우. 이는 행렬의 열(또는 행)들 간에 중복된 정보(선형 종속)가 있음을 의미하며, 변환 시 차원 축소가 일어납니다.

## 4. 특별한 변환을 하는 특수 행렬들

### 대각 행렬 (Diagonal Matrix): 각 축을 독립적으로 스케일링

대각 행렬은 주대각선 원소를 제외한 모든 원소가 0인 행렬입니다. 이를 곱하는 변환은, 공간을 회전시키지 않고 **각 좌표축 방향으로만 독립적으로 늘리거나 줄이는** 변환입니다.

```python
# x축으로 3배, y축으로 0.5배 스케일링하는 변환
scaler = np.array([
    [3, 0],
    [0, 0.5]
])
point = np.array([1, 2])
scaled_point = scaler @ point
print(f"원본: {point}, 변환 후: {scaled_point}")
```

### 직교 행렬 (Orthogonal Matrix): 공간을 왜곡 없이 회전

직교 행렬은 모든 열(또는 행) 벡터들이 서로 직교하고 크기가 1인 행렬입니다. 이를 곱하는 변환은 공간의 형태를 전혀 왜곡시키지 않고, 오직 **회전(Rotation)**  또는 **반사(Reflection)**  만 시킵니다.
- 벡터의 **길이**가 보존됩니다.
- 벡터 간의 **각도**가 보존됩니다.
- 역변환(원래대로 되돌리기)이 매우 쉽습니다. (역행렬 = 전치행렬)

> **머신러닝 관점**:
> 직교 행렬은 데이터의 구조적 손실 없이 정보를 처리할 수 있기 때문에, PCA나 신경망의 특정 레이어 등에서 매우 중요하게 사용됩니다.