# 9. 일변수 함수 미분법: 모델을 최적화하는 언어

### 목차
- [9. 일변수 함수 미분법: 모델을 최적화하는 언어](#9-일변수-함수-미분법-모델을-최적화하는-언어)
    - [목차](#목차)
  - [**1. 왜 데이터 사이언티스트에게 미분이 필요한가?**](#1-왜-데이터-사이언티스트에게-미분이-필요한가)
  - [**2. 미분: 손실 함수의 '경사'를 찾는 도구**](#2-미분-손실-함수의-경사를-찾는-도구)
    - [**2.1. 미분의 정의: 순간 변화율**](#21-미분의-정의-순간-변화율)
    - [**2.2. 기본 미분 공식: 계산의 기초**](#22-기본-미분-공식-계산의-기초)
    - [**2.3. 미분 법칙: 복잡한 함수를 다루는 법**](#23-미분-법칙-복잡한-함수를-다루는-법)
  - [**3. 연쇄 법칙(Chain Rule): 딥러닝의 심장을 관통하는 원리**](#3-연쇄-법칙chain-rule-딥러닝의-심장을-관통하는-원리)
  - [**4. 2차 도함수와 테일러 급수: 더 빠르고 정확한 최적화**](#4-2차-도함수와-테일러-급수-더-빠르고-정확한-최적화)
    - [**4.1. 2차 도함수: '볼록성'과 극값 판별**](#41-2차-도함수-볼록성과-극값-판별)
    - [**4.2. 테일러 급수: 함수를 다항식으로 근사하기**](#42-테일러-급수-함수를-다항식으로-근사하기)

---
<br>

## **1. 왜 데이터 사이언티스트에게 미분이 필요한가?**

결론부터 말하자면, **모델의 성능을 최적화하기 위해서**입니다.

머신러닝 모델 학습의 본질은 **손실 함수(Loss Function)** 의 값을 최소화하는 파라미터(가중치)를 찾는 과정입니다. 손실 함수는 모델의 예측이 실제 값과 얼마나 차이 나는지를 나타내는 지표로, 이 값이 작을수록 모델의 성능이 좋다고 말할 수 있습니다.

여기서 **미분**이 등장합니다. 미분은 특정 지점에서 함수의 **순간 변화율**, 즉 **기울기(Gradient)** 를 알려줍니다. 손실 함수의 기울기를 알면, 함수 값이 가장 가파르게 감소하는 방향으로 파라미터를 업데이트할 수 있습니다. 이 과정을 반복하여 손실 함수의 최솟값에 도달하는 기법이 바로 **경사 하강법(Gradient Descent)** 입니다.

- **미분(Derivative)**: 손실 함수의 특정 지점에서 어느 방향으로 가야 손실이 줄어드는지 알려주는 '나침반' 역할을 합니다.
- **경사 하강법(Gradient Descent)**: 그 나침반을 보고 한 걸음씩 최저점을 향해 걸어가는 과정입니다.

이것이 우리가 미분을 배우는 단 하나의, 그러나 가장 중요한 이유입니다.

---

## **2. 미분: 손실 함수의 '경사'를 찾는 도구**

### **2.1. 미분의 정의: 순간 변화율**

**미분 계수(Derivative)** 는 함수 $ f(x) $ 위의 한 점 $(a, f(a))$에서의 **접선의 기울기**를 의미합니다. 기하학적으로는 '어느 한 순간의 변화량'을 나타내죠.

수식적으로는 x가 a에서 h만큼 아주 조금 변할 때의 평균 변화율에서 h를 0에 가깝게 보내 극한을 취한 값입니다.

$
f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}
$

이 $ f'(a) $가 바로 손실 함수의 특정 지점에서 우리가 나아가야 할 방향과 강도를 알려주는 '경사(Gradient)' 정보가 됩니다.

```python
# f(x) = x^2 함수의 x=3 에서의 미분 계수(기울기)를 코드로 확인해봅시다.
# 수치 미분(Numerical Differentiation)을 이용한 근사 계산입니다.

def f(x):
    return x**2

def numerical_derivative(f, x, h=1e-5):
    """
    아주 작은 h를 이용해 특정 지점 x에서의 순간 변화율을 근사합니다.
    """
    return (f(x + h) - f(x)) / h

# x=3 에서의 기울기는 2*3 = 6 입니다. 코드의 결과와 비교해보세요.
gradient_at_3 = numerical_derivative(f, 3)
print(f"f(x) = x^2 의 x=3 지점에서의 기울기(미분 계수): {gradient_at_3:.4f}")

# 출력: f(x) = x^2 의 x=3 지점에서의 기울기(미분 계수): 6.0000
```

> **팁:**
>
> 실제 프로젝트에서는 위와 같은 수치 미분을 직접 구현하지 않습니다. 부동소수점 오차와 계산 비효율성 때문이죠. PyTorch, TensorFlow 같은 딥러닝 프레임워크는 이 과정을 훨씬 정교하고 효율적으로 처리하는 **자동 미분(Automatic Differentiation)** 기능을 내장하고 있습니다. 하지만 그 내부 원리를 이해하기 위해 우리는 지금 미분을 배우고 있는 겁니다.

### **2.2. 기본 미분 공식: 계산의 기초**

자주 사용되는 함수의 도함수는 공식으로 기억해두는 것이 효율적입니다.

| 함수 $f(x)$ | 도함수 $f'(x)$ | 머신러닝 활용 예시 |
| :--- | :--- | :--- |
| 상수 $c$ | 0 | |
| $x^n$ | $n \cdot x^{n-1}$ | 다항 회귀(Polynomial Regression) |
| $\ln(x)$ | $\frac{1}{x}$ | 로그 변환, KL Divergence |
| $e^x$ | $e^x$ | 지수적 성장 모델, Softmax |
| $\sigma(x) = \frac{1}{1+e^{-x}}$ | $\sigma(x)(1-\sigma(x))$ | **시그모이드(Sigmoid)**, 로지스틱 회귀 |

### **2.3. 미분 법칙: 복잡한 함수를 다루는 법**

복잡한 함수는 기본 함수의 조합으로 이루어집니다. 이들을 미분하기 위한 규칙입니다.

- **선형 법칙**: $(c \cdot f(x) + d \cdot g(x))' = c \cdot f'(x) + d \cdot g'(x)$
- **곱셈 법칙**: $(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$

---

## **3. 연쇄 법칙(Chain Rule): 딥러닝의 심장을 관통하는 원리**

> **주의:**
>
> 이 파트를 이해하지 못하면 딥러닝을 절대 깊이 있게 이해할 수 없습니다. **역전파(Backpropagation)** 알고리즘은 연쇄 법칙 그 자체이기 때문입니다. 다른 모든 공식을 잊더라도 연쇄 법칙만큼은 반드시 정복해야 합니다.

**연쇄 법칙**은 '함수 안에 또 다른 함수가 있는' **합성 함수**를 미분하는 규칙입니다.

함수 $y = f(g(x))$가 있을 때, 이 함수의 도함수는 **'겉미분'과 '속미분'의 곱**으로 표현됩니다.

$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} \quad \text{where } u = g(x)
$

이를 라이프니츠 표기법이 아닌 프라임 표기법으로 쓰면 다음과 같습니다.

$
(f(g(x)))' = f'(g(x)) \cdot g'(x)
$

**딥러닝에서의 의미:**
신경망은 입력부터 출력까지 수많은 함수(레이어)가 중첩된 거대한 합성 함수입니다. 최종 출력(손실 값)에 대한 각 파라미터의 기울기를 계산하려면, 출력층에서부터 입력층까지 연쇄 법칙을 연쇄적으로 적용하며 미분 값을 전달해야 합니다. 이것이 바로 **역전파(Backpropagation)**의 핵심 원리입니다.

```python
# y = (3x + 1)^2 을 연쇄 법칙으로 미분해봅시다.
# f(u) = u^2 (겉함수), g(x) = 3x + 1 (속함수)
# f'(u) = 2u, g'(x) = 3
# y' = f'(g(x)) * g'(x) = 2(3x+1) * 3 = 18x + 6

def g(x): # 속함수
    return 3 * x + 1

def f(u): # 겉함수
    return u**2

def model(x): # 합성함수
    return f(g(x))

# x=1 에서의 기울기: 18*1 + 6 = 24
# 연쇄 법칙을 이용한 계산
gradient_chain_rule = numerical_derivative(f, g(1)) * numerical_derivative(g, 1)
print(f"연쇄 법칙으로 계산한 기울기 at x=1: {gradient_chain_rule:.4f}")

# 모델 전체를 직접 미분한 결과와 비교
gradient_direct = numerical_derivative(model, 1)
print(f"직접 계산한 기울기 at x=1: {gradient_direct:.4f}")

# 출력:
# 연쇄 법칙으로 계산한 기울기 at x=1: 24.0000
# 직접 계산한 기울기 at x=1: 24.0000
```

---

## **4. 2차 도함수와 테일러 급수: 더 빠르고 정확한 최적화**

### **4.1. 2차 도함수: '볼록성'과 극값 판별**

**2차 도함수($f''(x)$)** 는 도함수의 도함수로, **함수의 곡률(curvature) 또는 '볼록성'** 에 대한 정보를 줍니다.

- $f''(x) > 0$: 함수가 아래로 볼록(Convex). 이 지점은 **극소점(local minimum)** 일 가능성이 높습니다. (손실 함수에서 우리가 원하는 지점)
- $f''(x) < 0$: 함수가 위로 볼록(Concave). 이 지점은 **극대점(local maximum)** 일 가능성이 높습니다.
- $f''(x) = 0$: 변곡점일 수 있으며, 추가적인 판별이 필요합니다.

경사 하강법으로 찾은 지점($f'(x)=0$)이 손실을 최소화하는 지점인지, 오히려 최대화하는 지점인지 판별할 때 2차 도함수가 사용됩니다.

### **4.2. 테일러 급수: 함수를 다항식으로 근사하기**

**테일러 급수(Taylor Series)** 는 복잡한 함수를 특정 지점 근처에서 다항 함수의 합으로 근사하는 방법입니다. 특히 1차항까지 근사하는 것을 **선형 근사**, 2차항까지 근사하는 것을 **2차 근사**라고 합니다.

$
f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dots
$

- **경사 하강법**은 손실 함수를 각 지점에서 **1차 근사(선형 근사)** 하여 최적점을 찾아가는 방법입니다.
- **뉴턴 방법(Newton's Method)** 같은 더 발전된 최적화 알고리즘은 손실 함수를 **2차 근사(이차 함수)** 하여 훨씬 더 빠르고 정확하게 최저점을 찾아냅니다. 2차 도함수 정보(헤시안 행렬)를 사용하기 때문입니다.

> **팁:**
>
> 모든 최적화 알고리즘은 '어떻게 함수를 근사하여 다음 스텝을 결정할 것인가'에 대한 답입니다. 경사 하강법이 왜 때로는 비효율적인지, 뉴턴 방법이 왜 강력한지를 이해하려면 테일러 급수 관점에서 이들을 바라볼 수 있어야 합니다. 이것이 바로 이론의 힘입니다.
