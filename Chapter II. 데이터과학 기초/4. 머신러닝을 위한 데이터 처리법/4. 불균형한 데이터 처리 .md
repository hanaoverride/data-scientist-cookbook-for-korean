# 4. 불균형한 데이터 처리 ⚖️

## 목차 📑

### 1. 데이터 불균형
- [1.1 데이터 불균형 개념](#데이터-불균형-개념-) 📉
- [1.2 데이터 불균형이 미치는 영향](#데이터-불균형이-미치는-영향-) ⚠️

### 2. 데이터 불균형 처리
- [2.1 언더샘플링](#언더샘플링-) 🗑️
  - [랜덤 샘플링](#랜덤-샘플링)
  - [Tomek Links](#tomek-links)
  - [CNN Rule](#cnn-rule)
- [2.2 오버샘플링](#오버샘플링-) ➕
  - [Resampling](#resampling)
  - [SMOTE](#smote)
  - [Borderline SMOTE](#borderline-smote)

---

## 데이터 불균형 개념 📉

**데이터 불균형**이란 분류 문제에서 특정 클래스(예: 긍정/부정, 정상/이상)의 데이터 수가 현저히 차이나는 현상을 의미한다. 이는 실제로 추론하고자 하는 데이터의 분포와 학습 데이터의 분포가 다를 때 발생한다. 이러한 불균형은 **머신러닝 모델의 성능 저하**와 **과적합**을 유발할 수 있으므로, 적절한 처리 방법이 필요하다.

```python
# 불균형 데이터 예시 생성
import pandas as pd

# 클래스 0이 95%, 클래스 1이 5%인 데이터프레임 생성
data = {
    'feature1': [1.2, 2.3, 1.8, 2.1, 1.5, 2.2, 1.7, 2.0, 1.9, 2.4, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9],
    'label':    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
}
df = pd.DataFrame(data)
print(df['label'].value_counts())
```

**출력 예시**:
```
0    19
1     1
Name: label, dtype: int64
```

> **⚠️ 주의사항**: 데이터 불균형이 심할 경우, 모델이 소수 클래스의 패턴을 제대로 학습하지 못하고 다수 클래스에 치우친 예측을 하게 된다.

---

## 데이터 불균형이 미치는 영향 ⚠️

데이터 불균형이 존재하면 **모델의 일반화 성능**이 저하되고, 특히 소수 클래스에 대한 **재현율(Recall)** 이나 **정밀도(Precision)** 가 낮아질 수 있다. 또한, 불균형 데이터는 **과적합**을 유발하여 실제 서비스 환경에서 예측력이 떨어질 수 있다.

### 주요 영향

- **과적합**: 소수 클래스의 데이터가 적어 모델이 다수 클래스에만 최적화됨
- **평가지표 왜곡**: 단순 정확도(Accuracy)만으로 모델 성능을 평가하면 오해가 발생할 수 있음
- **실제 업무 적용 한계**: 소수 클래스가 중요한 경우(예: 이상 탐지, 질병 진단 등) 문제 발생

---

## 언더샘플링 🗑️

**언더샘플링(Undersampling)** 은 다수 클래스의 데이터를 줄여서 클래스 간의 비율을 맞추는 방법이다. 데이터가 충분히 많을 때 주로 사용되며, 정보 손실이 발생할 수 있다.

### 랜덤 샘플링

설명:  
**랜덤 샘플링**은 다수 클래스에서 임의로 데이터를 선택하여 소수 클래스와 데이터 수를 맞추는 방법이다. 가장 간단한 언더샘플링 방식으로, 데이터의 대표성이 떨어질 수 있다.

```python
import pandas as pd

# 예시 데이터 생성
df = pd.DataFrame({
    'feature': range(100),
    'label': [0]*90 + [1]*10
})

# 다수 클래스(0)에서 10개만 무작위 추출
majority = df[df['label'] == 0].sample(n=10, random_state=42)
minority = df[df['label'] == 1]
balanced = pd.concat([majority, minority])
print(balanced['label'].value_counts())
```

**출력 예시**:
```
0    10
1    10
Name: label, dtype: int64
```

**주의사항**:  
랜덤 샘플링은 중요한 정보가 포함된 데이터를 제거할 수 있으므로, 데이터의 다양성이 손실될 수 있다.

---

### Tomek Links

설명:  
**Tomek Links**는 서로 다른 클래스에 속한 두 샘플이 서로 가장 가까운 이웃일 때, 이 두 샘플을 연결하는 것을 의미한다. 이 방법은 다수 클래스의 경계에 위치한 데이터를 제거하여 클래스 간 경계를 명확히 한다.

```python
from imblearn.under_sampling import TomekLinks
import numpy as np

# 예시 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [8, 9], [9, 10], [10, 11]])
y = np.array([0, 0, 0, 1, 1, 1])

tl = TomekLinks()
X_res, y_res = tl.fit_resample(X, y)
print("Resampled class distribution:", dict(zip(*np.unique(y_res, return_counts=True))))
```

**출력 예시**:
```
Resampled class distribution: {0: 2, 1: 3}
```

**주의사항**:  
데이터셋이 크면 계산 시간이 오래 걸릴 수 있으며, 경계 데이터가 너무 많이 제거되면 오히려 성능이 저하될 수 있다.

---

### CNN Rule

설명:  
**CNN Rule(Condensed Nearest Neighbor)** 은 소수 클래스와 가까운 다수 클래스 샘플만 남기고 나머지를 제거하는 방식이다. 이 방법은 데이터의 주요 경계 샘플을 유지하여, 데이터의 구조와 패턴을 보존한다.

```python
from imblearn.under_sampling import CondensedNearestNeighbour

# 예시 데이터 생성
X = np.array([[1, 1], [1, 2], [2, 2], [8, 8], [9, 9], [10, 10]])
y = np.array([0, 0, 0, 1, 1, 1])

cnn = CondensedNearestNeighbour(random_state=0)
X_res, y_res = cnn.fit_resample(X, y)
print("Resampled shape:", X_res.shape)
```

**출력 예시**:
```
Resampled shape: (4, 2)
```

**주의사항**:  
CNN Rule은 경계 샘플만 남기므로, 데이터의 전체 분포를 반영하지 못할 수 있다.

---

## 오버샘플링 ➕

**오버샘플링(Oversampling)** 은 소수 클래스의 데이터를 인위적으로 늘려서 데이터의 불균형을 해소하는 방법이다. 데이터가 부족할 때 주로 사용된다.

### Resampling

설명:  
**Resampling**은 소수 클래스의 데이터를 무작위로 복제하여 다수 클래스와 데이터 수를 맞추는 방식이다. 간단하지만, 중복 데이터로 인해 과적합이 발생할 수 있다.

```python
from sklearn.utils import resample
import pandas as pd

# 예시 데이터 생성
df = pd.DataFrame({
    'feature': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'label':   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
})

# 소수 클래스(1) 복제
minority = df[df['label'] == 1]
minority_upsampled = resample(minority, replace=True, n_samples=5, random_state=42)
balanced = pd.concat([df[df['label'] == 0], minority_upsampled])
print(balanced['label'].value_counts())
```

**출력 예시**:
```
0    5
1    5
Name: label, dtype: int64
```

**주의사항**:  
단순 복제는 데이터의 다양성을 보장하지 못하며, 모델이 복제된 데이터를 과도하게 학습할 수 있다.

---

### SMOTE

설명:  
**SMOTE(Synthetic Minority Over-sampling Technique)** 는 K-최근접 이웃(K-NN) 알고리즘을 활용해 소수 클래스의 데이터를 인공적으로 생성하는 방법이다. 기존 데이터를 단순 복제하는 것이 아니라, 새로운 데이터를 생성하여 과적합 문제를 완화할 수 있다.

```python
from imblearn.over_sampling import SMOTE
import numpy as np

# 예시 데이터 생성
X = np.array([[2, 3], [3, 4], [4, 5], [10, 10], [11, 11]])
y = np.array([0, 0, 0, 1, 1])

smote = SMOTE(random_state=1)
X_res, y_res = smote.fit_resample(X, y)
print("Resampled class distribution:", dict(zip(*np.unique(y_res, return_counts=True))))
```

**출력 예시**:
```
Resampled class distribution: {0: 3, 1: 3}
```

**주의사항**:  
SMOTE로 생성된 데이터는 기존 분포와 다를 수 있으므로, 데이터의 특성을 잘 파악한 후 적용해야 한다.

---

### Borderline SMOTE

설명:  
**Borderline SMOTE**는 소수 클래스 중에서도 경계에 위치한 샘플에 SMOTE를 적용하는 방법이다. 이를 통해 모델이 경계 영역에서 더 잘 작동하도록 할 수 있지만, 노이즈가 증폭될 위험이 있다.

```python
from imblearn.over_sampling import BorderlineSMOTE

# 예시 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 4], [8, 9], [9, 10], [10, 11]])
y = np.array([0, 0, 0, 1, 1, 1])

bsmote = BorderlineSMOTE(random_state=0)
X_res, y_res = bsmote.fit_resample(X, y)
print("Resampled class distribution:", dict(zip(*np.unique(y_res, return_counts=True))))
```

**출력 예시**:
```
Resampled class distribution: {0: 3, 1: 3}
```

**주의사항**:  
경계에 위치한 데이터에 집중하여 노이즈가 증가할 수 있으므로, 데이터 품질을 사전에 점검해야 한다.

---

## 결측치 및 이상치 처리 참고

불균형 데이터 처리 전, **결측치**와 **이상치**를 적절히 처리해야 한다. 결측치는 평균, 중앙값, 예측값 등으로 대체하거나 제거할 수 있으며, 이상치는 변환 또는 제거 방법을 활용한다.

```python
import pandas as pd
import numpy as np

# 결측치 처리 예시
df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [4, 5, np.nan]})
df_filled = df.fillna(df.mean())
print(df_filled)

# 이상치 처리 예시
data = [10, 12, 13, 100]  # 100은 이상치
filtered = [x for x in data if x < 50]
print(filtered)
```

**주의사항**:  
결측치와 이상치 처리 방법에 따라 데이터 분포가 달라질 수 있으므로, 분석 목적에 맞게 신중히 선택해야 한다.

---

**요약**:  
불균형 데이터는 머신러닝 모델의 성능에 큰 영향을 미치므로, **언더샘플링**과 **오버샘플링** 등 다양한 방법을 활용하여 데이터 분포를 조정해야 한다. 각 방법의 장단점과 데이터 특성을 고려하여 적절한 기법을 선택하는 것이 중요하다.