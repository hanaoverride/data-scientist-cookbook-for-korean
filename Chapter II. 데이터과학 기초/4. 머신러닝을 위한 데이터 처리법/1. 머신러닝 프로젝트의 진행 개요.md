# 1. 머신러닝 프로젝트의 진행 개요 📚

## 목차 📑

### 1. 머신러닝 프로젝트 개요
- [1.1 전체 프로세스](#전체-프로세스-) 🛤️
- [1.2 데이터 수집](#데이터-수집-) 📥
- [1.3 탐색적 데이터 분석(EDA)](#탐색적-데이터-분석eda-) 🔍
- [1.4 모델 선택](#모델-선택-) 🏗️
- [1.5 데이터 전처리](#데이터-전처리-) 🧹
  - [데이터 분리](#데이터-분리)
- [1.6 모델 학습](#모델-학습-) 🧠
- [1.7 평가 및 예측](#평가-및-예측-) 📏
  - [과적합과 교차 검증](#과적합과-교차-검증)

### 2. 탐색적 데이터 분석(EDA)
- [2.1 EDA 개념](#eda-개념-) 🔎
- [2.2 데이터 확인 및 정제](#데이터-확인-및-정제-) 📝

### 3. 결측치 처리
- [3.1 결측치 개념](#결측치-개념-) 🚫
- [3.2 결측치 확인](#결측치-확인-) 🔦
- [3.3 결측치 처리 방법](#결측치-처리-방법-) 🧩

### 4. 이상치 처리
- [4.1 이상치 개념](#이상치-개념-) ⚠️
- [4.2 이상치 확인](#이상치-확인-) 📊
- [4.3 이상치 처리 방법](#이상치-처리-방법-) 🛠️

### 5. 특성 엔지니어링
- [5.1 특성 엔지니어링 개념](#특성-엔지니어링-개념-) 🧬
- [5.2 특성 생성](#특성-생성-) 🏗️
- [5.3 특성 선택](#특성-선택-) 🎯
- [5.4 특성 변환](#특성-변환-) 🔄

### 6. 상관관계 분석
- [6.1 상관관계 개념](#상관관계-개념-) 🔗
- [6.2 상관관계 분석 방법](#상관관계-분석-방법-) 📈

---

## 전체 프로세스 🛤️

머신러닝 프로젝트는 **데이터 수집**부터 **모델 배포**까지 일련의 단계를 거쳐 진행된다. 각 단계는 데이터의 품질과 모델의 성능에 직접적인 영향을 미치므로 체계적으로 수행해야 한다.

### 주요 단계

1. **데이터 수집**: 분석 목적에 맞는 데이터를 확보한다.
2. **탐색적 데이터 분석(EDA)**: 데이터의 구조와 특성을 파악한다.
3. **데이터 전처리**: 결측치, 이상치 처리 등 데이터 품질을 높인다.
4. **모델 선택**: 다양한 알고리즘 중 최적의 모델을 선정한다.
5. **모델 학습**: 훈련 데이터를 이용해 모델을 학습시킨다.
6. **평가 및 예측**: 테스트 데이터로 성능을 평가하고, 실제 예측에 활용한다.
7. **배포**: 성능이 검증된 모델을 실제 환경에 적용한다.

> **⚠️ 주의사항**: 각 단계는 순차적으로 반복될 수 있으며, 데이터의 품질이 낮으면 전체 프로젝트 결과에 부정적 영향을 준다.

---

## 데이터 수집 📥

**데이터 수집**은 머신러닝 프로젝트의 출발점으로, 신뢰할 수 있는 데이터를 충분히 확보하는 것이 중요하다. 데이터의 양과 다양성은 모델의 예측 성능과 일반화 능력에 큰 영향을 미친다.

### 설명

- 데이터는 데이터베이스, 웹 크롤링, 공공 데이터 등 다양한 경로를 통해 수집할 수 있다.
- 수집된 데이터의 품질이 낮으면 이후 단계에서 많은 문제를 야기할 수 있다.

### 예시

```python
import pandas as pd

# 예시: 웹에서 CSV 데이터 다운로드
# pd.read_csv는 하이퍼링크를 지원
url = "https://example.com/sample_data.csv"
data = pd.read_csv(url)
print(data.head())
```

### 주의사항

- 데이터의 출처와 신뢰성을 반드시 확인해야 한다.
- 개인정보 등 민감한 정보가 포함된 경우 적절한 보안 조치가 필요하다.

---

## 탐색적 데이터 분석(EDA) 🔍

**EDA(Exploratory Data Analysis)** 는 데이터를 깊이 있게 이해하고, 변수 간의 관계와 패턴을 파악하는 과정이다.

### 설명

- 데이터의 분포, 변수의 특성, 이상치 및 결측치 존재 여부 등을 시각화와 통계적 방법으로 탐색한다.
- EDA 결과를 바탕으로 문제 정의와 모델링 방향을 구체화할 수 있다.

### 예시

```python
import pandas as pd
import matplotlib.pyplot as plt

# 임의의 데이터프레임 생성
df = pd.DataFrame({
    '나이': [23, 35, 29, 41, 38, 27],
    '점수': [88, 92, 85, 90, 87, 95]
})

# 변수별 분포 시각화
plt.hist(df['나이'], bins=5)
plt.title('나이 분포')
plt.xlabel('나이')
plt.ylabel('빈도')
plt.show()
```

### 주의사항

- EDA 과정에서 발견된 문제(이상치, 결측치 등)는 반드시 전처리 단계에서 해결해야 한다.
- 데이터의 특성을 잘못 해석하면 잘못된 모델링으로 이어질 수 있다.

---

## 모델 선택 🏗️

**모델 선택**은 다양한 머신러닝 알고리즘 중에서 주어진 데이터와 문제에 가장 적합한 모델을 선정하는 단계이다.

### 설명

- 여러 모델을 비교 평가하여 예측 성능이 가장 우수한 모델을 선택한다.
- 모델의 일반화 능력과 과적합 여부를 반드시 고려해야 한다.

### 예시

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# 임의의 데이터
X = [[1, 2], [2, 3], [3, 4], [4, 5]]
y = [0, 1, 0, 1]

# 두 가지 모델 학습 및 평가
model1 = LogisticRegression().fit(X, y)
model2 = RandomForestClassifier().fit(X, y)

print("LogisticRegression 예측:", model1.predict([[2, 2]]))
print("RandomForest 예측:", model2.predict([[2, 2]]))
```

### 주의사항

- 단일 모델에만 의존하지 말고, 다양한 모델을 실험해보는 것이 바람직하다.
- 과적합(overfitting) 방지를 위해 교차 검증 등 평가 기법을 활용해야 한다.

---

## 데이터 전처리 🧹

**데이터 전처리**는 모델 학습에 적합하도록 데이터를 정제하는 과정이다. 이 단계에서는 결측치, 이상치, 데이터 분할 등 다양한 처리가 이루어진다.

### 설명

- 전처리를 통해 데이터의 품질을 높이고, 모델의 일관된 입력을 보장한다.
- 데이터 분할은 모델의 일반화 성능을 평가하기 위해 필수적이다.

---

### 데이터 분리

전체 데이터셋을 **훈련 데이터**와 **테스트 데이터**로 분리하여, 모델의 성능을 공정하게 평가할 수 있도록 한다.

#### 예시

```python
from sklearn.model_selection import train_test_split
import numpy as np

# 임의의 특성 및 레이블 데이터 생성
features = np.random.randint(1, 20, 60)
labels = np.random.randint(0, 3, 60)

# 8:2 비율로 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(
    features, labels, test_size=0.2, random_state=123
)
print("훈련 데이터 크기:", len(X_train))
print("테스트 데이터 크기:", len(X_test))
```

#### 주의사항

- `test_size`는 테스트 데이터의 비율을 의미하며, 일반적으로 0.2~0.3 사이로 설정한다.
- `random_state`를 지정하면 데이터 분할 결과를 재현할 수 있다.
- 전체 데이터를 모두 학습에 사용하면 모델의 일반화 성능을 제대로 평가할 수 없다.

---

## 모델 학습 🧠

**모델 학습**은 훈련 데이터를 이용해 데이터의 패턴을 학습하는 과정이다.

### 설명

- 모델이 데이터의 특성을 파악하여 예측 능력을 갖추도록 한다.
- 학습 과정에서 과적합을 방지하고, 새로운 데이터에 대한 일반화 성능을 확보하는 것이 중요하다.

### 예시

```python
from sklearn.tree import DecisionTreeClassifier

# 임의의 데이터
X_train = [[2, 3], [3, 5], [5, 7], [7, 9]]
y_train = [0, 1, 0, 1]

# 모델 학습
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# 예측
print("예측 결과:", clf.predict([[4, 6]]))
```

### 주의사항

- 학습 데이터에만 최적화된 모델은 실제 데이터에서 성능이 저하될 수 있다.
- 적절한 하이퍼파라미터 튜닝이 필요하다.

---

## 평가 및 예측 📏

**평가 및 예측** 단계에서는 테스트 데이터를 이용해 모델의 성능을 검증하고, 실제 예측에 활용한다.

### 설명

- 평가 지표(정확도, 정밀도, 재현율 등)를 사용하여 모델의 신뢰성을 확인한다.
- 평가 결과를 바탕으로 모델의 약점을 파악하고 개선한다.

### 예시

```python
from sklearn.metrics import accuracy_score

# 임의의 예측 결과와 실제 값
y_true = [1, 0, 1, 1, 0]
y_pred = [1, 0, 0, 1, 1]

# 정확도 계산
acc = accuracy_score(y_true, y_pred)
print(f"정확도: {acc:.2f}")
```

### 주의사항

- 평가 방법은 모델의 종류에 따라 다를 수 있다.
- 단일 지표만으로 모델을 평가하지 말고, 다양한 지표를 함께 고려해야 한다.

---

### 과적합과 교차 검증

**과적합**은 모델이 훈련 데이터에 지나치게 맞춰져 새로운 데이터에 대한 예측력이 떨어지는 현상이다. 이를 방지하기 위해 **교차 검증**을 활용한다.

#### 설명

- **K-겹 교차 검증(K-fold cross-validation)** 은 데이터를 여러 부분으로 나누어 반복적으로 학습과 평가를 수행한다.
- 교차 검증을 통해 모델의 일반화 성능을 신뢰성 있게 평가할 수 있다.

#### 예시

```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import Ridge
import numpy as np

# 임의의 데이터
X = np.random.rand(30, 2)
y = np.random.randint(0, 2, 30)

# 5-겹 교차 검증
kf = KFold(n_splits=5, shuffle=True, random_state=42)
model = Ridge()
scores = cross_val_score(model, X, y, cv=kf)
print("교차 검증 점수:", scores)
print("평균 점수:", scores.mean())
```

#### 주의사항

- 교차 검증을 통해 과적합 여부를 효과적으로 판단할 수 있다.
- 데이터가 충분히 크지 않은 경우, 교차 검증의 fold 수를 너무 크게 설정하면 각 fold의 데이터가 부족해질 수 있다.

---

## EDA 개념 🔎

**탐색적 데이터 분석(EDA)** 는 데이터의 구조와 특성을 파악하고, 적절한 모델링 방향을 설정하는 데 필수적인 과정이다.

### 설명

- EDA는 데이터의 분포, 변수 간 관계, 이상치 및 결측치 존재 여부 등을 시각화와 통계적 방법으로 탐색한다.
- 데이터의 특성을 이해함으로써 명확한 문제 정의와 목표 설정이 가능하다.

### 예시

```python
import pandas as pd

# 임의의 데이터프레임 생성
df = pd.DataFrame({
    '연령': [21, 34, 28, 45, 39],
    '소득': [3200, 4500, 3900, 5200, 4100]
})

# 데이터 타입 및 크기 확인
print(df.info())
print("데이터 크기:", df.shape)

# 상위 3개 행 확인
print(df.head(3))
```

### 주의사항

- 데이터의 변수별 자료형과 분포를 반드시 확인해야 한다.
- EDA 과정에서 발견된 문제는 전처리 단계에서 반드시 해결해야 한다.

---

## 데이터 확인 및 정제 📝

**데이터 확인**은 변수의 자료형, 결측치, 이상치 등 데이터의 상태를 점검하는 과정이다. **데이터 정제**는 불필요하거나 오류가 있는 데이터를 수정·제거하는 작업이다.

### 설명

- 변수별 통계량(평균, 중앙값, 최빈값 등)을 확인하여 데이터의 분포를 파악한다.
- 결측치와 이상치를 탐색하고, 적절한 방법으로 처리한다.

### 예시

```python
import pandas as pd
import numpy as np

# 임의의 데이터프레임 생성
df = pd.DataFrame({
    '연령': [25, np.nan, 30, 27, 35],
    '점수': [88, 92, np.nan, 90, 87]
})

# 결측치 확인
print(df.isnull().sum())

# 변수별 통계량 확인
print(df.describe())
```

### 주의사항

- 결측치와 이상치를 방치하면 모델의 성능이 저하될 수 있다.
- 데이터 정제 과정에서 중요한 정보가 손실되지 않도록 주의해야 한다.

---

## 결측치 개념 🚫

**결측치**는 데이터셋에서 값이 누락된 상태를 의미한다. 결측치는 데이터 수집 과정에서 발생할 수 있으며, 머신러닝 모델의 성능에 부정적인 영향을 미친다.

### 설명

- 결측치는 데이터의 신뢰성을 저하시킬 수 있으므로 반드시 처리해야 한다.
- 결측치가 많은 변수는 분석에서 제외하거나, 적절한 값으로 대체해야 한다.

---

## 결측치 확인 🔦

**결측치 확인**은 데이터프레임 내 결측치의 존재 여부와 개수를 파악하는 과정이다.

### 예시

```python
import pandas as pd
import numpy as np

# 임의의 데이터프레임 생성
df = pd.DataFrame({
    '나이': [28, np.nan, 35, 40, np.nan],
    '점수': [85, 90, 88, np.nan, 92]
})

# 결측치 개수 확인
print(df.isnull().sum())
```

**출력 예시**:
```
나이    2
점수    1
dtype: int64
```

### 주의사항

- `isnull()` 함수는 결측치 여부를 bool 값으로 반환한다.
- `isnull().sum()`을 사용하면 각 변수별 결측치 개수를 알 수 있다.

---

## 결측치 처리 방법 🧩

결측치는 **삭제**하거나 **대체**하는 방식으로 처리할 수 있다.

### 1. 전체 삭제

결측치가 많은 열 또는 행을 전체 삭제하는 방법이다.

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, np.nan, 8]
})

# 결측치가 포함된 열 전체 삭제
df_cleaned = df.drop(['B'], axis=1)
print(df_cleaned)
```

### 2. 부분 삭제

결측치가 있는 행만 삭제하는 방법이다.

```python
# 결측치가 있는 행 삭제
df_no_na = df.dropna()
print(df_no_na)
```

### 3. 값 대체

평균, 중앙값, 최빈값 등으로 결측치를 대체한다.

```python
# 평균값으로 결측치 대체
mean_val = df['A'].mean()
df['A'] = df['A'].fillna(mean_val)
print(df)
```

### 4. 유사 유형 대체

머신러닝 모델(예: 회귀, K-최근접 이웃 등)을 활용해 결측치를 예측하여 대체할 수 있다.

### 주의사항

- 결측치가 많은 변수는 삭제하는 것이 바람직하다.
- 대체값이 데이터의 분포를 왜곡하지 않도록 주의해야 한다.
- 머신러닝 기반 대체는 추가적인 모델링이 필요하다.

---

## 이상치 개념 ⚠️

**이상치**는 데이터 분포에서 다른 값들과 현저히 동떨어진 값을 의미한다. 이상치는 데이터 수집 과정에서 오류로 발생하거나, 실제로 특이한 현상을 반영할 수 있다.

### 설명

- 이상치는 모델의 학습에 부정적인 영향을 줄 수 있으므로 반드시 탐지하고 처리해야 한다.
- 이상치의 판단 기준은 데이터의 특성에 따라 다르다.

---

## 이상치 확인 📊

이상치는 **시각화** 또는 **통계적 방법**으로 탐지할 수 있다.

### 예시

```python
import pandas as pd
import matplotlib.pyplot as plt

# 임의의 데이터프레임 생성
df = pd.DataFrame({'연령': [22, 25, 28, 31, -3, 27, 29]})

# 상자그림(boxplot)으로 이상치 확인
plt.boxplot(df['연령'])
plt.title('연령 분포의 이상치')
plt.show()
```

### 주의사항

- boxplot, histogram 등 시각화 도구를 활용하면 이상치 탐지가 용이하다.
- 이상치가 반드시 오류는 아니므로, 도메인 지식을 바탕으로 처리 여부를 결정해야 한다.

---

## 이상치 처리 방법 🛠️

이상치는 **삭제**, **대체**, **변수화** 등 다양한 방식으로 처리할 수 있다.

### 1. 단순 삭제

이상치가 명백한 오류(예: 음수 나이)인 경우 해당 행을 삭제한다.

```python
import pandas as pd

df = pd.DataFrame({'연령': [22, 25, 28, -3, 27]})

# 0 이하의 연령값 삭제
df_clean = df[df['연령'] > 0]
print(df_clean)
```

### 2. 값 대체

이상치를 중앙값 등으로 대체한다.

```python
median_age = df[df['연령'] > 0]['연령'].median()
df['연령'] = df['연령'].apply(lambda x: median_age if x <= 0 else x)
print(df)
```

### 3. 변수화

이상치 여부를 새로운 변수로 만들어 모델에 활용한다.

```python
def is_outlier(val):
    return val <= 0 or val != int(val)

df['이상치여부'] = df['연령'].apply(is_outlier)
print(df)
```

### 주의사항

- 이상치가 자연스러운 현상일 수 있으므로, 무조건 삭제하지 말고 데이터의 맥락을 고려해야 한다.
- 이상치 처리 방식에 따라 분석 결과가 달라질 수 있다.

---

## 특성 엔지니어링 개념 🧬

**특성 엔지니어링**은 기존 데이터에서 더 유용한 정보를 추출하거나, 모델에 적합한 형태로 변환하는 과정이다.

### 설명

- 특성 생성, 선택, 변환 등 다양한 기법을 통해 데이터의 표현력을 높인다.
- 좋은 특성은 모델의 성능을 크게 향상시킬 수 있다.

---

## 특성 생성 🏗️

기존 데이터를 활용해 새로운 특성을 만들어내는 과정이다.

### 설명

- 날짜 데이터를 연, 월, 일, 요일 등으로 분리하거나, 두 변수의 조합으로 새로운 변수를 생성할 수 있다.
- 텍스트 데이터의 길이, 단어 수 등도 특성으로 활용할 수 있다.

### 예시

```python
import pandas as pd

# 날짜 데이터를 연, 월, 일로 분리
df = pd.DataFrame({'날짜': ['2024-05-01', '2024-06-15']})
df[['연', '월', '일']] = df['날짜'].str.split('-', expand=True)
print(df)
```

### 주의사항

- 특성 생성은 도메인 지식과 데이터의 특성을 잘 이해해야 효과적이다.
- 불필요한 특성은 오히려 모델의 성능을 저하시킬 수 있다.

---

## 특성 선택 🎯

모델에 유용한 특성만을 선택하고, 불필요하거나 중복된 특성은 제거하는 과정이다.

### 설명

- 상관관계 분석, 특성 중요도 평가, 차원 축소 기법 등을 활용한다.
- 불필요한 특성을 제거하면 모델의 복잡도를 낮추고, 과적합을 방지할 수 있다.

### 예시

```python
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier

# 임의의 데이터
X = pd.DataFrame({
    '특성1': [1, 2, 3, 4],
    '특성2': [4, 3, 2, 1],
    '특성3': [1, 1, 0, 0]
})
y = [0, 1, 0, 1]

# 특성 중요도 평가
model = ExtraTreesClassifier()
model.fit(X, y)
print("특성 중요도:", model.feature_importances_)
```

### 주의사항

- 상관관계가 높은 특성은 중복 정보를 제공할 수 있으므로 주의해야 한다.
- 차원 축소 기법 사용 시 해석 가능성이 떨어질 수 있다.

---

## 특성 변환 🔄

특성의 단위, 분포, 형태를 변경하여 모델에 적합하게 만드는 과정이다.

### 설명

- **스케일링**: 데이터의 범위를 조정(정규화, 표준화 등)
- **바이닝**: 연속형 변수를 구간별로 나누어 범주형 변수로 변환
- **더미화**: 범주형 변수를 연속형(0/1) 변수로 변환

### 예시

#### 스케일링

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({'가격': [100, 200, 300, 400, 0]})

# 로그 변환 (0 또는 음수는 0으로 처리)
df['가격_로그'] = df['가격'].apply(lambda x: np.log(x + 1) if x > 0 else 0)
print(df)
```

#### 바이닝

```python
import pandas as pd

df = pd.DataFrame({'연령': [15, 25, 45, 65]})

# 연령 구간별로 값 부여
df['연령_구간'] = pd.cut(df['연령'], bins=[0, 20, 40, 60, 100], labels=[0, 1, 2, 3])
print(df)
```

### 주의사항

- 스케일링 시 0 또는 음수 값 처리에 주의해야 한다.
- 바이닝, 더미화 등 변환 방식은 모델의 특성에 따라 달라진다.

---

## 상관관계 개념 🔗

**상관관계**는 두 변수 간의 선형적 관계의 강도와 방향을 나타내는 통계적 개념이다.

### 설명

- 상관계수는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 양의 상관관계, -1에 가까울수록 음의 상관관계를 의미한다.
- 0에 가까울수록 두 변수는 독립적이다.

---

## 상관관계 분석 방법 📈

상관관계는 **수치적 계산**과 **시각화**를 통해 분석할 수 있다.

### 1. 피어슨 상관계수

```python
import pandas as pd

df = pd.DataFrame({
    '키': [170, 180, 175, 160, 165],
    '몸무게': [65, 80, 75, 55, 60],
    '나이': [25, 28, 22, 30, 27]
})

# 피어슨 상관계수 계산
corr = df.corr(method='pearson')
print(corr)
```

### 2. pairplot

```python
import seaborn as sns
import matplotlib.pyplot as plt

# pairplot으로 변수 간 산점도 시각화
sns.pairplot(df)
plt.show()
```

### 3. heatmap

```python
import seaborn as sns
import matplotlib.pyplot as plt

# 상관계수 히트맵 시각화
sns.heatmap(corr, annot=True)
plt.show()
```

### 주의사항

- 상관관계는 인과관계를 의미하지 않는다.
- 변수 간 상관관계가 높으면 다중공선성 문제가 발생할 수 있으므로 주의해야 한다.