# 5. 데이터와 특성 중요도 📊

## 목차 📑

### 1. 의사결정나무로 알아보는 특성 중요도
- [1.1 의사결정나무 개념](#의사결정나무-개념-) 🌳
- [1.2 의사결정나무의 구조](#의사결정나무의-구조-) 🏗️
- [1.3 회귀 트리와 구역 분할](#회귀-트리와-구역-분할-) 📈
- [1.4 분류 트리와 불순도](#분류-트리와-불순도-) 🧩
- [1.5 불순도 측정: 지니계수](#불순도-측정-지니계수-) ⚖️
- [1.6 특성 중요도와 시각화](#특성-중요도와-시각화-) 🎯

### 2. 특성 엔지니어링
- [2.1 특성 엔지니어링 개념](#특성-엔지니어링-개념-) 🛠️
- [2.2 주요 특성 엔지니어링 기법](#주요-특성-엔지니어링-기법-) 🔧
  - [스케일링](#스케일링)
  - [비닝](#비닝)
  - [더미(One-hot Encoding)](#더미one-hot-encoding)
- [2.3 실습 예시: 타이타닉 데이터](#실습-예시-타이타닉-데이터-) 🚢

---

## 의사결정나무 개념 🌳

**설명**  
**의사결정나무**는 데이터를 분할하여 예측을 수행하는 **머신러닝 모델**이다. 이 모델은 일련의 질문을 통해 데이터를 여러 구역으로 나누며, 각 구역에서 예측을 수행한다. **분류**와 **회귀** 문제 모두에 적용할 수 있으며, 시각적으로 구조를 이해하기 쉽다는 장점이 있다.

**예시**

```python
from sklearn.tree import DecisionTreeClassifier

# 예시 데이터 생성
features = [[2, 1], [3, 2], [10, 5], [12, 7]]
labels = [0, 0, 1, 1]

# 의사결정나무 분류기 학습
model = DecisionTreeClassifier(max_depth=2)
model.fit(features, labels)

# 예측
result = model.predict([[4, 2]])
print(f"예측 결과: {result}")
```
**출력 예시**  
```
예측 결과: [0]
```

**주의사항**  
의사결정나무는 데이터에 과적합(overfitting)되기 쉬우므로, **트리의 깊이**나 **최소 샘플 수** 등 하이퍼파라미터를 적절히 조정해야 한다.

---

## 의사결정나무의 구조 🏗️

**설명**  
의사결정나무는 **Root Node(루트 노드)** 에서 시작하여, 각 노드에서 특정 조건에 따라 데이터를 분할한다. 분할은 아래 방향으로 진행되며, 더 이상 분할이 불가능하거나 조건을 만족하지 않을 때 **Terminal Node(단말 노드, 리프 노드)** 에 도달한다. 각 리프 노드는 최종 예측값을 가진다.

**예시**

```python
# 의사결정나무 구조 예시 (텍스트 기반)
# Root Node: 나이 < 30?
#   ├─ 예: 소득 < 50?
#   │    ├─ 예: 클래스 0
#   │    └─ 아니오: 클래스 1
#   └─ 아니오: 클래스 1
```

**주의사항**  
트리의 구조가 너무 복잡해지면 해석이 어려워질 수 있으므로, **적절한 가지치기(pruning)**가 필요하다.

---

## 회귀 트리와 구역 분할 📈

**설명**  
**회귀를 위한 의사결정나무**는 입력 공간을 여러 구역으로 나누고, 각 구역의 평균값을 예측값으로 사용한다. 구역 분할 기준은 **RSS(Residual Sum of Squares, 잔차제곱합)** 의 감소량이 가장 큰 지점이다. 즉, 좌우 노드의 RSS 합이 최소가 되는 분할을 반복한다.

**예시**

```python
from sklearn.tree import DecisionTreeRegressor

# 예시 데이터
X = [[1], [2], [3], [10], [12], [14]]
y = [2, 3, 2.5, 10, 12, 13]

# 회귀 트리 학습
reg = DecisionTreeRegressor(max_depth=2)
reg.fit(X, y)

# 예측
pred = reg.predict([[4]])
print(f"예측값: {pred[0]:.2f}")
```
**출력 예시**  
```
예측값: 2.50
```

**주의사항**  
구역이 너무 세분화되면 각 구역에 데이터가 적어져 예측이 불안정해질 수 있다. **최소 샘플 수**와 **최대 깊이**를 설정하여 과적합을 방지해야 한다.

---

## 분류 트리와 불순도 🧩

**설명**  
**분류를 위한 의사결정나무**는 구역을 나눌 때 **불순도(impurity)**를 기준으로 한다. 불순도는 한 구역에 다양한 클래스가 섞여 있는 정도를 의미한다. 불순도가 낮을수록 한 클래스가 우세하게 분포한다.

**예시**

```python
from sklearn.tree import DecisionTreeClassifier

# 예시 데이터
X = [[20], [22], [35], [40], [42], [50]]
y = ['A', 'A', 'B', 'B', 'B', 'A']

# 분류 트리 학습
clf = DecisionTreeClassifier(max_depth=2)
clf.fit(X, y)

# 예측
result = clf.predict([[30]])
print(f"예측 클래스: {result[0]}")
```
**출력 예시**  
```
예측 클래스: B
```

**주의사항**  
불순도 기준으로는 **지니계수(Gini Impurity)**, **엔트로피(Entropy)** 등이 사용된다. 목적에 따라 적절한 기준을 선택해야 한다.

---

## 불순도 측정: 지니계수 ⚖️

**설명**  
**지니계수(Gini Impurity)** 는 한 구역에서 임의로 선택한 데이터가 잘못 분류될 확률을 나타낸다. 계산식은 다음과 같다.

- Gini Impurity = 1 - Σ(각 클래스의 비율²)

예를 들어, 한 구역에 클래스 A가 60%, 클래스 B가 40%라면  
Gini = 1 - (0.6)² - (0.4)² = 1 - 0.36 - 0.16 = 0.48

**예시**

```python
# 지니계수 계산 함수
def gini_impurity(class_counts):
    total = sum(class_counts)
    return 1 - sum((count/total)**2 for count in class_counts)

# 예시: 클래스 A 30개, 클래스 B 20개
impurity = gini_impurity([30, 20])
print(f"지니계수: {impurity:.2f}")
```
**출력 예시**  
```
지니계수: 0.48
```

**주의사항**  
지니계수는 0에 가까울수록 한 클래스가 우세함을 의미한다. 여러 클래스가 비슷하게 섞여 있으면 값이 커진다.

---

## 특성 중요도와 시각화 🎯

**설명**  
**특성 중요도(feature importance)** 는 각 특성이 모델의 예측에 얼마나 기여하는지 나타내는 지표이다. 의사결정나무에서는 각 특성별로 **지니계수 감소량** 또는 **불순도 감소량**을 합산하여 중요도를 계산한다. 중요도가 높을수록 해당 특성이 데이터 분할에 큰 영향을 미친다.

**예시**

```python
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import matplotlib.pyplot as plt

# 예시 데이터
data = pd.DataFrame({
    '길이': [5, 7, 8, 6, 9, 4],
    '무게': [50, 80, 90, 60, 100, 45],
    '색상': [0, 1, 1, 0, 1, 0],  # 0: 빨강, 1: 파랑
    '종류': [0, 1, 1, 0, 1, 0]
})

X = data[['길이', '무게', '색상']]
y = data['종류']

# 트리 학습
tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X, y)

# 특성 중요도 시각화
importances = tree.feature_importances_
plt.bar(['길이', '무게', '색상'], importances)
plt.title('특성 중요도')
plt.show()
```

**주의사항**  
특성 중요도는 데이터의 스케일이나 상관관계에 영향을 받을 수 있다. **스케일링**이나 **불필요한 특성 제거** 등 전처리 과정이 중요하다.

---

## 특성 엔지니어링 개념 🛠️

**설명**  
**특성 엔지니어링(feature engineering)** 은 새로운 데이터를 추가하지 않고, 기존 데이터를 변환하거나 조합하여 모델의 성능을 높이는 과정이다. 데이터의 특성을 더 잘 설명할 수 있는 변수를 생성하거나, 불필요한 정보를 제거하여 예측력을 향상시킨다.

**예시**

```python
import pandas as pd

# 기존 데이터
df = pd.DataFrame({
    '나이': [23, 45, 31, 27],
    '연봉': [3200, 5400, 4100, 3800]
})

# 새로운 특성 생성: 연봉/나이 비율
df['연봉_나이_비율'] = df['연봉'] / df['나이']
print(df)
```
**출력 예시**  
```
   나이   연봉  연봉_나이_비율
0  23  3200   139.13
1  45  5400   120.00
2  31  4100   132.26
3  27  3800   140.74
```

**주의사항**  
특성 엔지니어링은 도메인 지식과 데이터의 특성을 잘 이해하고 있어야 효과적으로 수행할 수 있다.

---

## 주요 특성 엔지니어링 기법 🔧

### 스케일링

**설명**  
**스케일링(scaling)**은 데이터의 값 범위를 일정한 구간으로 조정하는 과정이다. 값의 크기가 지나치게 크거나 작을 때 모델 학습에 악영향을 줄 수 있으므로, **표준화(standardization)**나 **정규화(normalization)** 등의 방법을 사용한다.

**예시**

```python
from sklearn.preprocessing import MinMaxScaler

# 예시 데이터
values = [[120], [200], [150], [300]]
scaler = MinMaxScaler()
scaled = scaler.fit_transform(values)
print("스케일링 결과:", scaled.flatten())
```
**출력 예시**  
```
스케일링 결과: [0.   0.4  0.15 1.  ]
```

**주의사항**  
스케일링은 훈련 데이터와 테스트 데이터에 동일한 기준으로 적용해야 한다. 훈련 데이터로 `fit`한 후, 테스트 데이터에는 `transform`만 수행해야 한다.

---

### 비닝

**설명**  
**비닝(binning)** 은 연속형(수치형) 데이터를 구간별로 나누어 **범주형 데이터**로 변환하는 기법이다. 데이터의 분포를 단순화하거나, 이상치의 영향을 줄이기 위해 사용된다.

**예시**

```python
import pandas as pd

# 예시 데이터
scores = pd.Series([55, 67, 80, 45, 90, 72])

# 3개 구간으로 비닝
bins = [0, 60, 75, 100]
labels = ['낮음', '중간', '높음']
categories = pd.cut(scores, bins=bins, labels=labels)
print(categories)
```
**출력 예시**  
```
0     낮음
1    중간
2    높음
3     낮음
4    높음
5    중간
dtype: category
```

**주의사항**  
비닝 구간의 개수와 경계값은 데이터의 분포와 분석 목적에 따라 신중하게 설정해야 한다.

---

### 더미(One-hot Encoding)

**설명**  
**더미(dummy) 변수** 또는 **One-hot Encoding**은 범주형 데이터를 0과 1로 이루어진 이진 벡터로 변환하는 방법이다. 머신러닝 모델이 범주형 데이터를 직접 처리할 수 없기 때문에 사용한다.

**예시**

```python
import pandas as pd

# 예시 데이터
df = pd.DataFrame({'도시': ['서울', '부산', '서울', '대구']})

# One-hot Encoding
df_encoded = pd.get_dummies(df, columns=['도시'])
print(df_encoded)
```
**출력 예시**  
```
   도시_대구  도시_부산  도시_서울
0       0       0       1
1       0       1       0
2       0       0       1
3       1       0       0
```

**주의사항**  
One-hot Encoding을 적용하면 변수 개수가 늘어나므로, 고유값이 많은 범주형 변수에는 주의해서 사용해야 한다.

---

## 실습 예시: 타이타닉 데이터 🚢

**설명**  
타이타닉 데이터셋은 머신러닝 실습에서 자주 사용되는 예제 데이터로, 승객의 생존 여부를 다양한 특성(나이, 성별, 선실 등)으로 예측하는 문제이다. 특성 엔지니어링을 통해 모델 성능을 높일 수 있다.

**예시**

```python
import pandas as pd

# 예시 데이터
data = {
    '성별': ['남성', '여성', '여성', '남성'],
    '나이': [22, 38, 26, 35],
    '요금': [7.25, 71.28, 7.92, 53.10],
    '생존': [0, 1, 1, 1]
}
df = pd.DataFrame(data)

# 성별 더미 변수화
df = pd.get_dummies(df, columns=['성별'])

# 나이 비닝
df['나이_구간'] = pd.cut(df['나이'], bins=[0, 18, 35, 60], labels=['청소년', '성인', '노년'])

print(df)
```
**출력 예시**  
```
    나이    요금  생존  성별_남성  성별_여성 나이_구간
0   22   7.25   0      1      0    성인
1   38  71.28   1      0      1    노년
2   26   7.92   1      0      1    성인
3   35  53.10   1      1      0    성인
```

**주의사항**  
실제 데이터에는 결측치, 이상치, 데이터 불균형 등이 존재할 수 있으므로, **전처리 과정**을 반드시 거쳐야 한다.