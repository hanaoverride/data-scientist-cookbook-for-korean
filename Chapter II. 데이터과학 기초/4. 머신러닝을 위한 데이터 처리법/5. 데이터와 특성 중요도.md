# 5. 특성 공학: 데이터에 숨결을 불어넣는 기술

## 목차
- [5. 특성 공학: 데이터에 숨결을 불어넣는 기술](#5-특성-공학-데이터에-숨결을-불어넣는-기술)
  - [목차](#목차)
  - [1. 왜 특성 공학이 가장 중요한가?](#1-왜-특성-공학이-가장-중요한가)
  - [2. 특성 공학의 세 가지 축](#2-특성-공학의-세-가지-축)
  - [3. 축 1: 특성 생성 (Feature Creation) - 없던 정보 만들어내기](#3-축-1-특성-생성-feature-creation---없던-정보-만들어내기)
    - [날짜/시간 데이터 분해](#날짜시간-데이터-분해)
    - [여러 특성의 조합](#여러-특성의-조합)
  - [4. 축 2: 특성 변환 (Feature Transformation) - 데이터를 모델 친화적으로 바꾸기](#4-축-2-특성-변환-feature-transformation---데이터를-모델-친화적으로-바꾸기)
    - [로그 변환: 왜곡된 분포 바로잡기](#로그-변환-왜곡된-분포-바로잡기)
    - [범주형 데이터 인코딩: 문자를 숫자로](#범주형-데이터-인코딩-문자를-숫자로)
  - [5. 축 3: 특성 선택 (Feature Selection) - 핵심 정보만 남기기](#5-축-3-특성-선택-feature-selection---핵심-정보만-남기기)
    - [모델 기반 선택: 특성 중요도 활용](#모델-기반-선택-특성-중요도-활용)

---

## 1. 왜 특성 공학이 가장 중요한가?

머신러닝 모델은 데이터에 존재하는 패턴만 학습할 수 있습니다. 만약 데이터에 패턴이 명시적으로 드러나 있지 않다면, 아무리 좋은 모델을 써도 소용이 없습니다.

특성 공학은 **데이터를 재가공하여 모델이 이해하기 쉬운 형태로 패턴을 드러내 주는 과정**입니다.
- **선형 모델**이 비선형 관계를 학습하게 돕고,
- **트리 모델**이 더 적은 분기로 명확한 결정을 내리게 하며,
- 전반적인 **모델의 성능과 해석 가능성**을 높입니다.

## 2. 특성 공학의 세 가지 축

특성 공학은 크게 세 가지 활동으로 나눌 수 있습니다.

1.  **특성 생성**: 기존 특성들을 조합하거나 분해하여 새로운 특성을 만듭니다.
2.  **특성 변환**: 기존 특성을 모델이 더 잘 이해할 수 있는 다른 형태로 바꿉니다.
3.  **특성 선택**: 모델 성능에 도움이 되지 않거나, 오히려 방해되는 특성을 제거합니다.

## 3. 축 1: 특성 생성 (Feature Creation) - 없던 정보 만들어내기

도메인 지식을 활용하여 데이터에 숨어있는 정보를 명시적인 특성으로 만드는 과정입니다.

### 날짜/시간 데이터 분해

'2024-07-11 15:30:00'이라는 데이터는 그 자체로 모델이 사용하기 어렵습니다. 하지만 이를 분해하면 강력한 특성이 됩니다.

```python
import pandas as pd

df = pd.DataFrame({'datetime': ['2024-07-11 15:30:00', '2024-12-25 09:00:00']})
df['datetime'] = pd.to_datetime(df['datetime']) # datetime 타입으로 변환

df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour
df['weekday'] = df['datetime'].dt.weekday # 월요일=0, 일요일=6

print(df[['year', 'month', 'day', 'hour', 'weekday']])
```
> **인사이트**: '요일'이나 '시간' 특성은 사용자의 주기적인 행동 패턴(예: 주말에 구매 증가, 출퇴근 시간에 접속 증가)을 모델이 학습하는 데 결정적인 역할을 합니다.

### 여러 특성의 조합

기존 특성들을 사칙연산하거나 논리적으로 결합하여 새로운 의미를 갖는 특성을 만들 수 있습니다.

```python
# 타이타닉 데이터 예시
df = sns.load_dataset('titanic')

# '형제자매/배우자 수(sibsp)'와 '부모/자녀 수(parch)'를 더해 '가족 크기' 특성 생성
df['family_size'] = df['sibsp'] + df['parch'] + 1 # +1은 본인

# 혼자 탑승했는지 여부를 나타내는 'is_alone' 특성 생성
df['is_alone'] = (df['family_size'] == 1).astype(int)

print(df[['family_size', 'is_alone']].head())
```

## 4. 축 2: 특성 변환 (Feature Transformation) - 데이터를 모델 친화적으로 바꾸기

### 로그 변환: 왜곡된 분포 바로잡기

소득, 가격, 인구수 등 많은 현실 세계의 데이터는 소수의 값이 매우 큰, 오른쪽으로 꼬리가 긴(right-skewed) 분포를 보입니다. 이런 데이터에 **로그 변환(Log Transformation)**  을 적용하면, 분포가 정규분포에 가까워져 많은 모델의 성능이 향상됩니다.

```python
import numpy as np

# 로그 변환 전/후 시각화
fare_original = df['fare'][df['fare'] > 0]
fare_log = np.log1p(fare_original) # log1p는 log(1+x)로, x가 0일 때도 계산 가능

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(fare_original, kde=True)
plt.title('로그 변환 전 요금(fare) 분포')

plt.subplot(1, 2, 2)
sns.histplot(fare_log, kde=True)
plt.title('로그 변환 후 요금(fare) 분포')
plt.show()
```

### 범주형 데이터 인코딩: 문자를 숫자로

대부분의 머신러닝 모델은 문자열을 직접 입력받지 못합니다. 따라서 '성별', '도시명' 같은 범주형 데이터를 숫자 형태로 바꿔주어야 합니다.

- **원-핫 인코딩 (One-Hot Encoding)** : 각 카테고리를 새로운 열(변수)로 만들고, 해당하면 1, 아니면 0으로 표시합니다. 카테고리 간 순서가 없을 때 사용합니다. (예: `pd.get_dummies()`)
- **레이블 인코딩 (Label Encoding)** : 각 카테고리를 순서가 있는 숫자로 바꿉니다 (예: 'Bronze':0, 'Silver':1, 'Gold':2). 카테고리 간에 명확한 순위가 있을 때 사용합니다.

## 5. 축 3: 특성 선택 (Feature Selection) - 핵심 정보만 남기기

모든 특성이 모델 성능에 도움이 되는 것은 아닙니다. 관련 없는 특성이나 중복된 특성은 오히려 모델의 학습을 방해하고 과적합을 유발할 수 있습니다.

### 모델 기반 선택: 특성 중요도 활용

트리 기반 모델(RandomForest, XGBoost 등)은 학습이 끝난 후, 각 특성이 예측에 얼마나 기여했는지를 나타내는 **특성 중요도(Feature Importance)**  정보를 제공합니다. 이 중요도를 기준으로, 점수가 낮은 특성들을 제거하여 모델을 더 단순하고 강력하게 만들 수 있습니다.

```python
from sklearn.ensemble import RandomForestClassifier

# 타이타닉 데이터의 일부 특성으로 생존 예측 모델 학습
# (간단한 예시를 위해 결측치 등은 무시)
df_subset = df[['pclass', 'age', 'fare', 'family_size', 'survived']].dropna()
X = df_subset.drop('survived', axis=1)
y = df_subset['survived']

model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# 특성 중요도 시각화
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='barh')
plt.title('특성 중요도')
plt.show()
```
> **인사이트**: 위 결과로부터 'fare'(요금)와 'age'(나이)가 생존 예측에 매우 중요한 역할을 한다는 것을 알 수 있습니다. 반면, 'family_size'의 중요도는 상대적으로 낮아, 모델에서 제외하는 것을 고려해볼 수 있습니다.