# 3. 딥러닝 훈련의 장애물과 극복 전략


## 목차
- [3. 딥러닝 훈련의 장애물과 극복 전략](#3-딥러닝-훈련의-장애물과-극복-전략)
  - [목차](#목차)
  - [1. 문제 1: 학습이 너무 느리거나 불안정하다 (최적화 문제)](#1-문제-1-학습이-너무-느리거나-불안정하다-최적화-문제)
    - [해결책: 더 똑똑한 경사 하강법 (Advanced Optimizers)](#해결책-더-똑똑한-경사-하강법-advanced-optimizers)
  - [2. 문제 2: 층이 깊어질수록 학습이 멈춘다 (기울기 소실/폭주)](#2-문제-2-층이-깊어질수록-학습이-멈춘다-기울기-소실폭주)
    - [해결책 1: 똑똑한 활성화 함수 (ReLU)](#해결책-1-똑똑한-활성화-함수-relu)
    - [해결책 2: 똑똑한 시작 (He 초기화)](#해결책-2-똑똑한-시작-he-초기화)
  - [3. 문제 3: 훈련 데이터만 잘 맞춘다 (과적합 문제)](#3-문제-3-훈련-데이터만-잘-맞춘다-과적합-문제)
    - [해결책 1: 규제 (Regularization) - 모델에 제약 걸기](#해결책-1-규제-regularization---모델에-제약-걸기)
    - [해결책 2: 드롭아웃 (Dropout) - 뉴런을 랜덤으로 쉬게 하기](#해결책-2-드롭아웃-dropout---뉴런을-랜덤으로-쉬게-하기)
    - [해결책 3: 배치 정규화 (Batch Normalization) - 층마다 스트레칭 시키기](#해결책-3-배치-정규화-batch-normalization---층마다-스트레칭-시키기)

---

## 1. 문제 1: 학습이 너무 느리거나 불안정하다 (최적화 문제)

기본적인 경사 하강법은 전체 데이터를 사용해서 한 걸음을 내딛기 때문에 매우 느리고, 미니배치를 사용하는 SGD는 한 걸음 한 걸음이 불안정하여 비효율적으로 움직일 수 있습니다.

### 해결책: 더 똑똑한 경사 하강법 (Advanced Optimizers)

- **Momentum**: '관성'을 추가합니다. 이전 걸음의 방향을 기억하여, 현재 걸음에 반영합니다. 이로써 불필요한 지그재그 움직임을 줄이고, 올바른 방향으로 더 빠르게 나아갈 수 있습니다.
- **RMSProp**: '적응형 학습률'의 개념을 도입합니다. 각 파라미터마다 다른 학습률을 적용하여, 변화가 적었던 파라미터는 더 큰 걸음으로, 변화가 많았던 파라미터는 더 작은 걸음으로 걷게 합니다.
- **Adam (Adaptive Moment Estimation)** : **Momentum과 RMSProp의 장점을 결합**한, 현재 가장 널리 쓰이는 표준 옵티마이저입니다. 관성을 이용해 빠르게 나아가면서도, 각 파라미터에 맞는 최적의 보폭을 유지합니다. **특별한 이유가 없다면 Adam으로 시작하는 것이 가장 좋습니다.**

## 2. 문제 2: 층이 깊어질수록 학습이 멈춘다 (기울기 소실/폭주)

**기울기 소실(Vanishing Gradients)**  은 역전파 과정에서 기울기가 층을 거슬러 올라갈수록 점점 작아져, 결국 0에 가까워지는 현상입니다. 이로 인해 앞쪽 층의 가중치는 거의 업데이트되지 않고 학습이 멈추게 됩니다. 반대로 기울기가 점점 커지는 것을 **기울기 폭주(Exploding Gradients)**  라고 합니다.

### 해결책 1: 똑똑한 활성화 함수 (ReLU)

- **문제 원인**: 과거에 주로 쓰이던 시그모이드(Sigmoid) 함수는 입력값이 커지거나 작아지면 기울기가 0에 가까워지는 특성이 있어, 기울기 소실의 주범이었습니다.
- **해결책**: **ReLU(Rectified Linear Unit)**  함수를 사용합니다.
    - `f(x) = max(0, x)`
    - **원리**: 입력이 양수이면 기울기가 항상 1로 유지됩니다. 이 덕분에 층이 깊어져도 기울기가 소멸하지 않고 앞쪽 층까지 잘 전달될 수 있습니다. 계산 또한 매우 빠릅니다. 현대 딥러닝 모델의 은닉층에서는 대부분 ReLU나 그 변형(Leaky ReLU 등)을 사용합니다.

### 해결책 2: 똑똑한 시작 (He 초기화)

- **문제 원인**: 가중치 초기값이 너무 작으면 신호가 층을 거치며 약해지고, 너무 크면 신호가 폭주하여 학습이 불안정해집니다.
- **해결책**: 활성화 함수에 맞는 적절한 초기화 방법을 사용합니다.
    - **Xavier/Glorot 초기화**: Sigmoid, Tanh 함수에 적합합니다.
    - **He 초기화**: **ReLU 계열 활성화 함수에 특화**된 초기화 방법으로, 현재 가장 널리 사용됩니다. Keras의 `Dense` 레이어는 기본적으로 He 초기화를 사용합니다.

## 3. 문제 3: 훈련 데이터만 잘 맞춘다 (과적합 문제)

**과적합(Overfitting)**  은 모델이 훈련 데이터의 특정 패턴과 노이즈까지 모두 외워버려, 새로운 데이터에 대한 일반화 성능이 떨어지는 현상입니다.

### 해결책 1: 규제 (Regularization) - 모델에 제약 걸기

모델의 가중치가 너무 커지지 않도록 손실 함수에 페널티를 추가합니다. 가중치가 크다는 것은 모델이 훈련 데이터의 특정 부분에 과도하게 의존하고 있다는 신호일 수 있습니다.
- **L2 규제 (Weight Decay)** : 가중치 제곱합에 페널티를 부여. 가중치를 전반적으로 작게 유지합니다.
- **L1 규제**: 가중치 절대값 합에 페널티를 부여. 중요하지 않은 가중치를 0으로 만들어 특성 선택 효과를 줍니다.

### 해결책 2: 드롭아웃 (Dropout) - 뉴런을 랜덤으로 쉬게 하기

- **원리**: 훈련 과정에서 각 단계마다 **무작위로 일부 뉴런을 비활성화**시킵니다. 즉, 매번 다른 구조의 '작은' 신경망을 학습시키는 것과 같습니다.
- **효과**:
    - 뉴런들이 특정 동료 뉴런에 과도하게 의존하는 것을 막고, 각자 독립적으로 유용한 특징을 학습하도록 강제합니다.
    - 결과적으로 여러 개의 다른 모델을 학습시켜 그 결과를 평균 내는 **앙상블(Ensemble) 효과**를 주어, 과적합을 효과적으로 방지합니다.

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout

model = tf.keras.Sequential([
    Dense(128, activation='relu', input_shape=(...)),
    Dropout(0.5), # 훈련 시 50%의 뉴런을 랜덤으로 비활성화
    Dense(1, activation='sigmoid')
])
```

### 해결책 3: 배치 정규화 (Batch Normalization) - 층마다 스트레칭 시키기

- **원리**: 각 미니배치 데이터가 각 층을 통과할 때마다, 그 결과를 **평균 0, 분산 1인 표준정규분포로 정규화**합니다.
- **효과**:
    - 앞쪽 층의 가중치가 변하더라도 뒤쪽 층의 입력 분포가 크게 흔들리지 않도록 하여(내부 공변량 변화 문제 해결), 학습을 훨씬 안정적이고 빠르게 만듭니다.
    - 학습률을 더 높게 설정할 수 있게 해주고, 가중치 초기화에 대한 민감도를 줄여줍니다.
    - 약간의 규제 효과도 있어 과적합을 억제하는 데 도움이 됩니다.