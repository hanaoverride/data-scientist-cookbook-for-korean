# 2. 딥러닝 학습법과 텐서플로우 사용법 🧠

## 목차 📑

### 1. 딥러닝 모델의 학습 방법
- [1.1 딥러닝 모델 구조](#딥러닝-모델-구조-) 🏗️
- [1.2 손실 함수와 최적화](#손실-함수와-최적화-) ⚙️
- [1.3 순전파와 역전파](#순전파와-역전파-) 🔄

### 2. 텐서플로우(TensorFlow)
- [2.1 텐서플로우 개요](#텐서플로우-개요-) 📝
- [2.2 텐서와 데이터 플로우](#텐서와-데이터-플로우-) 🔢
- [2.3 텐서플로우 버전 변화](#텐서플로우-버전-변화-) 🔀

### 3. 텐서플로우 기초 사용법
- [3.1 기본 텐서 종류](#기본-텐서-종류-) 🧩
- [3.2 상수/시퀀스/변수 텐서 생성](#상수시퀀스변수-텐서-생성-) 🏷️
- [3.3 텐서 연산 및 함수 정의](#텐서-연산-및-함수-정의-) ➕

### 4. 텐서플로우로 딥러닝 모델 구현하기
- [4.1 모델 구현 절차](#모델-구현-절차-) 🛠️
- [4.2 데이터셋 준비](#데이터셋-준비-) 📦
- [4.3 모델 구축](#모델-구축-) 🏗️
- [4.4 모델 학습](#모델-학습-) 🏃
- [4.5 평가 및 예측](#평가-및-예측-) 🎯

---

## 딥러닝 모델 구조 🏗️

### 설명

**딥러닝 모델**은 입력층, 여러 개의 **은닉층(Hidden Layer)**, 그리고 출력층으로 구성된다. **은닉층**이 3개 이상일 때 **딥러닝(Deep Learning)** 이라는 용어를 사용한다. 각 층은 여러 **노드(Node)** 또는 **유닛(Unit)** 으로 이루어지며, 이들 사이의 연결 강도를 **가중치(Weight)** 라고 한다. **레이어(Layer)** 는 이러한 노드들의 집합으로, 입력 데이터가 입력층을 통해 들어와 은닉층을 거쳐 출력층에서 결과를 산출한다.

### 예시

```python
# 3층 신경망 구조 예시
layers = [
    {"type": "input", "units": 2},
    {"type": "hidden", "units": 4},
    {"type": "hidden", "units": 3},
    {"type": "output", "units": 1}
]
for idx, layer in enumerate(layers):
    print(f"{idx+1}번째 층: {layer['type']} ({layer['units']}개 노드)")
```

**출력 예시**:
```
1번째 층: input (2개 노드)
2번째 층: hidden (4개 노드)
3번째 층: hidden (3개 노드)
4번째 층: output (1개 노드)
```

### 주의사항

- **가중치**는 학습 과정에서 계속 갱신되며, 초기값에 따라 학습 결과가 달라질 수 있다.
- **은닉층**의 개수와 노드 수는 문제의 복잡도에 따라 조정해야 한다.

---

## 손실 함수와 최적화 ⚙️

### 설명

딥러닝 모델의 학습은 **손실 함수(Loss Function)** 를 최소화하는 **최적화(Optimization)** 과정이다. **손실 함수**는 예측값과 실제값 사이의 오차를 수치로 표현하며, 이 값을 줄이기 위해 **최적화 알고리즘**이 사용된다. 대표적인 최적화 방법은 **경사 하강법(Gradient Descent)** 으로, 각 가중치의 기울기를 계산하여 손실이 줄어드는 방향으로 가중치를 갱신한다. 이때 **학습률(learning rate)** 은 한 번에 이동하는 크기를 결정한다.

### 예시

```python
# 임의의 손실 함수와 경사 하강법 예시
import numpy as np

def loss_fn(w):
    return (w - 3) ** 2 + 2

w = 0.0
learning_rate = 0.2

for step in range(5):
    grad = 2 * (w - 3)
    w -= learning_rate * grad
    print(f"Step {step+1}: w={w:.2f}, loss={loss_fn(w):.2f}")
```

**출력 예시**:
```
Step 1: w=1.20, loss=3.24
Step 2: w=2.08, loss=1.85
Step 3: w=2.66, loss=1.22
Step 4: w=2.93, loss=1.01
Step 5: w=2.99, loss=1.00
```

### 주의사항

- **학습률**이 너무 크면 발산, 너무 작으면 수렴 속도가 느려진다.
- 손실 함수의 종류는 문제 유형(분류, 회귀 등)에 따라 달라진다.

---

## 순전파와 역전파 🔄

### 설명

**순전파(Forward Propagation)** 는 입력 데이터를 받아 각 층을 거치며 출력을 계산하는 과정이다. **역전파(Backpropagation)** 는 예측 결과와 실제 값의 오차를 계산한 뒤, 이 오차를 각 가중치에 대해 역으로 전달하여 가중치를 갱신하는 알고리즘이다. 역전파는 딥러닝의 핵심 학습 메커니즘으로, 각 층의 가중치가 오차를 줄이는 방향으로 조정된다.

### 예시

```python
# 순전파 및 역전파 개념 예시
import numpy as np

# 입력, 가중치, 바이어스
inputs = np.array([2.0, -1.0])
weights = np.array([1.5, -2.0])
bias = 0.5

# 순전파: 출력 계산
output = np.dot(inputs, weights) + bias
print(f"순전파 결과: {output:.2f}")

# 역전파: 손실 함수의 미분값(기울기) 계산
loss_grad = 2 * (output - 1.0)  # 예: (예측-정답)^2 손실의 미분
weights_grad = loss_grad * inputs
print(f"가중치에 대한 기울기: {weights_grad}")
```

**출력 예시**:
```
순전파 결과: 6.50
가중치에 대한 기울기: [11.  -5.]
```

### 주의사항

- 역전파 시 각 층의 출력값이 필요하므로, 순전파 결과를 저장해야 한다.
- 역전파는 **체인 룰**(연쇄법칙)을 이용해 미분값을 계산한다.

---

## 텐서플로우 개요 📝

### 설명

**텐서플로우(TensorFlow)** 는 구글에서 개발한 **딥러닝 프레임워크**로, 유연성·효율성·확장성을 갖추고 있다. 대규모 서버부터 모바일 기기까지 다양한 환경에서 동작하며, 복잡한 딥러닝 모델을 손쉽게 구현할 수 있다. 텐서플로우 외에도 다양한 프레임워크가 존재하지만, 텐서플로우는 높은 점유율과 빠른 성장률을 보이고 있다.

### 예시

```python
# 텐서플로우 버전 확인 예시
import tensorflow as tf
print("TensorFlow 버전:", tf.__version__)
```

**출력 예시**:
```
TensorFlow 버전: 2.12.0
```

### 주의사항

- 텐서플로우는 버전에 따라 사용법이 다르므로, 공식 문서에서 버전에 맞는 예제를 참고해야 한다.
- GPU 가속을 활용하려면 별도의 설치가 필요하다.

---

## 텐서와 데이터 플로우 🔢

### 설명

**텐서(Tensor)** 는 딥러닝에서 사용하는 **다차원 배열**을 의미한다. 1차원(벡터), 2차원(행렬), 3차원 이상(이미지, 시퀀스 등)까지 다양한 형태로 데이터를 표현한다. **플로우(Flow)** 는 데이터가 연산 노드를 따라 흐르는 과정을 뜻하며, 텐서플로우에서는 **데이터 플로우 그래프**를 통해 연산이 수행된다. 즉, 텐서플로우는 텐서(데이터)와 플로우(흐름)의 결합으로, 데이터를 그래프 구조로 처리한다.

### 예시

```python
# 1D, 2D, 3D 텐서 생성 예시
import tensorflow as tf

tensor_1d = tf.constant([7, 8, 9])
tensor_2d = tf.constant([[1, 2], [3, 4]])
tensor_3d = tf.constant([[[1], [2]], [[3], [4]]])

print("1D 텐서:", tensor_1d)
print("2D 텐서:", tensor_2d)
print("3D 텐서:", tensor_3d)
```

**출력 예시**:
```
1D 텐서: tf.Tensor([7 8 9], shape=(3,), dtype=int32)
2D 텐서: tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32)
3D 텐서: tf.Tensor(
[[[1]
  [2]]

 [[3]
  [4]]], shape=(2, 2, 1), dtype=int32)
```

### 주의사항

- 텐서의 차원과 데이터 타입(dtype)을 정확히 지정해야 한다.
- 연산 그래프의 흐름을 이해하면 모델 디버깅이 쉬워진다.

---

## 텐서플로우 버전 변화 🔀

### 설명

**텐서플로우 1.x**에서는 연산을 수행하기 위해 **계산 그래프**와 **세션(Session)** 을 명시적으로 생성해야 했다. 반면, **텐서플로우 2.x**부터는 **즉시 실행(Eager Execution)** 이 기본으로 적용되어, 파이썬 코드처럼 직관적으로 연산을 실행할 수 있다. 이로 인해 코드 작성과 디버깅이 훨씬 쉬워졌다.

### 예시

```python
# 텐서플로우 2.x의 즉시 실행 예시
import tensorflow as tf

a = tf.constant([2, 3])
b = tf.constant([4, 5])
result = a + b
print("즉시 실행 결과:", result)
```

**출력 예시**:
```
즉시 실행 결과: tf.Tensor([6 8], shape=(2,), dtype=int32)
```

### 주의사항

- 1.x 코드와 2.x 코드는 호환되지 않는 부분이 있으므로, 버전에 맞는 코드를 작성해야 한다.
- 즉시 실행 환경에서는 중간 결과를 바로 확인할 수 있다.

---

## 기본 텐서 종류 🧩

### 설명

텐서플로우에서는 다양한 **기본 텐서**를 제공한다. 대표적으로 **상수 텐서(Constant Tensor)**, **시퀀스 텐서(Sequence Tensor)**, **변수 텐서(Variable Tensor)** 가 있다. 각각의 텐서는 생성 방식과 용도가 다르며, 모델 구현 시 상황에 맞게 선택하여 사용한다.

### 예시

```python
import tensorflow as tf

# 상수 텐서
const_tensor = tf.constant([5, 6, 7], dtype=tf.int32)

# 시퀀스 텐서 (등간격)
seq_tensor = tf.linspace(0.0, 1.0, 4)  # 0.0, 0.33, 0.66, 1.0

# 변수 텐서
var_tensor = tf.Variable([1.0, 2.0, 3.0], dtype=tf.float32)

print("상수 텐서:", const_tensor)
print("시퀀스 텐서:", seq_tensor)
print("변수 텐서:", var_tensor)
```

**출력 예시**:
```
상수 텐서: tf.Tensor([5 6 7], shape=(3,), dtype=int32)
시퀀스 텐서: tf.Tensor([0.        0.3333333 0.6666667 1.       ], shape=(4,), dtype=float32)
변수 텐서: <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>
```

### 주의사항

- **상수 텐서**는 값이 고정되어 변경할 수 없다.
- **변수 텐서**는 학습 과정에서 값이 갱신된다.

---

## 상수/시퀀스/변수 텐서 생성 🏷️

### 설명

- **상수 텐서**는 고정된 값을 가지며, `tf.constant`로 생성한다.
- **시퀀스 텐서**는 일정한 간격의 값을 가지며, `tf.linspace` 또는 `tf.range`로 생성한다.
- **변수 텐서**는 학습 가능한 값으로, `tf.Variable`로 생성한다.

### 예시

```python
import tensorflow as tf

# 모든 원소가 0인 텐서
zero_tensor = tf.zeros([2, 3], dtype=tf.float32)

# 모든 원소가 1인 텐서
one_tensor = tf.ones([2, 3], dtype=tf.float32)

# 등차수열 텐서 (시작값 2, 끝값 10, 간격 2)
range_tensor = tf.range(2, 10, 2)

# 변수 텐서 (초기값 지정)
trainable_tensor = tf.Variable(initial_value=[0.5, 1.5], dtype=tf.float32)

print("0 텐서:", zero_tensor)
print("1 텐서:", one_tensor)
print("등차수열 텐서:", range_tensor)
print("변수 텐서:", trainable_tensor)
```

**출력 예시**:
```
0 텐서: tf.Tensor(
[[0. 0. 0.]
 [0. 0. 0.]], shape=(2, 3), dtype=float32)
1 텐서: tf.Tensor(
[[1. 1. 1.]
 [1. 1. 1.]], shape=(2, 3), dtype=float32)
등차수열 텐서: tf.Tensor([2 4 6 8], shape=(4,), dtype=int32)
변수 텐서: <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.5, 1.5], dtype=float32)>
```

### 주의사항

- `tf.linspace`는 실수 간격, `tf.range`는 정수 간격에 적합하다.
- 변수 텐서는 연산 및 학습 과정에서 값이 변경될 수 있다.

---

## 텐서 연산 및 함수 정의 ➕

### 설명

텐서플로우에서는 텐서 간의 다양한 연산이 가능하며, 사용자 정의 함수를 통해 연산 과정을 모듈화할 수 있다. 함수 내부에서 텐서 연산을 정의하면, 모델의 순전파 과정을 쉽게 구현할 수 있다.

### 예시

```python
import tensorflow as tf

# 상수 텐서 생성
input_tensor = tf.constant([[2.0, 3.0]], dtype=tf.float32)
weight = tf.constant([[1.0], [2.0]], dtype=tf.float32)
bias = tf.constant([0.7], dtype=tf.float32)

# 순전파 함수 정의
def model_forward(x):
    return tf.matmul(x, weight) + bias

# 연산 및 출력
result = model_forward(input_tensor)
print("모델 출력:", result)
```

**출력 예시**:
```
모델 출력: tf.Tensor([[7.7]], shape=(1, 1), dtype=float32)
```

### 주의사항

- 텐서의 shape(차원)이 연산에 맞게 일치해야 한다.
- 함수 내에서 사용하는 변수와 상수의 타입 및 shape을 주의해야 한다.

---

## 모델 구현 절차 🛠️

### 설명

딥러닝 모델 구현은 다음과 같은 절차로 진행된다.

1. **데이터셋 준비**: 학습에 사용할 데이터를 준비하고, **Epoch**와 **Batch** 단위로 분할한다.
2. **모델 구축**: 입력층, 은닉층, 출력층을 포함한 신경망 구조를 설계한다.
3. **모델 학습**: 준비된 데이터로 모델을 학습시킨다.
4. **평가 및 예측**: 테스트 데이터로 모델 성능을 평가하고, 새로운 데이터에 대해 예측을 수행한다.

### 예시

```python
# 절차 요약 출력
steps = ["데이터셋 준비", "모델 구축", "모델 학습", "평가 및 예측"]
for idx, step in enumerate(steps, 1):
    print(f"{idx}. {step}")
```

**출력 예시**:
```
1. 데이터셋 준비
2. 모델 구축
3. 모델 학습
4. 평가 및 예측
```

### 주의사항

- 각 단계별로 데이터 전처리, 하이퍼파라미터 설정 등 세부 작업이 필요하다.
- 데이터 분할 시 학습/검증/테스트 세트를 명확히 구분해야 한다.

---

## 데이터셋 준비 📦

### 설명

**Epoch**는 전체 데이터셋을 한 번 모두 학습하는 단위를 의미한다. **Batch**는 전체 데이터를 여러 묶음으로 나눈 것으로, 한 번에 처리하는 데이터의 크기를 의미한다. **Iteration**은 한 Epoch 내에서 Batch 단위로 학습이 반복되는 횟수이다. 데이터셋은 주로 **tf.data.Dataset API**를 활용해 생성하며, 효율적인 배치 처리가 가능하다.

### 예시

```python
import numpy as np
import tensorflow as tf

# 임의 데이터 생성
features = np.random.rand(120, 3)
targets = np.random.randint(0, 2, size=(120, 1))

# Dataset 생성 및 배치 적용
dataset = tf.data.Dataset.from_tensor_slices((features, targets))
dataset = dataset.batch(24)

for batch_features, batch_targets in dataset.take(1):
    print("배치 feature shape:", batch_features.shape)
    print("배치 target shape:", batch_targets.shape)
```

**출력 예시**:
```
배치 feature shape: (24, 3)
배치 target shape: (24, 1)
```

### 주의사항

- **Batch size**가 너무 크면 메모리 부족, 너무 작으면 학습 속도가 느려질 수 있다.
- 데이터는 반드시 **shuffle**(섞기) 과정을 거쳐야 일반화 성능이 향상된다.

---

## 모델 구축 🏗️

### 설명

텐서플로우에서는 **Keras API**를 통해 신경망 모델을 쉽게 구축할 수 있다. **Sequential 모델**은 층을 순차적으로 쌓는 방식이며, 각 층은 `tf.keras.layers.Dense` 등으로 추가한다. 첫 번째 층에는 입력 데이터의 형태를 지정해야 하며, `input_shape` 또는 `input_dim` 인자를 사용한다.

### 예시

```python
import tensorflow as tf

# Sequential 모델 생성 및 층 추가
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(8, input_dim=3, activation='relu'),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

또는,

```python
# add 메서드로 층을 순차적으로 추가
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(8, input_dim=3, activation='relu'))
model.add(tf.keras.layers.Dense(5, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
```

### 주의사항

- 입력층의 형태(`input_shape` 또는 `input_dim`)를 반드시 지정해야 한다.
- 활성화 함수(`activation`)는 문제 유형에 따라 적절히 선택한다.

---

## 모델 학습 🏃

### 설명

모델 학습을 위해서는 먼저 **컴파일(compile)** 단계에서 **최적화 알고리즘(optimizer)** 과 **손실 함수(loss)** 를 지정해야 한다. 이후 **fit** 메서드로 데이터를 입력하여 학습을 진행한다. 학습은 지정한 Epoch 수만큼 반복된다.

### 예시

```python
# 모델 컴파일 및 학습 예시
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(dataset, epochs=20)
```

**출력 예시**:
```
Epoch 1/20
5/5 [==============================] - 0s 2ms/step - loss: 0.6931
...
Epoch 20/20
5/5 [==============================] - 0s 1ms/step - loss: 0.5123
```

### 주의사항

- **optimizer**와 **loss**는 데이터와 문제 유형에 맞게 선택해야 한다.
- **fit** 메서드의 입력 데이터는 배치 처리된 형태여야 한다.

---

## 평가 및 예측 🎯

### 설명

학습된 모델의 성능을 평가하려면 **evaluate** 메서드를 사용한다. 새로운 데이터에 대한 예측은 **predict** 메서드로 수행한다. 평가에는 테스트 데이터와 정답 레이블이 필요하며, 예측에는 입력 데이터만 필요하다.

### 예시

```python
# 테스트 데이터 준비
test_features = np.random.rand(30, 3)
test_targets = np.random.randint(0, 2, size=(30, 1))
test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_targets)).batch(10)

# 평가 및 예측
loss = model.evaluate(test_dataset)
predictions = model.predict(test_features)
print("테스트 손실:", loss)
print("예측 결과:", predictions[:5])
```

**출력 예시**:
```
3/3 [==============================] - 0s 2ms/step - loss: 0.4890
테스트 손실: 0.4890
예측 결과: [[0.65]
 [0.42]
 [0.78]
 [0.33]
 [0.55]]
```

### 주의사항

- 평가와 예측 시 입력 데이터의 shape이 모델과 일치해야 한다.
- 예측 결과는 확률(0~1) 또는 클래스 값으로 반환될 수 있다.