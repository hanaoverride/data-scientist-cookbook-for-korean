# 1. 딥러닝과 퍼셉트론 🧠

## 목차 📑

### 1. 딥러닝 개론
- [1.1 인공지능, 머신러닝, 딥러닝](#인공지능-머신러닝-딥러닝-) 🤖
- [1.2 딥러닝의 정의와 인공신경망](#딥러닝의-정의와-인공신경망-) 🧬
- [1.3 인공신경망의 특징 및 역사](#인공신경망의-특징-및-역사-) ⏳
- [1.4 딥러닝의 실제 적용 사례](#딥러닝의-실제-적용-사례-) 💡

### 2. 퍼셉트론(Perceptron)
- [2.1 퍼셉트론의 등장과 한계](#퍼셉트론의-등장과-한계-) 🚦
- [2.2 퍼셉트론의 구조와 수식](#퍼셉트론의-구조와-수식-) 🏗️
- [2.3 활성화 함수](#활성화-함수-) 🔑
- [2.4 퍼셉트론 동작 예시](#퍼셉트론-동작-예시-) 🧩
- [2.5 퍼셉트론 코드 예시](#퍼셉트론-코드-예시-) 💻

### 3. 퍼셉트론 선형 분류기
- [3.1 논리 회로와 퍼셉트론](#논리-회로와-퍼셉트론-) 🔌
- [3.2 단층 퍼셉트론의 구조](#단층-퍼셉트론의-구조-) 🧱
- [3.3 선형 분류기의 확장](#선형-분류기의-확장-) ➕

### 4. 비선형적인 문제와 한계
- [4.1 선형 분류의 한계: XOR 문제](#선형-분류의-한계-xor-문제-) ❌
- [4.2 비선형적 접근의 필요성](#비선형적-접근의-필요성-) 🔄

### 5. 다층 퍼셉트론(Multi Layer Perceptron)
- [5.1 다층 퍼셉트론의 구조](#다층-퍼셉트론의-구조-) 🏢
- [5.2 히든 레이어와 딥러닝](#히든-레이어와-딥러닝-) 🏊
- [5.3 다층 퍼셉트론의 표현력](#다층-퍼셉트론의-표현력-) 🌐

---

## 인공지능, 머신러닝, 딥러닝 🤖

**인공지능(Artificial Intelligence)** 은 컴퓨터가 인간과 유사한 **지능적 작업**을 수행하도록 하는 기술이다. **머신러닝(Machine Learning)** 은 인공지능의 한 분야로, 데이터로부터 **규칙**을 스스로 학습하는 방법론이다. **딥러닝(Deep Learning)** 은 머신러닝의 하위 개념으로, **인공신경망**을 기반으로 하여 복잡한 패턴을 자동으로 학습한다.

| 구분 | 설명 | 특징 |
|------|------|------|
| **인공지능** | 인간의 지능적 작업 모방 | 광범위한 응용 분야 |
| **머신러닝** | 데이터 기반 학습 | 지도/비지도/강화학습 |
| **딥러닝** | 인공신경망 기반 학습 | 대용량 데이터·고성능 연산 필요 |

---

## 딥러닝의 정의와 인공신경망 🧬

**딥러닝**은 **인공신경망(Artificial Neural Network, ANN)** 에 기반한 머신러닝 방법 중 하나로, 컴퓨터가 사람의 사고방식을 모방하여 **데이터의 특징**을 스스로 학습한다. 인공신경망은 생물학적 신경망에서 영감을 받아 설계된 알고리즘 구조이다.

**신경 시스템(Neuron System)** 은 두뇌의 가장 작은 정보 처리 단위인 **뉴런(Neuron)** 으로 구성된다. 인공신경망은 이러한 뉴런의 구조와 신호 전달 방식을 모방하여 입력, 가중치, 활성화 함수, 출력을 가진다.

---

## 인공신경망의 특징 및 역사 ⏳

**인공신경망**은 입력 데이터의 특성을 스스로 학습하며, **지도학습**과 **비지도학습** 모두에 적용 가능하다. 주요 특징은 다음과 같다.

- **회귀 분석** 및 **분류** 등 다양한 문제에 활용 가능
- **패턴 인식**에 강점
- **다층 구조**를 통해 복잡한 문제 해결 가능

### 딥러닝의 역사

- **1958년**: 퍼셉트론(Perceptron) 최초 제안
- **1969년**: 첫 번째 AI 겨울(연구 침체기)
- **1986년**: 신경망 연구 재개, 역전파 알고리즘 도입
- **1990년대~2010년대**: 컴퓨팅 파워 및 데이터 증가로 딥러닝 급성장
- **2012년 이후**: 딥러닝 혁명, 다양한 분야에 적용

---

## 딥러닝의 실제 적용 사례 💡

딥러닝은 다양한 실생활 분야에 적용되고 있다.

- **얼굴 인식 카메라**: 이미지 내 인물 식별
- **기계 번역 모델**: 자동 번역(예: Google 번역)
- **알파고 제로**: 바둑 등 게임에서 인간을 능가하는 전략 학습

---

## 퍼셉트론의 등장과 한계 🚦

**퍼셉트론(Perceptron)** 은 1958년 제안된 **초기 인공신경망 모델**로, 입력값과 가중치의 선형 결합에 활성화 함수를 적용하여 출력을 생성한다. 퍼셉트론 이전에는 **명시적 프로그래밍(Explicit Programming)** 방식이 주류였으나, 이는 복잡한 문제(예: 자율주행)에서 한계가 있었다. 퍼셉트론은 기계가 스스로 학습할 수 있는 기반을 마련하였다.

---

## 퍼셉트론의 구조와 수식 🏗️

퍼셉트론은 다음과 같은 구조를 가진다.

- **입력값**: 여러 개의 입력(예: x₁, x₂)
- **가중치**: 각 입력에 곱해지는 값(예: w₁, w₂)
- **바이어스(Bias)**: 전체 합에 더해지는 상수항(b)
- **활성화 함수**: 최종 출력을 결정하는 함수

**수식**:  
y = activation(w₁x₁ + w₂x₂ + b)

| 변수 | 의미      |
|------|-----------|
| x₁, x₂ | 입력값   |
| w₁, w₂ | 가중치   |
| b      | 바이어스 |
| y      | 출력값   |

---

## 활성화 함수 🔑

**활성화 함수(Activation Function)** 는 입력의 선형 결합 결과를 비선형적으로 변환하여 출력한다. 퍼셉트론에서는 주로 **계단 함수**(Step Function)를 사용한다.

**계단 함수 예시**:  
activation(z) = 1 if z ≥ 0, 0 otherwise

---

## 퍼셉트론 동작 예시 🧩

퍼셉트론은 입력값, 가중치, 바이어스를 받아 선형 결합 후 활성화 함수를 적용하여 결과를 출력한다.

```python
# 새로운 퍼셉트론 동작 예시
def step_function(z):
    return 1 if z >= 0 else 0

inputs = [2, -1]
weights = [0.5, 1.2]
bias = -0.3

z = weights[0] * inputs[0] + weights[1] * inputs[1] + bias
output = step_function(z)
print(f"퍼셉트론 출력: {output}")  # 출력 예시: 0
```

**출력 예시**:
```
퍼셉트론 출력: 0
```

> **⚠️ 주의사항**: 입력값, 가중치, 바이어스의 조합에 따라 출력이 달라지므로, 각 값의 의미를 정확히 이해해야 한다.

---

## 퍼셉트론 코드 예시 💻

퍼셉트론은 입력 벡터와 가중치 벡터, 바이어스를 받아 동작한다.

```python
def simple_perceptron(inputs, params):
    # params[0]은 바이어스, params[1:]은 가중치
    total = params[0]
    for idx in range(len(inputs)):
        total += params[idx + 1] * inputs[idx]
    return 1 if total >= 0 else 0

# 예시 입력 및 파라미터
sample_inputs = [1, 0]
sample_params = [-0.7, 0.8, 1.5]  # bias, w1, w2
result = simple_perceptron(sample_inputs, sample_params)
print(f"결과: {result}")  # 출력 예시: 1
```

**출력 예시**:
```
결과: 1
```

> **⚠️ 주의사항**: 입력 벡터와 가중치 벡터의 길이가 일치해야 하며, 바이어스는 항상 첫 번째 요소로 전달해야 한다.

---

## 논리 회로와 퍼셉트론 🔌

퍼셉트론은 **논리 회로**의 역할을 수행할 수 있다. 입력값에 따라 논리 연산(AND, OR, NAND, NOR 등)을 구현할 수 있다.

### AND 게이트

- 두 입력이 모두 1일 때만 출력이 1이다.

```python
def and_gate(a, b):
    return 1 if (a + b) > 1.5 else 0

print(and_gate(1, 1))  # 1
print(and_gate(0, 1))  # 0
```

### OR 게이트

- 입력 중 하나라도 1이면 출력이 1이다.

```python
def or_gate(a, b):
    return 1 if (a + b) > 0.5 else 0

print(or_gate(1, 0))  # 1
print(or_gate(0, 0))  # 0
```

### NAND 게이트

- 두 입력이 모두 1일 때만 출력이 0이다.

```python
def nand_gate(a, b):
    return 1 if (-(a + b) + 1.5) > 0 else 0

print(nand_gate(1, 1))  # 0
print(nand_gate(0, 1))  # 1
```

### NOR 게이트

- 입력이 모두 0일 때만 출력이 1이다.

```python
def nor_gate(a, b):
    return 1 if (-(a + b) + 0.5) > 0 else 0

print(nor_gate(0, 0))  # 1
print(nor_gate(1, 0))  # 0
```

> **⚠️ 주의사항**: 각 논리 게이트의 임계값(바이어스)은 연산 목적에 맞게 조정해야 하며, 입력값은 0 또는 1로 제한해야 한다.

---

## 단층 퍼셉트론의 구조 🧱

**단층 퍼셉트론(Single Layer Perceptron)** 은 입력층과 출력층만으로 구성된 신경망이다. 입력층은 외부 데이터를 받아들이고, 출력층은 최종 결과를 반환한다.

- **입력층(Input Layer)**: 데이터가 신경망에 처음 들어오는 부분
- **출력층(Output Layer)**: 신경망의 최종 연산 결과가 나오는 부분

단층 퍼셉트론은 **선형 분류기**로서, 입력 데이터가 선형적으로 구분 가능한 경우에만 올바른 분류가 가능하다.

---

## 선형 분류기의 확장 ➕

퍼셉트론은 논리 회로에서 0, 1 데이터를 처리하는 것에서 출발하여, **선형 분류기**로 확장되어 다양한 데이터 분류 문제에 적용할 수 있다.

---

## 선형 분류의 한계: XOR 문제 ❌

**XOR 게이트**와 같이 하나의 직선으로 구분할 수 없는 **비선형적 문제**는 단층 퍼셉트론으로 해결할 수 없다.

| A | B | XOR(A, B) |
|---|---|-----------|
| 0 | 0 |     0     |
| 0 | 1 |     1     |
| 1 | 0 |     1     |
| 1 | 1 |     0     |

단층 퍼셉트론은 이러한 문제에서 올바른 출력을 내지 못한다.

---

## 비선형적 접근의 필요성 🔄

비선형적인 문제를 해결하기 위해서는 **비선형적 접근 방법**이 필요하다. 단일 레이어로는 해결이 불가능하므로, 여러 층을 쌓는 구조가 요구된다.

---

## 다층 퍼셉트론의 구조 🏢

**다층 퍼셉트론(Multi Layer Perceptron, MLP)** 은 단층 퍼셉트론을 여러 층으로 쌓아 구성한다.

- **입력층(Input Layer)**: 외부 데이터 입력
- **히든 레이어(Hidden Layer)**: 입력과 출력을 연결하는 중간 층
- **출력층(Output Layer)**: 최종 결과 출력

다층 퍼셉트론은 **비선형적 문제(XOR 등)**도 해결할 수 있다.

---

## 히든 레이어와 딥러닝 🏊

**히든 레이어(Hidden Layer)** 는 입력층과 출력층 사이에 위치하며, 신경망의 표현력을 높인다. 히든 레이어가 3개 이상일 때 **딥러닝(Deep Learning)** 이라고 부른다.

```python
# 다층 퍼셉트론 구조 예시
import numpy as np

def relu(x):
    return np.maximum(0, x)

# 입력층
inputs = np.array([1, 0])

# 첫 번째 히든 레이어
weights1 = np.array([[1.0, -1.0], [0.5, 1.2]])
bias1 = np.array([0.0, 0.5])
hidden1 = relu(np.dot(weights1, inputs) + bias1)

# 출력층
weights2 = np.array([1.5, -0.7])
bias2 = -0.2
output = relu(np.dot(weights2, hidden1) + bias2)
print(f"다층 퍼셉트론 출력: {output}")
```

**출력 예시**:
```
다층 퍼셉트론 출력: 0.55
```

> **⚠️ 주의사항**: 히든 레이어의 개수와 뉴런 수가 많아질수록 모델의 복잡도가 증가하며, 과적합(overfitting)에 주의해야 한다.

---

## 다층 퍼셉트론의 표현력 🌐

다층 퍼셉트론은 히든 레이어의 수와 각 레이어의 뉴런 수에 따라 **복잡한 패턴**과 **비선형적 경계**를 학습할 수 있다. 히든 레이어가 많아질수록 더 복잡한 영역을 결정할 수 있다.

---

**요약**

- **퍼셉트론**은 인공신경망의 기본 단위로, 선형 분류 문제에 적합하다.
- **단층 퍼셉트론**은 선형적으로 구분 가능한 문제만 해결 가능하다.
- **비선형 문제(XOR 등)** 는 단층 퍼셉트론으로 해결할 수 없으므로, **다층 퍼셉트론**이 필요하다.
- **다층 퍼셉트론**은 입력층, 히든 레이어, 출력층으로 구성되며, 복잡한 비선형 문제도 해결할 수 있다.
- **딥러닝**은 히든 레이어가 3개 이상인 심층 신경망을 의미한다.

> **⚠️ 주의사항**: 퍼셉트론의 구조, 활성화 함수, 가중치 및 바이어스의 설정에 따라 모델의 성능이 크게 달라질 수 있으므로, 각 요소의 역할을 명확히 이해해야 한다.

---