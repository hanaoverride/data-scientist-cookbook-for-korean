# 4. 딥러닝의 특화 부대: CNN과 RNN


## 목차
- [1. 이미지 데이터의 특성과 CNN의 등장](#1-이미지-데이터의-특성과-cnn의-등장)
  - [CNN의 핵심 아이디어 1: 지역적 특징 학습 (Convolution)](#cnn의-핵심-아이디어-1-지역적-특징-학습-convolution)
  - [CNN의 핵심 아이디어 2: 특징 요약 및 강화 (Pooling)](#cnn의-핵심-아이디어-2-특징-요약-및-강화-pooling)
  - [CNN의 전체 구조](#cnn의-전체-구조)
- [2. 순차 데이터의 특성과 RNN의 등장](#2-순차-데이터의-특성과-rnn의-등장)
  - [RNN의 핵심 아이디어: 과거를 기억하는 순환 구조](#rnn의-핵심-아이디어-과거를-기억하는-순환-구조)
  - [자연어 처리를 위한 재료 준비: 워드 임베딩](#자연어-처리를-위한-재료-준비-워드-임베딩)

---

## 1. 이미지 데이터의 특성과 CNN의 등장

이미지 데이터는 단순한 숫자 배열이 아닙니다.
- **공간적 구조**: 주변 픽셀들은 서로 연관성이 높습니다. (고양이의 눈은 코 옆에 있습니다.)
- **지역적 패턴**: 이미지의 작은 부분(예: 눈, 코, 입)들이 모여 전체를 이룹니다.

기존의 MLP는 이미지를 한 줄의 긴 벡터로 취급하여, 이 중요한 '공간적 구조' 정보를 모두 파괴해버립니다. **합성곱 신경망(CNN, Convolutional Neural Network)**  은 바로 이 문제를 해결하기 위해 탄생했습니다.

### CNN의 핵심 아이디어 1: 지역적 특징 학습 (Convolution)

CNN은 **필터(Filter) 또는 커널(Kernel)**  이라는 작은 행렬을 사용하여, 이미지의 **지역적인 특징**을 찾아냅니다.
- **필터의 역할**: 필터는 '특징 탐지기'입니다. 어떤 필터는 수직선을, 어떤 필터는 수평선을, 또 다른 필터는 특정 색상이나 질감을 감지하도록 학습됩니다.
- **합성곱(Convolution) 연산**: 이 필터가 이미지 전체를 훑고 지나가면서(슬라이딩), 각 위치에서 필터와 이미지의 해당 부분 간의 내적을 계산합니다. 이 결과로 만들어진 새로운 행렬을 **특징 맵(Feature Map)**  이라고 부릅니다.

> **핵심**: CNN은 이미지 전체를 한 번에 보는 대신, 작은 필터를 통해 **지역적인 특징(선, 곡선, 질감 등)을 먼저 학습**하고, 뒤쪽 레이어에서 이들을 조합하여 **더 복잡한 특징(눈, 코, 입)을 학습**하는 계층적인 구조를 가집니다.

### CNN의 핵심 아이디어 2: 특징 요약 및 강화 (Pooling)

**풀링(Pooling)**  은 Convolution을 통해 얻은 특징 맵의 크기를 줄여, 연산량을 감소시키고 더 중요한 특징을 강조하는 과정입니다.
- **최대 풀링(Max Pooling)** : 특정 구역에서 가장 중요한 특징(가장 큰 값)만 남기고 나머지는 버립니다. 이를 통해 이미지의 위치가 조금 변하거나 회전해도 동일한 특징을 잡아내는 **불변성(invariance)**  을 얻게 됩니다.

### CNN의 전체 구조

`[ (Convolution -> ReLU) -> Pooling ] -> [ (Convolution -> ReLU) -> Pooling ] -> ... -> [ Fully Connected Layer ] -> [ Softmax ]`

1.  **특징 추출 부분 (앞단)** : Convolution과 Pooling 레이어를 여러 겹 쌓아, 이미지의 저수준 특징(선, 색)부터 고수준 특징(형태, 객체)까지 계층적으로 추출합니다.
2.  **분류 부분 (뒷단)** : 추출된 최종 특징 맵을 한 줄로 펼쳐, MLP(Fully Connected Layer)에 입력하여 최종적으로 이미지가 어떤 클래스에 속하는지 분류합니다.

## 2. 순차 데이터의 특성과 RNN의 등장

텍스트나 음성, 시계열 주가 데이터 같은 **순차 데이터(Sequential Data)**  는 '순서'가 매우 중요합니다. "I am happy"와 "Am I happy?"는 단어는 같지만 순서가 달라 의미가 완전히 다릅니다.

기존의 MLP나 CNN은 입력 데이터의 순서를 고려하지 못합니다. **순환 신경망(RNN, Recurrent Neural Network)**  은 이 '순서'와 '맥락'을 기억하기 위해 설계되었습니다.

### RNN의 핵심 아이디어: 과거를 기억하는 순환 구조

RNN은 **'순환'** 하는 구조를 가지고 있습니다.
- **은닉 상태 (Hidden State)** : RNN은 각 타임스텝(예: 문장의 각 단어)을 처리할 때, 현재 입력뿐만 아니라 **이전 타임스텝의 결과(은닉 상태)**  를 함께 입력으로 받습니다.
- **기억의 전달**: 이 은닉 상태는 '과거 정보의 요약본' 역할을 합니다. 현재 단어를 처리한 결과는 다음 단어를 처리할 때의 은닉 상태에 영향을 주며, 이런 식으로 정보가 문장의 끝까지 계속 전달됩니다.

> **핵심**: RNN은 **과거의 정보를 요약한 '기억(은닉 상태)'을 다음 단계로 계속 넘겨주면서**, 데이터의 순서와 맥락을 이해합니다.

### 자연어 처리를 위한 재료 준비: 워드 임베딩

RNN이 텍스트를 처리하려면, 먼저 '단어'를 숫자로 바꿔주어야 합니다.
- **원-핫 인코딩**: 간단하지만, 단어 간의 의미적 유사성을 전혀 표현하지 못하고, 단어 수가 많아지면 벡터의 차원이 너무 커지는 문제가 있습니다.
- **워드 임베딩 (Word Embedding)** : **단어의 '의미'를 밀집된 저차원 벡터로 표현**하는 기술입니다. (예: Word2vec, GloVe, FastText)
    - 의미적으로 비슷한 단어들은 벡터 공간에서도 가까운 위치에 놓이게 됩니다. (예: '왕' - '남자' + '여자' ≈ '여왕')
    - 이 임베딩 벡터가 RNN의 각 타임스텝에 입력되는 '재료'가 됩니다.

> **기억하세요**: RNN의 성능은 얼마나 좋은 워드 임베딩을 사용하느냐에 크게 좌우됩니다.
