# 3. 머신러닝 분류 모델과 성능 평가 지표

## 목차 📑

### 1. 주성분 분석(PCA)
- [1.1 주성분 분석 개념](#주성분-분석-개념-) 🔍
- [1.2 주성분 분석 예시](#주성분-분석-예시-) 📉
- [1.3 주성분 분석의 원리](#주성분-분석의-원리-) ⚙️
- [1.4 주성분 분석 시각화](#주성분-분석-시각화-) 🖼️

### 2. K-최근접 이웃(KNN)
- [2.1 KNN 개념](#knn-개념-) 🧑‍🤝‍🧑
- [2.2 거리 계산 방법](#거리-계산-방법-) 📏
- [2.3 KNN 분류 원리](#knn-분류-원리-) 🗺️
- [2.4 KNN 분류 지도](#knn-분류-지도-) 🗺️

### 3. 결정 트리
- [3.1 결정 트리 개념](#결정-트리-개념-) 🌳
- [3.2 불순도와 엔트로피](#불순도와-엔트로피-) 🔥
- [3.3 결정 트리의 원리](#결정-트리의-원리-) 🧩
- [3.4 결정 트리 시각화](#결정-트리-시각화-) 🖼️

### 4. 랜덤 포레스트
- [4.1 결정 트리의 한계](#결정-트리의-한계-) ⚠️
- [4.2 앙상블 모델 개념](#앙상블-모델-개념-) 🧩
- [4.3 랜덤 포레스트 원리](#랜덤-포레스트-원리-) 🌲
- [4.4 랜덤 포레스트 활용 예시](#랜덤-포레스트-활용-예시-) 🎮

### 5. XGBoost
- [5.1 부스팅 개념](#부스팅-개념-) 🚀
- [5.2 경사 하강법](#경사-하강법-) 📉
- [5.3 그레디언트 부스팅](#그레디언트-부스팅-) 🔄
- [5.4 XGBoost 특징](#xgboost-특징-) 🏆
- [5.5 XGBoost 대회 성적](#xgboost-대회-성적-) 🥇
- [5.6 분류 모델 비교](#분류-모델-비교-) ⚖️

### 6. 분류 모델 성능 평가 지표
- [6.1 회귀 모델 평가 지표](#회귀-모델-평가-지표-) 📏
- [6.2 오차 행렬](#오차-행렬-) 🧮
- [6.3 정밀도와 재현율](#정밀도와-재현율-) 🎯
- [6.4 F1 점수](#f1-점수-) 🏅
- [6.5 AUC-ROC 점수](#auc-roc-점수-) 📈
- [6.6 주요 분류 지표 요약](#주요-분류-지표-요약-) 📝

---

## 주성분 분석 개념 🔍

**주성분 분석(PCA)** 는 데이터의 차원을 축소하는 대표적인 기법이다. 고차원 데이터의 분포를 최대한 보존하면서 저차원 공간으로 변환하여, 사람이 이해하기 어려운 다차원 데이터를 간접적으로 파악할 수 있게 한다. PCA는 데이터의 분산을 최대한 유지하는 새로운 축을 찾아, 데이터의 구조적 특성을 효과적으로 요약한다.

### 예시

```python
import numpy as np
from sklearn.decomposition import PCA

# 임의의 4차원 데이터 생성
data = np.array([
    [2.1, 3.5, 1.2, 4.0],
    [1.9, 3.2, 1.0, 3.8],
    [2.3, 3.8, 1.4, 4.2],
    [2.0, 3.6, 1.1, 4.1]
])

# 2차원으로 차원 축소
pca = PCA(n_components=2)
reduced = pca.fit_transform(data)
print("축소된 데이터:\n", reduced)
```

**출력 예시**:
```
축소된 데이터:
 [[ 0.14142136  0.        ]
 [-0.42426407  0.        ]
 [ 0.70710678  0.        ]
 [-0.42426407  0.        ]]
```

### 주의사항

- PCA는 데이터의 **분산**이 큰 축을 우선적으로 선택하므로, 스케일이 다른 변수는 반드시 표준화해야 한다.
- 차원 축소로 인해 일부 정보가 손실될 수 있다.

---

## 주성분 분석 예시 📉

PCA를 활용하면 6차원 이상의 고차원 데이터를 2차원 또는 3차원으로 축소하여 시각화할 수 있다. 이를 통해 데이터의 군집 구조나 분포 특성을 쉽게 파악할 수 있다.

### 예시

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# 데이터 준비
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# 2차원으로 축소
pca = PCA(n_components=2)
components = pca.fit_transform(df)

# 시각화
plt.scatter(components[:, 0], components[:, 1], c=iris.target)
plt.xlabel('주성분 1')
plt.ylabel('주성분 2')
plt.title('PCA를 이용한 2차원 시각화')
plt.show()
```

### 주의사항

- 차원 축소 후 시각화된 결과가 항상 명확한 군집을 보장하지는 않는다.
- 데이터의 본질적 구조가 저차원에서 잘 드러나지 않을 수 있다.

---

## 주성분 분석의 원리 ⚙️

PCA의 핵심 원리는 데이터가 분산되는 정도를 최대한 보존하는 새로운 축(주성분)을 찾는 것이다. 이를 위해 입력 데이터의 **공분산 행렬**을 계산하고, **고유값 분해(Eigen Decomposition)** 를 수행하여 분산이 가장 큰 방향을 주성분으로 선정한다.

### 예시

```python
import numpy as np

# 2차원 데이터 예시
data = np.array([[2, 0], [0, 2], [3, 1], [1, 3]])

# 공분산 행렬 계산
cov_matrix = np.cov(data, rowvar=False)
print("공분산 행렬:\n", cov_matrix)

# 고유값 분해
eig_vals, eig_vecs = np.linalg.eig(cov_matrix)
print("고유값:", eig_vals)
print("고유벡터:\n", eig_vecs)
```

### 주의사항

- **고유값**이 큰 축이 데이터의 분산을 더 많이 설명한다.
- 모든 변수의 단위가 동일하지 않으면 결과가 왜곡될 수 있다.

---

## 주성분 분석 시각화 🖼️

PCA로 차원을 축소한 후, 저차원 공간에서 데이터가 잘 구분되는지 확인할 수 있다. 만약 2차원 또는 3차원에서 클래스 간 구분이 명확하다면 분류 문제의 난이도가 낮을 수 있다. 반대로, 저차원에서 구분이 어렵다면 추가적인 차원 축소나 다른 분석 방법이 필요하다.

### 예시

```python
# 3차원으로 축소 후 시각화
pca_3d = PCA(n_components=3)
components_3d = pca_3d.fit_transform(df)
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(components_3d[:, 0], components_3d[:, 1], components_3d[:, 2], c=iris.target)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
plt.title('PCA 3차원 시각화')
plt.show()
```

### 주의사항

- 시각화는 데이터의 분포를 직관적으로 파악하는 데 유용하지만, 해석은 신중해야 한다.

---

## KNN 개념 🧑‍🤝‍🧑

**K-최근접 이웃(KNN)** 은 예측하고자 하는 데이터 포인트와 학습 데이터 간의 거리를 계산하여, 가장 가까운 K개의 이웃의 클래스를 기준으로 분류를 수행하는 알고리즘이다. 별도의 모델 학습 과정이 없으며, **게으른 학습(Lazy Learning)** 방식으로 분류를 진행한다.

### 예시

```python
from sklearn.neighbors import KNeighborsClassifier

# 임의의 데이터 생성
X = [[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]]
y = [0, 0, 0, 1, 1, 1]

# KNN 분류기 생성 및 예측
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X, y)
prediction = knn.predict([[5, 5]])
print("예측 결과:", prediction)
```

**출력 예시**:
```
예측 결과: [1]
```

### 주의사항

- KNN은 데이터의 **특징 간 거리**가 의미가 있을 때만 효과적이다.
- 고차원 데이터에서는 성능이 저하될 수 있다.

---

## 거리 계산 방법 📏

KNN에서 가장 널리 사용되는 거리는 **유클리드 거리(Euclidean Distance)** 이다. 이는 피타고라스 정리를 일반화한 것으로, 두 점 사이의 직선 거리를 계산한다.

### 예시

```python
import numpy as np

# 두 점의 좌표
point1 = np.array([2, 3])
point2 = np.array([5, 7])

# 유클리드 거리 계산
distance = np.linalg.norm(point1 - point2)
print("유클리드 거리:", distance)
```

**출력 예시**:
```
유클리드 거리: 5.0
```

### 주의사항

- 변수의 단위가 다르면 거리 계산이 왜곡될 수 있으므로, **정규화** 또는 **표준화**가 필요하다.

---

## KNN 분류 원리 🗺️

KNN은 K값에 따라 분류 결과가 달라진다. 예를 들어, K=3이면 가장 가까운 3개의 이웃 중 다수 클래스로 분류한다. K가 너무 작으면 이상치에 민감해지고, 너무 크면 경계가 모호해질 수 있다.

### 예시

```python
# K=1, K=5로 예측 비교
knn1 = KNeighborsClassifier(n_neighbors=1)
knn5 = KNeighborsClassifier(n_neighbors=5)
knn1.fit(X, y)
knn5.fit(X, y)
print("K=1 예측:", knn1.predict([[5, 5]]))
print("K=5 예측:", knn5.predict([[5, 5]]))
```

**출력 예시**:
```
K=1 예측: [1]
K=5 예측: [0]
```

### 주의사항

- K값은 데이터의 특성에 따라 적절히 조정해야 하며, 교차 검증을 통해 최적의 값을 찾는 것이 바람직하다.

---

## KNN 분류 지도 🗺️

KNN의 K값에 따라 분류 경계가 달라진다. K가 작을수록 이상치에 민감하고, K가 커질수록 분류가 더 일반화된다.

### 예시

```python
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np

# 데이터셋
X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]])
y = np.array([0, 0, 0, 1, 1, 1])

# KNN 분류기
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X, y)

# 분류 경계 시각화
xx, yy = np.meshgrid(np.arange(0, 10, 0.1), np.arange(0, 10, 0.1))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')
plt.title('KNN 분류 지도 (K=5)')
plt.show()
```

### 주의사항

- KNN은 이상치(outlier)에 민감할 수 있으므로, 데이터 전처리가 중요하다.

---

## 결정 트리 개념 🌳

**결정 트리(Decision Tree)** 는 데이터를 분할하여 규칙을 생성하는 지도 학습 알고리즘이다. 각 **노드(Node)** 는 입력값과 목표값을 연결하는 예측의 최소 단위로, 분류와 회귀 문제 모두에 적용할 수 있다. 결정 트리는 구조가 직관적이고 해석이 쉬우나, 과적합이 발생하기 쉽다.

### 예시

```python
from sklearn.tree import DecisionTreeClassifier

# 데이터 예시
X = [[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]]
y = [0, 0, 0, 1, 1, 1]

# 결정 트리 모델 학습
tree = DecisionTreeClassifier(max_depth=2)
tree.fit(X, y)
print("예측 결과:", tree.predict([[4, 4]]))
```

**출력 예시**:
```
예측 결과: [1]
```

### 주의사항

- 트리의 깊이가 깊어질수록 과적합 위험이 커진다.
- 데이터가 불균형할 경우 분할 기준이 왜곡될 수 있다.

---

## 불순도와 엔트로피 🔥

**불순도(Impurity)** 는 한 노드 내에 서로 다른 클래스가 얼마나 섞여 있는지를 나타내는 척도이다. **엔트로피(Entropy)** 는 대표적인 불순도 함수로, 값이 높을수록 데이터가 혼합되어 있음을 의미한다.

- 엔트로피 공식:  
  **E = - Σ pi log2(pi)**  
  (여기서 pi는 각 클래스의 비율)

### 예시

```python
import numpy as np

# 두 클래스의 비율
p = [0.7, 0.3]
entropy = -sum([v * np.log2(v) for v in p if v > 0])
print("엔트로피:", entropy)
```

**출력 예시**:
```
엔트로피: 0.8812908992306927
```

### 주의사항

- 엔트로피가 0이면 완전히 순수한 노드, 1에 가까울수록 혼합도가 높다.

---

## 결정 트리의 원리 🧩

결정 트리는 **정보 이득(Information Gain)** 을 최대화하는 방향으로 데이터를 분할한다. 정보 이득은 분기 전후의 불순도 차이로 정의된다. 트리는 모든 데이터를 분류할 수 있을 때까지 노드를 추가하며, 분할 기준은 각 노드에서 최적의 속성을 선택하여 결정된다.

### 예시

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 데이터 및 모델
tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X, y)

# 트리 시각화
plt.figure(figsize=(8, 4))
plot_tree(tree, feature_names=['특성1', '특성2'], class_names=['A', 'B'], filled=True)
plt.show()
```

### 주의사항

- 트리의 깊이가 너무 깊으면 과적합이 발생할 수 있으므로, **가지치기(Pruning)** 등의 기법이 필요하다.

---

## 결정 트리 시각화 🖼️

결정 트리는 각 노드의 분기가 이진 분류 모델의 역할을 하며, 노드가 많아질수록 복잡한 분류 경계를 표현할 수 있다.

### 예시

```python
# 위 plot_tree 예시 참고
# 실제 데이터와 트리 구조를 함께 시각화하면 해석이 용이하다
```

### 주의사항

- 트리의 크기가 커지면 시각화가 복잡해질 수 있다.

---

## 결정 트리의 한계 ⚠️

결정 트리는 노드가 많아질수록 **과적합(Overfitting)** 될 가능성이 크다. 모든 데이터를 완벽히 분류하려고 복잡한 규칙을 추가하면, 새로운 데이터에 대한 일반화 성능이 저하된다. 가지치기 등 다양한 해결책이 있으나, 복잡한 데이터에서는 한계가 있다.

---

## 앙상블 모델 개념 🧩

**앙상블(Ensemble) 모델**은 여러 개의 예측 모델을 조합하여, 과적합을 방지하고 예측 성능을 높이는 방법이다. 최종 결과는 **투표(Voting)**, **평균** 등의 방식으로 통합된다.

---

## 랜덤 포레스트 원리 🌲

**랜덤 포레스트(Random Forest)** 는 여러 개의 결정 트리를 앙상블로 결합한 모델이다. 각 트리는 데이터의 일부를 **중복 허용**하여 랜덤하게 추출한 샘플로 학습하며, 이 과정을 **배깅(Bagging)** 이라고 한다. 각 트리의 예측을 평균 또는 다수결로 통합하여 최종 결과를 도출한다.

### 예시

```python
from sklearn.ensemble import RandomForestClassifier

# 데이터 예시
rf = RandomForestClassifier(n_estimators=10, random_state=42)
rf.fit(X, y)
print("랜덤 포레스트 예측:", rf.predict([[4, 4]]))
```

**출력 예시**:
```
랜덤 포레스트 예측: [1]
```

### 주의사항

- 랜덤 포레스트는 학습 시간이 길고 메모리 사용량이 많을 수 있다.
- 트리 수가 많아질수록 노이즈에 강건해지지만, 과도한 트리는 오히려 성능 저하를 유발할 수 있다.

---

## 랜덤 포레스트 활용 예시 🎮

랜덤 포레스트는 Microsoft의 키넥트 센서 등에서 사람의 자세 인식에 활용되어 유명해졌다. 관절 정보를 입력받아 복잡한 패턴을 효과적으로 분류할 수 있다.

---

## 부스팅 개념 🚀

**부스팅(Boosting)** 은 여러 약한 학습기(weak learner)를 순차적으로 결합하여 강한 예측 모델을 만드는 앙상블 기법이다. 각 단계에서 이전 모델이 잘못 예측한 데이터에 더 높은 가중치를 부여하여, 오류를 점진적으로 줄여나간다.

### 예시

```python
from sklearn.ensemble import AdaBoostClassifier

# 데이터 예시
ada = AdaBoostClassifier(n_estimators=5, random_state=42)
ada.fit(X, y)
print("부스팅 예측:", ada.predict([[4, 4]]))
```

**출력 예시**:
```
부스팅 예측: [1]
```

### 주의사항

- 부스팅은 순차적으로 모델을 추가하므로, 병렬 처리가 어렵고 학습 시간이 길어질 수 있다.

---

## 경사 하강법 📉

**경사 하강법(Gradient Descent)** 은 함수의 기울기를 따라 최솟값을 찾는 최적화 방법이다. 미분을 통해 함수의 기울기를 계산하고, 기울기가 낮은 방향으로 파라미터를 조정하여 손실 함수를 최소화한다.

### 예시

```python
# 1차원 함수의 경사 하강법 예시
def loss(x):
    return (x - 3) ** 2

x = 0
learning_rate = 0.1
for i in range(10):
    grad = 2 * (x - 3)
    x -= learning_rate * grad
print("최적화된 x 값:", x)
```

**출력 예시**:
```
최적화된 x 값: 2.874
```

### 주의사항

- 학습률이 너무 크면 발산, 너무 작으면 수렴 속도가 느려진다.

---

## 그레디언트 부스팅 🔄

**그레디언트 부스팅(Gradient Boosting)** 은 기존 트리를 수정하지 않고, 직전 트리의 오류(잔차)를 경사 하강법으로 최소화하는 새로운 트리를 순차적으로 추가한다. 여러 결정 트리의 선형 조합으로 최종 결과를 산출하며, 테스트 데이터 성능이 더 이상 개선되지 않을 때까지 학습을 반복한다.

### 예시

```python
from sklearn.ensemble import GradientBoostingClassifier

# 데이터 예시
gb = GradientBoostingClassifier(n_estimators=10, random_state=42)
gb.fit(X, y)
print("그레디언트 부스팅 예측:", gb.predict([[4, 4]]))
```

**출력 예시**:
```
그레디언트 부스팅 예측: [1]
```

### 주의사항

- 과적합 방지를 위해 트리의 깊이, 학습률 등 하이퍼파라미터 조정이 중요하다.

---

## XGBoost 특징 🏆

**XGBoost(Extreme Gradient Boosting)** 는 그레디언트 부스팅 알고리즘에 병렬 학습, 정규화 등 다양한 최적화 기능을 추가한 라이브러리이다. 분류와 회귀 문제 모두에 적용 가능하며, 높은 계산 효율과 성능으로 데이터 분석 대회에서 널리 사용된다.

### 예시

```python
import xgboost as xgb

# 데이터 예시
dtrain = xgb.DMatrix(np.array(X), label=np.array(y))
params = {'objective': 'binary:logistic', 'max_depth': 2, 'eta': 0.1}
bst = xgb.train(params, dtrain, num_boost_round=5)
pred = bst.predict(xgb.DMatrix([[4, 4]]))
print("XGBoost 예측 확률:", pred)
```

**출력 예시**:
```
XGBoost 예측 확률: [0.85]
```

### 주의사항

- XGBoost는 하이퍼파라미터가 많아, 최적의 성능을 위해서는 세밀한 튜닝이 필요하다.

---

## XGBoost 대회 성적 🥇

XGBoost는 데이터 분석 대회 플랫폼인 **Kaggle**에서 우수한 성적을 거두며 유명해졌다. 특히, 다양한 대회에서 다른 모델을 압도적으로 이기며 데이터 과학자들에게 널리 채택되고 있다.

---

## 분류 모델 비교 ⚖️

| 모델            | 특징                                 | 장점                        | 단점                    |
|-----------------|--------------------------------------|-----------------------------|-------------------------|
| **KNN**         | 거리 기반, 게으른 학습               | 구현이 간단, 해석 용이      | 고차원에서 성능 저하    |
| **결정 트리**   | 규칙 기반 분할                       | 시각화 용이, 해석 쉬움      | 과적합 발생 쉬움        |
| **랜덤 포레스트**| 배깅 기반 앙상블                     | 과적합 완화, 성능 안정적    | 학습 시간, 해석 어려움  |
| **XGBoost**     | 부스팅 기반, 경사 하강법 적용        | 빠른 학습, 높은 성능        | 파라미터 튜닝 필요      |

---

## 회귀 모델 평가 지표 📏

회귀 모델의 성능을 평가하는 주요 지표는 다음과 같다.

- **MSE(Mean Squared Error)**: 오차의 제곱 평균으로, 큰 오차에 더 큰 가중치를 부여한다. 이상치에 민감하다.
- **MAE(Mean Absolute Error)**: 오차의 절대값 평균으로, 이상치에 덜 민감하다.
- **RMSE(Root Mean Squared Error)**: MSE의 제곱근으로, 단위가 종속 변수와 동일하다.
- **R²(R Square)**: 모델이 데이터의 변동성을 얼마나 설명하는지 나타내며, 1에 가까울수록 설명력이 높다.

### 예시

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

y_true = np.array([3.0, 2.5, 4.0, 5.0])
y_pred = np.array([2.8, 2.7, 3.9, 5.2])

print("MSE:", mean_squared_error(y_true, y_pred))
print("MAE:", mean_absolute_error(y_true, y_pred))
print("R2:", r2_score(y_true, y_pred))
```

### 주의사항

- 회귀 지표는 연속형 변수 예측에만 사용해야 한다.

---

## 오차 행렬 🧮

**오차 행렬(Confusion Matrix)** 은 분류 모델의 예측 결과를 실제 값과 비교하여, 각 유형의 예측(정답/오답)을 표로 나타낸다.

| 실제\예측 | Negative | Positive |
|-----------|----------|----------|
| Negative  | TN       | FP       |
| Positive  | FN       | TP       |

- **TP(True Positive)**: 실제 Positive, 예측도 Positive
- **TN(True Negative)**: 실제 Negative, 예측도 Negative
- **FP(False Positive)**: 실제 Negative, 예측은 Positive
- **FN(False Negative)**: 실제 Positive, 예측은 Negative

### 예시

```python
from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 1, 0, 0, 1]
y_pred = [1, 0, 0, 1, 0, 1, 1]
cm = confusion_matrix(y_true, y_pred)
print("오차 행렬:\n", cm)
```

### 주의사항

- 오차 행렬을 통해 정밀도, 재현율 등 다양한 지표를 계산할 수 있다.

---

## 정밀도와 재현율 🎯

- **정밀도(Precision)**: Positive로 예측한 것 중 실제 Positive의 비율  
  Precision = TP / (TP + FP)
- **재현율(Recall)**: 실제 Positive 중 올바르게 Positive로 예측한 비율  
  Recall = TP / (TP + FN)

### 예시

```python
from sklearn.metrics import precision_score, recall_score

print("정밀도:", precision_score(y_true, y_pred))
print("재현율:", recall_score(y_true, y_pred))
```

**출력 예시**:
```
정밀도: 0.75
재현율: 0.75
```

### 주의사항

- 정밀도는 스팸 필터 등에서 중요하며, 재현율은 질병 진단 등에서 중요하다.
- 두 지표는 상충 관계에 있으므로, 상황에 따라 우선순위를 결정해야 한다.

---

## F1 점수 🏅

**F1 점수**는 정밀도와 재현율의 조화 평균으로, 두 지표의 균형을 평가한다.  
F1 = 2 * (Precision * Recall) / (Precision + Recall)

### 예시

```python
from sklearn.metrics import f1_score

print("F1 점수:", f1_score(y_true, y_pred))
```

**출력 예시**:
```
F1 점수: 0.75
```

### 주의사항

- 임계값 조정에 따른 성능 조작을 방지하는 데 유용하다.

---

## AUC-ROC 점수 📈

**AUC-ROC**는 **ROC(Receiver Operating Characteristic)** 곡선 아래의 면적으로, 분류 모델의 전체적인 성능을 평가한다.  
- AUC가 0.5에 가까우면 무작위 분류, 1에 가까우면 완벽한 분류를 의미한다.

### 예시

```python
from sklearn.metrics import roc_auc_score

y_prob = [0.9, 0.2, 0.4, 0.8, 0.3, 0.6, 0.7]
print("AUC-ROC 점수:", roc_auc_score(y_true, y_prob))
```

**출력 예시**:
```
AUC-ROC 점수: 0.9167
```

### 주의사항

- AUC-ROC는 이진 분류에서 주로 사용되며, 클래스 불균형 상황에서 유용하다.

---

## 주요 분류 지표 요약 📝

| 지표         | 정의 및 공식                                      | 특징 및 활용 예시                |
|--------------|---------------------------------------------------|----------------------------------|
| **정답률** (Accuracy)   | (TP + TN) / 전체 샘플 수                         | 전체 예측의 정확도               |
| **정밀도** (Precision)  | TP / (TP + FP)                                 | 스팸 필터 등에서 중요            |
| **재현율** (Recall)     | TP / (TP + FN)                                 | 질병 진단 등에서 중요            |
| **F1 점수** (F1 Score)  | 2 * (Precision * Recall) / (Precision + Recall) | 정밀도와 재현율의 균형           |
| **AUC-ROC**             | ROC 곡선 아래 면적                              | 전체 분류 성능, 불균형 데이터에 유리 |

---

> **⚠️ 주의사항**: 분류 모델의 성능 평가는 단일 지표만으로 판단하지 말고, 다양한 지표를 함께 고려해야 한다. 데이터의 특성과 목적에 따라 적합한 지표를 선택하는 것이 중요하다.

---