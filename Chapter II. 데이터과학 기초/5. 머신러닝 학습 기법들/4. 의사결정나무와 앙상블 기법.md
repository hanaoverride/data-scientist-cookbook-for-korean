# 4. 의사결정나무와 앙상블 기법 🌳

## 목차 📑

### 1. 의사결정나무
- [1.1 개념 및 구조](#의사결정나무-개념-및-구조-) 🌲
- [1.2 특징](#의사결정나무-특징-) ⭐
- [1.3 회귀와 분류](#의사결정나무-회귀와-분류-) 🔀

### 2. 의사결정나무 회귀
- [2.1 회귀 트리 원리](#회귀-트리-원리-) 📉
- [2.2 RSS와 분할](#rss와-분할-) 🧮
- [2.3 회귀 트리 구현](#회귀-트리-구현-) 🛠️

### 3. 의사결정나무 분류
- [3.1 분류 트리 원리](#분류-트리-원리-) 🗂️
- [3.2 불순도 지표](#불순도-지표-) 📏
- [3.3 분류 트리 구현](#분류-트리-구현-) 🛠️

### 4. 앙상블 기법
- [4.1 개념 및 종류](#앙상블-기법-개념-및-종류-) 🧩
- [4.2 보팅(Voting)](#보팅-voting-) 🗳️
- [4.3 배깅(Bagging)](#배깅-bagging-) 👜
- [4.4 부스팅(Boosting)](#부스팅-boosting-) 🚀

### 5. 랜덤포레스트와 부스팅 알고리즘
- [5.1 랜덤포레스트](#랜덤포레스트-) 🌳🌳
- [5.2 AdaBoost](#adaboost-) ⚡
- [5.3 Gradient Boosting](#gradient-boosting-) 📈

### 6. Gradient Boosting 기반 모델
- [6.1 XGBoost](#xgboost-) 🏆
- [6.2 LightGBM](#lightgbm-) 💡
- [6.3 CatBoost](#catboost-) 🐱

---

## 의사결정나무 개념 및 구조 🌲

**의사결정나무(Decision Tree)** 는 데이터를 반복적으로 분할하여 예측 또는 분류를 수행하는 지도학습 알고리즘이다. 트리 구조는 **Root Node(뿌리 노드)** 에서 시작해 여러 **자식 노드**를 거쳐 **Terminal Node(잎 노드)** 에 도달한다. 각 분할은 특정 조건(질문)에 따라 이루어지며, 최종적으로 각 잎 노드에는 예측값 또는 분류 결과가 저장된다.

트리의 각 **노드(Node)** 는 다음과 같이 구분된다:
- **Root Node**: 트리의 최상단에 위치하며 전체 데이터를 포함한다.
- **내부 노드(Internal Node)**: 데이터를 조건에 따라 분할하는 역할을 한다.
- **Terminal Node(잎 노드, Leaf Node)**: 더 이상 분할이 없는 최종 노드로, 예측값 또는 클래스가 할당된다.
- **서브트리(Sub-tree)**: 트리의 일부로, 가지치기(pruning) 등에 활용된다.

---

## 의사결정나무 특징 ⭐

의사결정나무는 **직관적**이고 **해석이 쉬운** 모델로, 데이터의 **비선형 관계**도 효과적으로 모델링할 수 있다. 데이터 전처리에 덜 민감하며, 결측치나 이상치에 어느 정도 강인하다. 하지만 트리의 깊이가 지나치게 깊어지면 **과적합**이 발생할 수 있으므로 주의가 필요하다.

> **⚠️ 주의사항**: 트리의 깊이가 깊어질수록 훈련 데이터에 과도하게 적합(overfitting)될 수 있으므로, `max_depth` 등 하이퍼파라미터 조정이 중요하다.

---

## 의사결정나무 회귀와 분류 🔀

의사결정나무는 **회귀(Regression)** 와 **분류(Classification)** 문제 모두에 적용할 수 있다. 회귀 트리는 연속적인 값을 예측하며, 분류 트리는 범주형 클래스를 예측한다.

---

## 회귀 트리 원리 📉

**회귀 의사결정나무**는 입력 데이터를 여러 구간으로 분할하여 각 구간의 평균값 등으로 예측값을 할당한다. 각 분할은 데이터의 **잔차 제곱합(RSS, Residual Sum of Squares)**을 최소화하는 방향으로 이루어진다.

---

## RSS와 분할 🧮

회귀 트리에서 각 분할 기준은 해당 구간의 **실제값(Y)** 과 **예측값**의 오차 제곱의 합, 즉 **RSS**를 최소화하는 방향으로 선택된다. 모든 가능한 분할을 한 번에 탐색하는 것은 비효율적이므로, **Top-down Greedy Approach** 방식으로 각 단계에서 최적의 분할을 선택한다.

---

## 회귀 트리 구현 🛠️

회귀 의사결정나무는 `sklearn.tree.DecisionTreeRegressor` 클래스를 사용하여 구현할 수 있다. 트리의 최대 깊이 등 하이퍼파라미터를 조정하여 모델의 복잡도를 제어한다.

```python
from sklearn.tree import DecisionTreeRegressor

# 예시 데이터 생성
X = [[2], [4], [6], [8], [10]]
y = [5, 9, 13, 17, 21]

# 회귀 트리 모델 생성 및 학습
reg_tree = DecisionTreeRegressor(max_depth=3)
reg_tree.fit(X, y)

# 예측
pred = reg_tree.predict([[7]])
print(f"7에 대한 예측값: {pred[0]}")
```

**출력 예시**:
```
7에 대한 예측값: 13.0
```

> **주의사항**: 회귀 트리의 분할 기준은 RSS(잔차 제곱합) 최소화이며, 분할이 지나치게 많아지면 과적합이 발생할 수 있다.

---

## 분류 트리 원리 🗂️

**분류 의사결정나무**는 데이터를 여러 구간으로 분할하여 각 구간에 **클래스 레이블**을 할당한다. 분할 기준은 해당 구간의 **불순도(Impurity)**를 최소화하는 방향으로 선택된다.

---

## 불순도 지표 📏

분류 트리에서 노드의 **불순도**는 데이터가 얼마나 섞여 있는지를 나타낸다. 대표적인 불순도 지표로는 **지니지수(Gini Index)** 와 **엔트로피(Entropy)** 가 있다.

- **지니지수(Gini Index)**:  
  $$
  Gini = 1 - \sum_{i=1}^{C} p_i^2
  $$
  여기서 $p_i$는 클래스 $i$의 비율이다.

- **엔트로피(Entropy)**:  
  $$
  Entropy = -\sum_{i=1}^{C} p_i \log_2 p_i
  $$
  엔트로피가 낮을수록 노드가 순수하다.

> **주의사항**: 불순도 지표 선택에 따라 트리의 분할 방식과 결과가 달라질 수 있다. `criterion` 파라미터로 `'gini'`, `'entropy'`, `'log_loss'` 등을 선택할 수 있다.

---

## 분류 트리 구현 🛠️

분류 의사결정나무는 `sklearn.tree.DecisionTreeClassifier` 클래스를 사용하여 구현할 수 있다. 분할 기준은 `criterion` 파라미터로 지정한다.

```python
from sklearn.tree import DecisionTreeClassifier

# 예시 데이터 생성
X = [[1, 2], [2, 3], [3, 1], [6, 5], [7, 8], [8, 6]]
y = ['A', 'A', 'A', 'B', 'B', 'B']

# 분류 트리 모델 생성 및 학습
clf = DecisionTreeClassifier(criterion='entropy', max_depth=2)
clf.fit(X, y)

# 예측
pred = clf.predict([[4, 4]])
print(f"입력 [4, 4]의 예측 클래스: {pred[0]}")
```

**출력 예시**:
```
입력 [4, 4]의 예측 클래스: A
```

> **주의사항**: 분류 트리의 분할 기준은 불순도(지니지수, 엔트로피 등) 최소화이며, 클래스 불균형이 심한 경우 성능 저하가 발생할 수 있다.

---

## 앙상블 기법 개념 및 종류 🧩

**앙상블(Ensemble) 기법**은 여러 개의 모델을 결합하여 단일 모델보다 더 높은 예측 성능과 안정성을 얻는 방법이다. 대표적인 앙상블 기법에는 **보팅(Voting)**, **배깅(Bagging)**, **부스팅(Boosting)** 이 있다.

- **보팅(Voting)**: 서로 다른 모델의 예측 결과를 결합하여 최종 예측을 도출한다.
- **배깅(Bagging)**: 데이터 샘플을 복원 추출하여 여러 모델을 학습시키고, 결과를 결합한다.
- **부스팅(Boosting)**: 약한 모델을 순차적으로 학습시키며, 이전 모델의 오분류 데이터를 보완한다.

---

## 보팅(Voting) 🗳️

**보팅**은 여러 모델의 예측 결과를 모아 최종 결정을 내리는 방식이다. 분류에서는 **다수결(하드 보팅)** 또는 **확률 평균(소프트 보팅)** 을 사용하며, 회귀에서는 예측값의 평균을 사용한다.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier

# 예시 모델 생성
model1 = LogisticRegression()
model2 = KNeighborsClassifier()

# 보팅 분류기 생성 (소프트 보팅)
voting_clf = VotingClassifier(
    estimators=[('lr', model1), ('knn', model2)],
    voting='soft'
)

# 학습 및 예측
voting_clf.fit([[0, 1], [1, 1], [2, 0], [2, 2]], [0, 0, 1, 1])
result = voting_clf.predict([[1, 0]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 1
```

> **주의사항**: 보팅에 참여하는 모델들은 서로 다른 알고리즘을 사용할 수 있으며, 소프트 보팅은 각 모델이 `predict_proba`를 지원해야 한다.

---

## 배깅(Bagging) 👜

**배깅**은 데이터셋을 복원 추출(bootstrap)하여 여러 개의 하위 데이터셋을 만들고, 각각의 모델을 독립적으로 학습시킨 후 결과를 결합한다. 대표적으로 **랜덤포레스트**가 있다.

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# 배깅 분류기 생성
bagging_clf = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=10
)

# 학습 및 예측
bagging_clf.fit(X, y)
result = bagging_clf.predict([[2, 2]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 0
```

> **주의사항**: 배깅은 데이터가 불균형하거나 적을 때도 효과적이며, 각 모델이 서로 다른 데이터셋을 학습하므로 다양성이 확보된다.

---

## 부스팅(Boosting) 🚀

**부스팅**은 여러 개의 약한 모델(Weak Learner)을 순차적으로 학습시키며, 이전 모델이 잘못 예측한 데이터에 더 많은 가중치를 부여하여 점차 성능을 개선하는 방식이다. 대표적인 부스팅 계열로 **AdaBoost**, **Gradient Boosting** 등이 있다.

---

## 랜덤포레스트 🌳🌳

**랜덤포레스트(Random Forest)** 는 여러 개의 의사결정나무를 배깅 방식으로 학습시키고, 각 트리의 예측 결과를 다수결(분류) 또는 평균(회귀)으로 결합하는 앙상블 모델이다. 각 트리는 데이터 샘플뿐만 아니라 입력 변수도 무작위로 선택하여 학습한다.

```python
from sklearn.ensemble import RandomForestClassifier

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# 랜덤포레스트 분류기 생성 및 학습
rf_clf = RandomForestClassifier(n_estimators=50, max_features=1)
rf_clf.fit(X, y)

# 예측
result = rf_clf.predict([[3, 2]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 0
```

> **주의사항**: 랜덤포레스트는 변수 일부만 사용하여 각 트리를 학습하므로 과적합을 방지할 수 있다. 변수 중요도(feature importance) 평가에도 활용된다.

---

## AdaBoost ⚡

**AdaBoost(Adaptive Boosting)** 는 이전 단계에서 오분류된 데이터에 더 높은 가중치를 부여하여 다음 모델이 이를 더 잘 학습하도록 한다. 각 단계에서 모델의 가중치와 데이터의 가중치가 반복적으로 조정된다.

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# AdaBoost 분류기 생성 및 학습
ada_clf = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=20
)
ada_clf.fit(X, y)

# 예측
result = ada_clf.predict([[2, 3]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 0
```

> **주의사항**: AdaBoost는 순차적으로 모델을 학습하므로 병렬 처리가 어렵다. 다양한 약한 모델과 결합이 가능하며, 오분류 데이터에 집중하여 일반화 성능을 높인다.

---

## Gradient Boosting 📈

**Gradient Boosting**은 오차(잔차)를 줄이기 위해 각 단계에서 이전 모델의 예측 오차에 대해 새로운 모델을 학습시키는 방식이다. 오차의 기울기(gradient)를 따라 모델 파라미터를 최적화한다.

```python
from sklearn.ensemble import GradientBoostingClassifier

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# Gradient Boosting 분류기 생성 및 학습
gb_clf = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1)
gb_clf.fit(X, y)

# 예측
result = gb_clf.predict([[5, 5]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 1
```

> **주의사항**: Gradient Boosting은 계산량이 많으나, 높은 예측 성능을 보인다. 학습률(learning_rate)과 트리 개수(n_estimators) 등 하이퍼파라미터 조정이 중요하다.

---

## XGBoost 🏆

**XGBoost(eXtreme Gradient Boosting)** 는 Gradient Boosting을 개선한 모델로, 정규화와 분산 병렬 처리 등 다양한 기능을 제공한다. 과적합 방지와 빠른 연산이 특징이다.

```python
import xgboost as xgb

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# XGBoost 분류기 생성 및 학습
xgb_clf = xgb.XGBClassifier(n_estimators=25, use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X, y)

# 예측
result = xgb_clf.predict([[3, 4]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 1
```

> **주의사항**: XGBoost는 다양한 하이퍼파라미터를 지원하며, 분산 환경에서 빠른 학습이 가능하다. 과적합 방지 옵션이 다양하다.

---

## LightGBM 💡

**LightGBM(Light Gradient Boosting Machine)** 은 XGBoost보다 더 빠르고 메모리 효율적인 Gradient Boosting 모델이다. 대용량 데이터와 범주형 변수 처리에 강점을 가진다.

```python
import lightgbm as lgb

# 예시 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4], [6, 6]]
y = [0, 0, 0, 1, 1, 1]

# LightGBM 분류기 생성 및 학습
lgbm_clf = lgb.LGBMClassifier(n_estimators=20)
lgbm_clf.fit(X, y)

# 예측
result = lgbm_clf.predict([[4, 4]])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 1
```

> **주의사항**: LightGBM은 소규모 데이터에서는 과적합이 발생할 수 있으므로, 데이터 크기와 하이퍼파라미터를 신중히 조정해야 한다.

---

## CatBoost 🐱

**CatBoost(Categorical Boosting)** 는 범주형 변수 처리가 자동화된 Gradient Boosting 모델이다. 별도의 인코딩 없이 범주형 데이터를 직접 입력할 수 있으며, 변수 간 상관관계도 효과적으로 반영한다.

```python
from catboost import CatBoostClassifier

# 예시 데이터
X = [[1, 'red'], [2, 'blue'], [3, 'red'], [4, 'green'], [5, 'blue'], [6, 'green']]
y = [0, 0, 0, 1, 1, 1]

# CatBoost 분류기 생성 및 학습
cat_clf = CatBoostClassifier(iterations=15, verbose=0)
cat_clf.fit(X, y, cat_features=[1])

# 예측
result = cat_clf.predict([[2, 'green']])
print(f"예측 결과: {result[0]}")
```

**출력 예시**:
```
예측 결과: 1
```

> **주의사항**: CatBoost는 범주형 변수 자동 처리가 가능하나, 데이터 타입이 일관되지 않으면 오류가 발생할 수 있다. 다중 범주형 데이터셋에 특히 유용하다.