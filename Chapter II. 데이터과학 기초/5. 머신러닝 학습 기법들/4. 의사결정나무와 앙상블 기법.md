# 4. 앙상블 학습: 약한 모델들이 모여 세상을 구하는 법

## 목차
- [4. 앙상블 학습: 약한 모델들이 모여 세상을 구하는 법](#4-앙상블-학습-약한-모델들이-모여-세상을-구하는-법)
  - [목차](#목차)
  - [1. 앙상블의 핵심 철학: 편향-분산 트레이드오프](#1-앙상블의-핵심-철학-편향-분산-트레이드오프)
  - [2. 배깅 (Bagging): 분산을 줄이는 전략](#2-배깅-bagging-분산을-줄이는-전략)
    - [랜덤 포레스트 (Random Forest): 배깅의 왕](#랜덤-포레스트-random-forest-배깅의-왕)
  - [3. 부스팅 (Boosting): 편향을 줄이는 전략](#3-부스팅-boosting-편향을-줄이는-전략)
    - [그래디언트 부스팅 (Gradient Boosting): 오차를 학습하는 똑똑한 방법](#그래디언트-부스팅-gradient-boosting-오차를-학습하는-똑똑한-방법)
  - [4. 현대 부스팅 삼대장: XGBoost, LightGBM, CatBoost](#4-현대-부스팅-삼대장-xgboost-lightgbm-catboost)
    - [언제 무엇을 쓸까?](#언제-무엇을-쓸까)

---

## 1. 앙상블의 핵심 철학: 편향-분산 트레이드오프

모델의 총 오차는 크게 **편향(Bias)**  과 **분산(Variance)**  이라는 두 가지 요소로 나눌 수 있습니다.
- **편향**: 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못하는 경향. (예측이 한쪽으로 치우침)
- **분산**: 모델이 훈련 데이터의 노이즈까지 너무 민감하게 학습하여, 데이터가 조금만 바뀌어도 예측이 크게 흔들리는 경향.

**편향과 분산은 서로 반비례 관계(Trade-off)**  에 있습니다. 모델이 복잡해질수록 편향은 줄지만 분산은 커지고, 모델이 단순해질수록 분산은 줄지만 편향은 커집니다.

> **앙상블의 목표**: 앙상블 기법은 이 편향-분산 트레이드오프를 현명하게 관리하여, **편향과 분산을 모두 낮은 상태로 유지**함으로써 모델의 전체적인 성능을 극대화하는 것을 목표로 합니다.

## 2. 배깅 (Bagging): 분산을 줄이는 전략

**배깅(Bagging, Bootstrap Aggregating)**  은 **분산이 높은 모델들**의 예측을 안정시키는 데 탁월한 전략입니다.
- **핵심 원리**:
    1.  원본 훈련 데이터에서 **복원 추출(Bootstrap)**  을 통해 여러 개의 서로 다른 훈련 데이터셋을 만듭니다.
    2.  각 데이터셋으로 **독립적인** 모델(주로 의사결정나무)을 각각 학습시킵니다.
    3.  모든 모델의 예측 결과를 **평균(회귀) 또는 다수결 투표(분류)**  하여 최종 예측을 만듭니다.
- **효과**: 각각의 모델은 특정 데이터에 과적합되어 분산이 높을 수 있지만, 이들의 예측을 평균내면 **서로의 오류가 상쇄되면서 전체 모델의 분산이 크게 줄어듭니다.**

### 랜덤 포레스트 (Random Forest): 배깅의 왕

랜덤 포레스트는 배깅의 대표적인 알고리즘으로, 의사결정나무를 기반으로 합니다. 일반적인 배깅에 한 가지 규칙을 더 추가합니다.
- **'랜덤'의 비밀**: 각 트리를 학습할 때, 전체 특성 중 **일부 특성만 무작위로 선택**하여 사용합니다.
- **왜?**: 이렇게 하면 각 트리들이 서로 다른 특성을 기반으로 학습하게 되어, 트리들 간의 **상관관계를 줄여줍니다.** 결과적으로 개별 트리의 성능은 조금 나빠질 수 있지만, 전체 앙상블의 분산은 훨씬 더 효과적으로 감소하여 최종 성능이 향상됩니다.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=100, n_features=20, random_state=42)

# 100개의 의사결정나무를 사용하는 랜덤 포레스트
# 각 트리는 최대 5개의 특성만 무작위로 사용 (max_features)
rf_model = RandomForestClassifier(n_estimators=100, max_features=5, random_state=42)
rf_model.fit(X, y)
```

## 3. 부스팅 (Boosting): 편향을 줄이는 전략

**부스팅(Boosting)**  은 **편향이 높은 약한 모델들**을 순차적으로 결합하여 강력한 모델 하나를 만드는 전략입니다.
- **핵심 원리**:
    1.  간단한 모델(약한 학습기)을 하나 학습시킵니다.
    2.  이 모델이 **틀린 문제(오차)에 집중**하여, 다음 모델이 이 오차를 잘 맞추도록 학습시킵니다.
    3.  이 과정을 반복하며, 이전 모델의 실수를 계속 보완해나가는 새로운 모델을 순차적으로 추가합니다.
- **효과**: 얕은 의사결정나무처럼 편향은 높지만 분산은 낮은 모델들을 계속 더해가면서, 점진적으로 **전체 모델의 편향을 줄여나갑니다.**

### 그래디언트 부스팅 (Gradient Boosting): 오차를 학습하는 똑똑한 방법

그래디언트 부스팅은 부스팅의 가장 대표적인 알고리즘입니다. '이전 모델의 오차'를 학습한다는 아이디어를 **손실 함수의 경사(Gradient)**  를 이용하여 일반화한 것입니다. 각 단계의 새로운 모델은, 전체 손실 함수를 가장 가파르게 줄일 수 있는 방향(음의 그래디언트)으로 예측하도록 학습됩니다.

## 4. 현대 부스팅 삼대장: XGBoost, LightGBM, CatBoost

그래디언트 부스팅을 기반으로, 성능과 속도를 극적으로 개선한 세 가지 라이브러리가 현대 머신러닝 대회의 승자를 결정짓고 있습니다.

- **XGBoost (eXtreme Gradient Boosting)** : 병렬 처리, 규제(Regularization) 기능 추가 등으로 그래디언트 부스팅의 성능과 속도를 크게 향상시킨 선구자.
- **LightGBM (Light Gradient Boosting Machine)** : XGBoost보다 더 빠른 학습 속도와 적은 메모리 사용량을 자랑합니다. 특히 대용량 데이터에서 강력한 성능을 보입니다.
- **CatBoost (Categorical Boosting)** : **범주형 변수**를 별도의 인코딩 없이 매우 효과적으로 처리하는 기능이 내장되어 있습니다.

### 언제 무엇을 쓸까?

| 모델 | 추천 상황 | 특징 |
| :--- | :--- | :--- |
| **XGBoost** | 안정성과 범용성이 중요할 때, 커뮤니티와 자료가 풍부함 | "부스팅의 표준" |
| **LightGBM** | **대용량 데이터**를 다룰 때, **학습 속도**가 가장 중요할 때 | 매우 빠름, 소규모 데이터에는 과적합 위험 |
| **CatBoost** | 데이터에 **범주형 변수가 매우 많을 때** | 원-핫 인코딩 등 전처리 과정 단축 가능 |
