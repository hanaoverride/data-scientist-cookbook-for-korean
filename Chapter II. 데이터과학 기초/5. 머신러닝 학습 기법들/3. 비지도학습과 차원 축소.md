# 3. 비지도 학습: 정답 없는 데이터에서 숨은 보석 찾기

## 목차
- [3. 비지도 학습: 정답 없는 데이터에서 숨은 보석 찾기](#3-비지도-학습-정답-없는-데이터에서-숨은-보석-찾기)
  - [목차](#목차)
  - [1. 클러스터링: 데이터에 숨겨진 그룹 찾기](#1-클러스터링-데이터에-숨겨진-그룹-찾기)
    - [K-평균 클러스터링 (K-Means): 쉽고 빠른 그룹화의 대명사](#k-평균-클러스터링-k-means-쉽고-빠른-그룹화의-대명사)
    - [가우시안 혼합 모델 (GMM): 유연한 확률적 그룹화](#가우시안-혼합-모델-gmm-유연한-확률적-그룹화)
    - [K-Means vs. GMM: 언제 무엇을 쓸까?](#k-means-vs-gmm-언제-무엇을-쓸까)
    - [클러스터링 성능 평가: 실루엣 점수](#클러스터링-성능-평가-실루엣-점수)
  - [2. 차원 축소: 복잡함 속에서 핵심만 남기기](#2-차원-축소-복잡함-속에서-핵심만-남기기)
    - [주성분 분석 (PCA): 데이터의 가장 중요한 축을 찾아서](#주성분-분석-pca-데이터의-가장-중요한-축을-찾아서)
    - [t-SNE: 고차원 데이터를 시각화하는 마법](#t-sne-고차원-데이터를-시각화하는-마법)
    - [PCA vs. t-SNE: 언제 무엇을 쓸까?](#pca-vs-t-sne-언제-무엇을-쓸까)

---

## 1. 클러스터링: 데이터에 숨겨진 그룹 찾기

클러스터링은 데이터 포인트 간의 유사성을 기반으로, 비슷한 데이터끼리 그룹(클러스터)으로 묶는 기법입니다. 고객을 구매 패턴에 따라 분류하거나, 문서를 주제별로 묶는 등 다양한 곳에 활용됩니다.

### K-평균 클러스터링 (K-Means): 쉽고 빠른 그룹화의 대명사

- **핵심 원리**:
    1.  사용자가 지정한 `K`개의 중심점(Centroid)을 무작위로 정합니다.
    2.  모든 데이터 포인트를 가장 가까운 중심점에 할당합니다.
    3.  각 클러스터에 속한 데이터들의 평균값으로 중심점을 업데이트합니다.
    4.  중심점이 더 이상 변하지 않을 때까지 2-3번을 반복합니다.
- **특징**:
    - **장점**: 구현이 간단하고, 계산 속도가 빠르며, 대용량 데이터에도 잘 동작합니다.
    - **단점**: 클러스터의 모양이 **원형(spherical)**  이라고 가정하므로, 길쭉하거나 복잡한 모양의 클러스터는 잘 찾지 못합니다. `K`값을 사용자가 직접 정해야 합니다.

### 가우시안 혼합 모델 (GMM): 유연한 확률적 그룹화

- **핵심 원리**: 데이터가 `K`개의 **가우시안(정규) 분포**가 혼합되어 생성되었다고 가정합니다. 각 데이터 포인트가 특정 클러스터에 속할 **확률**을 계산하여, 가장 확률이 높은 클러스터에 할당합니다. (Soft Clustering)
- **특징**:
    - **장점**: K-Means와 달리, **타원형** 등 다양한 모양의 클러스터를 잘 찾아내며, 각 데이터가 여러 클러스터에 속할 확률을 제공하여 더 유연한 분석이 가능합니다.
    - **단점**: K-Means보다 계산량이 많고, 수렴하는 데 시간이 더 오래 걸릴 수 있습니다.

### K-Means vs. GMM: 언제 무엇을 쓸까?

| 구분 | K-Means | Gaussian Mixture Model (GMM) |
| :--- | :--- | :--- |
| **클러스터 모양** | **원형**만 잘 찾음 | **타원형** 등 다양한 모양 가능 |
| **클러스터링 방식** | Hard Clustering (하나의 클러스터에만 속함) | Soft Clustering (여러 클러스터에 속할 확률) |
| **속도** | 빠름 | 상대적으로 느림 |
| **추천 상황** | 클러스터가 구형일 것으로 예상될 때, 빠르고 간단한 분석이 필요할 때 | 클러스터의 모양이 다양할 때, 확률적 할당이 필요할 때 |

### 클러스터링 성능 평가: 실루엣 점수

정답이 없는 비지도학습의 성능은 어떻게 평가할까요? **실루엣 점수(Silhouette Score)**  는 가장 대표적인 클러스터링 평가 지표입니다.

- **의미**: 각 데이터 포인트가 **"자신이 속한 클러스터와는 얼마나 가깝고, 다른 클러스터와는 얼마나 먼가?"** 를 측정합니다.
- **해석**: 점수는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 클러스터링이 잘 되었다는 의미입니다.

```python
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import numpy as np

X = np.array([[1, 2], [2, 1], [8, 9], [9, 8], [1, 1]])
kmeans = KMeans(n_clusters=2, random_state=0)
labels = kmeans.fit_predict(X)
score = silhouette_score(X, labels)
print(f"실루엣 점수: {score:.2f}") # 1에 가까울수록 좋음
```

## 2. 차원 축소: 복잡함 속에서 핵심만 남기기

차원 축소는 고차원 데이터의 **'차원의 저주'** 를 피하고, 데이터의 본질적인 구조를 파악하기 위한 핵심 기술입니다.

### 주성분 분석 (PCA): 데이터의 가장 중요한 축을 찾아서

- **핵심 원리**: 데이터의 **분산(정보량)을 가장 잘 보존하는 새로운 축(주성분)**  을 찾아, 데이터를 그 축에 투영합니다. 선형적인 관계를 기반으로 데이터의 전체적인 구조를 요약하는 데 탁월합니다.
- **특징**:
    - **장점**: 계산이 빠르고, 변환된 축의 의미(분산의 크기)를 해석하기 쉽습니다. 머신러닝 모델의 **전처리 단계**에서 특성 수를 줄이는 데 널리 사용됩니다.
    - **단점**: 데이터의 비선형적인 복잡한 구조는 잘 잡아내지 못합니다.

### t-SNE: 고차원 데이터를 시각화하는 마법

- **핵심 원리**: 고차원 공간에서 가까웠던 데이터 포인트들이, 저차원 공간에서도 **가깝게 유지되도록** 위치를 최적화합니다. 데이터의 **지역적 구조(local structure)**  를 보존하는 데 매우 뛰어납니다.
- **특징**:
    - **장점**: 복잡한 고차원 데이터의 군집 구조를 2차원이나 3차원으로 매우 아름답고 직관적으로 시각화해줍니다.
    - **단점**: 계산량이 매우 많아 대용량 데이터에는 적용하기 어렵고, **오직 시각화 목적**으로만 사용해야 합니다. 변환된 축은 특별한 의미가 없으며, `fit_transform`만 지원하고 새로운 데이터에 대한 `transform`은 지원하지 않습니다.

### PCA vs. t-SNE: 언제 무엇을 쓸까?

| 구분 | PCA | t-SNE |
| :--- | :--- | :--- |
| **목적** | **차원 축소 (전처리)** , 시각화 | **시각화** |
| **기반 원리** | 전역적 분산 최대화 (선형) | 지역적 유사도 보존 (비선형) |
| **속도** | 빠름 | 매우 느림 |
| **추천 상황** | 모델의 입력 데이터 차원을 줄일 때 | 고차원 데이터의 군집 구조를 눈으로 확인하고 싶을 때 |

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()
X = digits.data
y = digits.target

# PCA
X_pca = PCA(n_components=2).fit_transform(X)

# t-SNE
X_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X)

# 시각화 비교
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='jet')
plt.title('PCA of Digits Dataset')

plt.subplot(1, 2, 2)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='jet')
plt.title('t-SNE of Digits Dataset')
plt.show()
```
> **인사이트**: PCA는 데이터의 전체적인 분산을 보존하려 하지만, t-SNE는 각 숫자 클래스(군집)를 훨씬 더 명확하게 분리하여 시각화해주는 것을 볼 수 있습니다.