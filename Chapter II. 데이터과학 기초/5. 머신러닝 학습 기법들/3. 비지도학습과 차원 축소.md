# 3. 비지도학습과 차원 축소 🧩

## 목차 📑

### 1. 비지도학습과 클러스터링
- [1.1 비지도학습 개요](#비지도학습-개요-) 🧠
- [1.2 클러스터링 개념](#클러스터링-개념-) 🔗
- [1.3 클러스터링 유형](#클러스터링-유형-) 🗂️

### 2. K-means Clustering
- [2.1 K-means 개요](#k-means-개요-) 🔵
- [2.2 K-means 동작 원리](#k-means-동작-원리-) ⚙️
- [2.3 K값 결정법](#k값-결정법-) 📈
- [2.4 K-means 특징 및 구현](#k-means-특징-및-구현-) 💡

### 3. Gaussian Mixture Model(GMM)과 타당성 지표
- [3.1 GMM 개요](#gmm-개요-) 🌐
- [3.2 GMM 동작 원리](#gmm-동작-원리-) 🔄
- [3.3 GMM 특징 및 구현](#gmm-특징-및-구현-) 🛠️
- [3.4 클러스터링 타당성 평가](#클러스터링-타당성-평가-) 🏅

### 4. 차원 축소와 주성분 분석
- [4.1 차원 축소 개념](#차원-축소-개념-) 📉
- [4.2 주성분 분석(PCA)](#주성분-분석pca-) 🧮
- [4.3 PCA 특징 및 구현](#pca-특징-및-구현-) 🛠️

### 5. t-SNE
- [5.1 t-SNE 개요](#t-sne-개요-) 🌈
- [5.2 t-SNE 원리](#t-sne-원리-) 🔬
- [5.3 t-SNE 특징 및 구현](#t-sne-특징-및-구현-) 🛠️

---

## 비지도학습 개요 🧠

**비지도학습**은 데이터에 대한 정답(레이블) 없이, 입력 데이터만을 이용해 패턴이나 구조를 학습하는 머신러닝 방법이다. 대표적으로 **클러스터링**과 **차원 축소**가 있다. 비지도학습은 데이터의 숨겨진 구조를 발견하거나, 데이터 압축 및 시각화에 활용된다.

> **주의사항**: 비지도학습은 정답이 없으므로 결과 해석에 주의가 필요하다.

---

## 클러스터링 개념 🔗

**클러스터링**은 데이터의 그룹 정보 없이, 유사한 특성을 가진 데이터끼리 자동으로 묶는 방법이다. 즉, 데이터 간의 **유사성**을 기준으로 여러 개의 집단(클러스터)로 분할한다. 클러스터링의 목적은 **군집 내 유사성 최대화**와 **군집 간 유사성 최소화**이다.

예를 들어, 온라인 쇼핑몰 고객의 구매 패턴 데이터를 분석하여, 비슷한 구매 성향을 가진 고객 그룹을 찾을 수 있다.

```python
# 예시: 고객별 구매 데이터로 클러스터링
import numpy as np
from sklearn.cluster import KMeans

# 가상의 고객별 구매 데이터 (상의, 하의, 신발 구매 개수)
purchase_data = np.array([
    [3, 1, 2],
    [2, 2, 1],
    [8, 7, 6],
    [7, 8, 5],
    [1, 0, 1]
])

# K-means로 2개 그룹으로 클러스터링
kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(purchase_data)
print("클러스터 할당 결과:", labels)
```

**출력 예시**:
```
클러스터 할당 결과: [1 1 0 0 1]
```

> **주의사항**: 클러스터링 결과는 데이터의 분포와 클러스터 개수(K)에 따라 달라진다.

---

## 클러스터링 유형 🗂️

클러스터링은 **Hard Clustering**과 **Soft Clustering**으로 나뉜다.

- **Hard Clustering**: 각 데이터가 하나의 클러스터에만 속함. 대표적으로 `KMeans`가 있다.
- **Soft Clustering**: 각 데이터가 여러 클러스터에 속할 확률을 가짐. 대표적으로 `GaussianMixture`가 있다.

```python
# 예시: Hard vs. Soft Clustering 결과 비교
import numpy as np
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

data = np.array([[1, 2], [1, 4], [5, 8], [8, 8]])

# Hard Clustering
kmeans = KMeans(n_clusters=2, random_state=0)
hard_labels = kmeans.fit_predict(data)

# Soft Clustering
gmm = GaussianMixture(n_components=2, random_state=0)
gmm.fit(data)
soft_probs = gmm.predict_proba(data)

print("Hard Clustering 결과:", hard_labels)
print("Soft Clustering 확률:\n", soft_probs)
```

> **주의사항**: Soft Clustering은 각 데이터가 여러 군집에 속할 수 있으므로, 확률값 해석에 주의해야 한다.

---

## K-means 개요 🔵

**K-means Clustering**은 데이터를 사용자가 지정한 K개의 클러스터로 나누는 대표적인 비지도학습 알고리즘이다. 각 클러스터는 중심(centroid)을 가지며, 각 데이터는 가장 가까운 중심에 할당된다. K값은 하이퍼파라미터로 직접 설정해야 한다.

---

## K-means 동작 원리 ⚙️

K-means는 다음과 같은 절차로 동작한다:

1. 데이터 중 K개를 무작위로 선택하여 초기 중심점으로 설정한다.
2. 각 데이터를 가장 가까운 중심점에 할당한다.
3. 각 클러스터에 할당된 데이터의 평균을 계산하여 새로운 중심점으로 갱신한다.
4. 중심점의 변화가 없을 때까지 2~3단계를 반복한다.

```python
# 예시: K-means 클러스터링 단계 구현
import numpy as np
from sklearn.cluster import KMeans

# 임의의 2차원 데이터
points = np.array([
    [2, 3], [3, 4], [8, 7], [7, 8], [1, 2]
])

# K=2로 클러스터링
kmeans = KMeans(n_clusters=2, random_state=1)
kmeans.fit(points)
print("중심점 좌표:", kmeans.cluster_centers_)
print("각 데이터의 클러스터:", kmeans.labels_)
```

**출력 예시**:
```
중심점 좌표: [[7.5 7.5]
             [2.  3. ]]
각 데이터의 클러스터: [1 1 0 0 1]
```

> **주의사항**: K-means는 초기 중심점 선택에 따라 결과가 달라질 수 있다.

---

## K값 결정법 📈

K-means에서 **K값**(클러스터 개수)은 직접 지정해야 하며, 적절한 K를 찾기 위해 **Elbow Method**가 자주 사용된다. Elbow Method는 K값을 늘려가며 비용 함수(군집 내 거리 합계)의 변화를 관찰하여, 그래프가 꺾이는 지점을 최적의 K로 선택한다.

```python
# 예시: Elbow Method로 최적 K 찾기
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

data = np.random.rand(50, 2)
inertia = []
k_range = range(1, 8)
for k in k_range:
    model = KMeans(n_clusters=k, random_state=0)
    model.fit(data)
    inertia.append(model.inertia_)

plt.plot(k_range, inertia, marker='o')
plt.xlabel('클러스터 개수 K')
plt.ylabel('군집 내 거리 합계')
plt.title('Elbow Method')
plt.show()
```

> **주의사항**: Elbow Method는 명확한 꺾임점이 없을 수도 있으므로, 도메인 지식과 함께 해석해야 한다.

---

## K-means 특징 및 구현 💡

**K-means**의 주요 특징은 다음과 같다:

- 데이터가 원형 분포일 때 효과적이다.
- 변수의 단위가 다를 경우, **스케일링**이 필요하다.
- 이상치에 민감하다.
- 대용량 데이터에 적합하지만, 계산량이 많다.
- 여러 번 실행하여 가장 빈번히 등장하는 결과를 선택하는 것이 좋다.

```python
# 예시: K-means 구현
from sklearn.cluster import KMeans

# 임의의 데이터
sample = [[1, 2], [2, 1], [8, 9], [9, 8], [1, 1]]
kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)
kmeans.fit(sample)
print("클러스터 할당:", kmeans.labels_)
print("중심점 좌표:", kmeans.cluster_centers_)
```

> **주의사항**: `init` 파라미터로 중심점 초기화 방법을 지정할 수 있으며, `random_state`로 결과의 재현성을 확보할 수 있다.

---

## GMM 개요 🌐

**Gaussian Mixture Model(GMM)** 은 데이터가 여러 개의 정규분포(가우시안 분포)로 이루어져 있다고 가정하고, 각 데이터가 각 분포에 속할 **확률**을 계산하여 클러스터링을 수행한다. GMM은 **Soft Clustering**의 대표적인 방법이다.

---

## GMM 동작 원리 🔄

GMM은 다음과 같은 절차로 동작한다:

1. K개의 정규분포(가우시안)를 임의로 초기화한다.
2. 각 데이터가 각 분포에 속할 확률을 계산한다.
3. 각 분포의 평균과 분산, 그리고 각 분포가 선택될 확률을 갱신한다.
4. 2~3단계를 반복하여 수렴할 때까지 진행한다.

```python
# 예시: GMM 클러스터링
import numpy as np
from sklearn.mixture import GaussianMixture

# 임의의 2차원 데이터
data = np.array([
    [2, 3], [3, 2], [8, 7], [7, 8], [1, 1]
])

gmm = GaussianMixture(n_components=2, random_state=0)
gmm.fit(data)
probs = gmm.predict_proba(data)
labels = gmm.predict(data)
print("클러스터 확률:\n", probs)
print("클러스터 할당:", labels)
```

**출력 예시**:
```
클러스터 확률:
 [[0.98 0.02]
  [0.97 0.03]
  [0.01 0.99]
  [0.02 0.98]
  [0.99 0.01]]
클러스터 할당: [0 0 1 1 0]
```

> **주의사항**: GMM은 데이터가 정규분포를 따른다고 가정하므로, 분포가 크게 다를 경우 성능이 저하될 수 있다.

---

## GMM 특징 및 구현 🛠️

**GMM**의 특징은 다음과 같다:

- 데이터 분포가 원형이 아니어도 잘 동작한다.
- 이상치에 취약하다.
- 계산량이 많아 대용량 데이터에는 부적합할 수 있다.

```python
# 예시: GMM 구현
from sklearn.mixture import GaussianMixture

# 임의의 데이터
X = [[1, 2], [2, 1], [8, 9], [9, 8], [1, 1]]
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X)
print("클러스터 할당:", gmm.predict(X))
```

> **주의사항**: `n_components`는 군집 개수, `random_state`는 무작위성 제어를 위한 파라미터이다.

---

## 클러스터링 타당성 평가 🏅

비지도학습에서는 정답이 없으므로, 클러스터링 결과의 **타당성**을 평가하는 지표가 필요하다. 대표적으로 **Dunn Index**와 **Silhouette Score**가 있다.

### Dunn Index

- **Dunn Index**는 군집 간 거리의 최소값을 군집 내 거리의 최대값으로 나눈 값이다.
- 값이 클수록 군집 간 분리가 잘 되고, 군집 내 응집도가 높음을 의미한다.

### Silhouette Score

- **Silhouette Score**는 각 데이터가 자신의 군집 내에서 얼마나 가까운지와, 다른 군집과는 얼마나 멀리 떨어져 있는지를 정량적으로 평가한다.
- -1에서 1 사이의 값을 가지며, 1에 가까울수록 군집화가 잘 된 것이다.

```python
# 예시: Silhouette Score 계산
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import numpy as np

X = np.array([[1, 2], [2, 1], [8, 9], [9, 8], [1, 1]])
kmeans = KMeans(n_clusters=2, random_state=0)
labels = kmeans.fit_predict(X)
score = silhouette_score(X, labels)
print("Silhouette Score:", score)
```

> **주의사항**: Silhouette Score는 군집 수가 2개 이상일 때만 의미가 있다.

---

## 차원 축소 개념 📉

**차원 축소(Dimensionality Reduction)** 는 고차원 데이터를 더 적은 수의 변수(특성)로 변환하여, 데이터의 구조를 최대한 보존하면서 차원을 줄이는 기법이다. 차원 축소는 **차원의 저주**를 완화하고, 학습 속도 및 성능을 향상시키며, 데이터 시각화에도 활용된다.

> **차원의 저주**란, 데이터의 차원이 높아질수록 학습에 필요한 데이터 양이 기하급수적으로 증가하고, 과적합 위험이 커지는 현상이다.

---

## 주성분 분석(PCA) 🧮

**주성분 분석(Principal Component Analysis, PCA)** 는 고차원 데이터를 가장 잘 설명하는 새로운 축(주성분)을 찾아, 데이터의 분산을 최대한 보존하면서 차원을 축소하는 방법이다. PCA는 원본 데이터와의 차이를 최소화하는 방향으로 주성분을 선택한다.

```python
# 예시: PCA로 2차원 데이터를 1차원으로 축소
import numpy as np
from sklearn.decomposition import PCA

# 임의의 2차원 데이터
data = np.array([
    [2, 3],
    [3, 5],
    [4, 7],
    [5, 9]
])

pca = PCA(n_components=1)
reduced = pca.fit_transform(data)
print("차원 축소 결과:\n", reduced)
```

**출력 예시**:
```
차원 축소 결과:
 [[-3.3541]
  [-1.3856]
  [ 0.5830]
  [ 4.1567]]
```

> **주의사항**: PCA 적용 전 변수의 단위를 맞추기 위해 **스케일링**을 수행하는 것이 좋다.

---

## PCA 특징 및 구현 🛠️

- PCA는 데이터의 분산을 최대한 보존하는 방향으로 차원을 축소한다.
- 고차원 데이터를 압축적으로 표현할 수 있으나, 해석이 직관적이지 않을 수 있다.
- 대용량 데이터의 차원 축소 및 시각화에 유용하다.

```python
# 예시: PCA 구현
from sklearn.decomposition import PCA

X = [[2, 3], [3, 5], [4, 7], [5, 9]]
pca = PCA(n_components=1)
X_reduced = pca.fit_transform(X)
print("PCA 결과:", X_reduced.flatten())
```

> **주의사항**: `n_components`는 축소할 차원 수를 의미한다.

---

## t-SNE 개요 🌈

**t-SNE(t-Distributed Stochastic Neighbor Embedding)** 는 고차원 데이터의 구조를 저차원(주로 2~3차원)으로 시각화하는 데 특화된 차원 축소 알고리즘이다. 데이터 간의 상대적 거리를 최대한 보존하여, 군집 구조를 시각적으로 명확하게 표현할 수 있다.

---

## t-SNE 원리 🔬

t-SNE는 다음과 같은 원리로 동작한다:

1. 고차원 공간에서 각 데이터 쌍의 유사도를 확률로 계산한다.
2. 저차원 공간에서 데이터의 위치를 조정하여, 고차원에서의 유사도 분포와 최대한 비슷하게 만든다.
3. 이 과정을 반복하여, 데이터 간 거리가 잘 보존된 저차원 임베딩을 얻는다.

```python
# 예시: t-SNE로 2차원 시각화
import numpy as np
from sklearn.manifold import TSNE

# 임의의 4차원 데이터
data = np.random.rand(10, 4)
tsne = TSNE(n_components=2, random_state=0)
embedded = tsne.fit_transform(data)
print("t-SNE 임베딩 결과:\n", embedded)
```

**출력 예시**:
```
t-SNE 임베딩 결과:
 [[-12.3   5.4]
  [  7.1  -8.2]
  ...]
```

> **주의사항**: t-SNE는 실행할 때마다 결과가 달라질 수 있으며, 예측 모델 입력보다는 시각화 용도로 적합하다.

---

## t-SNE 특징 및 구현 🛠️

- t-SNE는 데이터 간 거리를 최대한 보존하여, 군집 구조를 시각적으로 명확하게 드러낸다.
- 반복 실행 시 결과가 달라질 수 있으므로, 시각화에 주로 사용된다.
- 계산량이 많아 대용량 데이터에는 부적합할 수 있다.

```python
# 예시: t-SNE 구현
from sklearn.manifold import TSNE

X = np.random.rand(15, 5)
tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X)
print("t-SNE 결과:", X_embedded)
```

> **주의사항**: `fit_transform()`은 학습과 변환을 동시에 수행하며, `n_components`는 축소할 차원 수를 의미한다.

---

**요약**:  
비지도학습은 데이터의 숨겨진 구조를 발견하는 데 중점을 두며, 대표적으로 클러스터링(K-means, GMM)과 차원 축소(PCA, t-SNE)가 있다. 각 기법은 데이터의 특성과 목적에 따라 적절히 선택해야 하며, 결과 해석과 평가에 주의를 기울여야 한다.