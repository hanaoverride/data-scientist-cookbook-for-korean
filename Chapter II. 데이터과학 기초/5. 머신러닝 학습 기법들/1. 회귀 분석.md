# 1. 회귀 분석 📈

## 목차 📑

### 1. 회귀 개념
- [1.1 회귀란 무엇인가](#회귀란-무엇인가-) 🧠
- [1.2 회귀 문제의 예시](#회귀-문제의-예시-) 🍦
- [1.3 회귀의 수학적 표현](#회귀의-수학적-표현-) 📐
- [1.4 손실 함수와 최적화](#손실-함수와-최적화-) ⚙️

### 2. 단순 선형 회귀
- [2.1 단순 선형 회귀 개요](#단순-선형-회귀-개요-) 📊
- [2.2 단순 선형 회귀의 특징](#단순-선형-회귀의-특징-) 🔍

### 3. 다중 선형 회귀와 다항 회귀
- [3.1 다중 선형 회귀](#다중-선형-회귀-) 🧩
- [3.2 다항 회귀](#다항-회귀-) 🌀
- [3.3 회귀 모델 구현](#회귀-모델-구현-) 🛠️

### 4. 과적합과 정규화
- [4.1 과적합 개념](#과적합-개념-) 🚨
- [4.2 과적합 방지 방법](#과적합-방지-방법-) 🛡️

### 5. 정규화 적용 회귀
- [5.1 정규화 개요](#정규화-개요-) 🧮
- [5.2 Lasso, Ridge, Elastic Net](#lasso-ridge-elastic-net-) 🏋️
- [5.3 정규화 회귀 구현](#정규화-회귀-구현-) 🧑‍💻

### 6. 회귀 알고리즘 평가 지표
- [6.1 주요 평가 지표](#주요-평가-지표-) 📏
- [6.2 평가 지표 구현](#평가-지표-구현-) 📝

---

## 회귀란 무엇인가 🧠

**회귀(Regression)** 는 **지도학습**의 한 종류로, 입력 변수(**독립 변수**)를 바탕으로 연속적인 출력 변수(**종속 변수**)를 예측하는 알고리즘이다. 회귀는 분류(Classification)와 달리 결과값이 연속적인 수치로 나타난다. 머신러닝에서 회귀는 데이터의 패턴을 수학적으로 모델링하여 미래 값을 예측하는 데 활용된다.

> **주의사항**: 회귀는 입력과 출력 간의 관계가 반드시 선형일 필요는 없으며, 다양한 형태의 함수로 모델링할 수 있다.

---

## 회귀 문제의 예시 🍦

회귀 문제는 실생활에서 다양하게 적용된다. 예를 들어, 한 카페에서 **평균 온도**를 바탕으로 **아이스크림 판매량**을 예측하고 싶을 때 회귀 분석을 사용할 수 있다. 이때 입력 변수는 평균 온도, 출력 변수는 판매량이 된다.

```python
# 예시: 평균 온도에 따른 음료 판매량 예측
import numpy as np

# 입력 데이터(평균 온도)
temperatures = np.array([12, 16, 20, 24, 28])
# 출력 데이터(판매량, 단위: 개)
sales = np.array([30, 45, 55, 70, 85])
```

> **주의사항**: 입력 변수와 출력 변수 간의 관계가 명확하지 않거나, 데이터가 충분하지 않으면 예측 정확도가 낮아질 수 있다.

---

## 회귀의 수학적 표현 📐

회귀는 일반적으로 **선형 방정식**으로 표현된다. 단순 선형 회귀의 경우, 다음과 같이 나타낼 수 있다.

- **수식**:  
  **Y ≈ β₀ + β₁X**
    - **Y**: 예측값(종속 변수)
    - **X**: 입력값(독립 변수)
    - **β₀**: 절편(intercept)
    - **β₁**: 기울기(coefficient)

회귀의 목적은 주어진 데이터에 가장 잘 맞는 **β₀**와 **β₁**을 찾는 것이다.

```python
# 예시: 임의의 데이터에 대한 선형 회귀식
def predict(temp, intercept, slope):
    return intercept + slope * temp

result = predict(22, 10, 2.5)  # 22도에서의 예측 판매량
print(f"예측 판매량: {result}")
```

> **주의사항**: 실제 데이터는 완벽하게 선형 관계를 따르지 않으므로, 오차가 발생할 수 있다.

---

## 손실 함수와 최적화 ⚙️

회귀 모델은 **손실 함수(Loss Function)** 를 최소화하는 방향으로 학습된다. 손실 함수는 예측값과 실제값의 차이를 수치로 나타내며, 대표적으로 **평균 제곱 오차(MSE)** 가 사용된다.

- **경사하강법(Gradient Descent)** 은 손실 함수의 값을 최소로 만드는 파라미터(β₀, β₁ 등)를 찾는 최적화 알고리즘이다.

```python
# 예시: 임의의 손실 함수 계산
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

actual = np.array([40, 50, 60])
predicted = np.array([38, 52, 58])
loss = mean_squared_error(actual, predicted)
print(f"MSE: {loss}")
```

> **주의사항**: 손실 함수의 종류와 최적화 방법에 따라 모델의 성능이 달라질 수 있다.

---

## 단순 선형 회귀 개요 📊

**단순 선형 회귀(Simple Linear Regression)** 는 입력 변수가 하나일 때 사용하는 가장 기본적인 회귀 알고리즘이다. 입력값 X와 출력값 Y의 관계를 1차 함수로 모델링한다.

- **적용 예시**: 평균 온도(X)만을 이용해 음료 판매량(Y)을 예측

```python
# 예시: 단순 선형 회귀 모델 학습 및 예측
from sklearn.linear_model import LinearRegression

X = np.array([[10], [15], [20], [25], [30]])
y = np.array([25, 35, 50, 65, 80])

model = LinearRegression()
model.fit(X, y)
prediction = model.predict([[18]])
print(f"18도에서의 예측 판매량: {prediction[0]}")
```

> **주의사항**: 단순 선형 회귀는 입력 변수가 1개일 때만 적용 가능하다.

---

## 단순 선형 회귀의 특징 🔍

단순 선형 회귀는 다음과 같은 특징을 가진다.

- **직관적 해석**이 가능하며, 입력 변수와 출력 변수 간의 관계를 쉽게 파악할 수 있다.
- 입력 변수가 1개일 때만 사용 가능하다.
- 두 변수 간의 관계가 선형적일 때 효과적이다.

> **주의사항**: 입력 변수와 출력 변수 간의 관계가 비선형적이거나, 입력 변수가 여러 개인 경우에는 적합하지 않다.

---

## 다중 선형 회귀 🧩

**다중 선형 회귀(Multiple Linear Regression)** 는 입력 변수가 2개 이상일 때 사용하는 회귀 알고리즘이다. 각 입력 변수에 대해 별도의 계수를 학습하여, 여러 변수의 영향을 동시에 고려한다.

- **수식**:  
  **Y ≈ β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ**

```python
# 예시: 다중 선형 회귀 모델
from sklearn.linear_model import LinearRegression

# 입력 데이터: [온도, 강수량]
X = np.array([[12, 5], [16, 7], [20, 8], [24, 10], [28, 12]])
y = np.array([30, 42, 55, 68, 80])

model = LinearRegression()
model.fit(X, y)
pred = model.predict([[18, 9]])
print(f"18도, 강수량 9mm에서의 예측 판매량: {pred[0]}")
```

> **주의사항**: 입력 변수들 간에 강한 상관관계가 있으면, 모델의 신뢰도가 낮아질 수 있다.

---

## 다항 회귀 🌀

**다항 회귀(Polynomial Regression)** 는 입력 변수와 출력 변수 간의 관계가 선형이 아닐 때 사용하는 회귀 방법이다. 입력 변수의 거듭제곱 항을 추가하여, 곡선 형태의 관계도 모델링할 수 있다.

```python
# 예시: 2차 다항 회귀 모델
from sklearn.preprocessing import PolynomialFeatures

X = np.array([[10], [15], [20], [25], [30]])
y = np.array([22, 36, 60, 90, 130])

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)
pred = model.predict(poly.transform([[18]]))
print(f"18도에서의 예측 판매량(다항 회귀): {pred[0]}")
```

> **주의사항**: 다항식의 차수가 높아질수록 과적합 위험이 커진다.

---

## 회귀 모델 구현 🛠️

회귀 모델을 구현할 때는 **데이터 분할**이 필수적이다. 일반적으로 전체 데이터를 **훈련 세트**와 **테스트 세트**로 나누어 모델의 성능을 평가한다.

```python
# 예시: 데이터 분할 및 회귀 모델 학습
from sklearn.model_selection import train_test_split

X = np.random.rand(100, 2) * 10  # 2개의 입력 변수
y = 2.5 * X[:, 0] + 1.2 * X[:, 1] + np.random.rand(100) * 3

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
score = model.score(X_test, y_test)
print(f"테스트 세트 R^2 점수: {score:.2f}")
```

> **주의사항**: 데이터 분할 시 무작위성을 제어하려면 `random_state`를 지정하는 것이 좋다.

---

## 과적합 개념 🚨

**과적합(Overfitting)** 은 모델이 훈련 데이터에 지나치게 맞춰져, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 현상이다. 주로 모델이 너무 복잡하거나, 데이터가 부족할 때 발생한다.

> **주의사항**: 과적합이 발생하면 실제 환경에서 모델의 예측력이 크게 떨어진다.

---

## 과적합 방지 방법 🛡️

과적합을 방지하기 위해 다음과 같은 방법을 사용할 수 있다.

- **검증 과정(Validation)**: 훈련 데이터와 별도로 검증 데이터를 사용하여 모델의 일반화 성능을 평가한다.
- **홀드아웃(hold-out) 검증**: 데이터를 훈련/검증/테스트 세트로 나누어 사용한다.
- **교차 검증(Cross Validation)**: 데이터를 여러 조각으로 나누어 여러 번 훈련과 검증을 반복한다. 대표적으로 **K-폴드 교차 검증**이 있다.

```python
# 예시: K-폴드 교차 검증
from sklearn.model_selection import KFold

kf = KFold(n_splits=4)
for train_idx, val_idx in kf.split(X_train):
    X_tr, X_val = X_train[train_idx], X_train[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]
    model = LinearRegression()
    model.fit(X_tr, y_tr)
    val_score = model.score(X_val, y_val)
    print(f"검증 점수: {val_score:.2f}")
```

> **주의사항**: 교차 검증을 사용할 때는 데이터의 분포가 고르게 나누어졌는지 확인해야 한다.

---

## 정규화 개요 🧮

**정규화(Regularization)** 는 모델이 훈련 데이터에 과도하게 적합되는 것을 방지하기 위해, 손실 함수에 **패널티 항**을 추가하는 기법이다. 대표적으로 **L1 정규화(Lasso)**, **L2 정규화(Ridge)**, 그리고 이 둘을 결합한 **Elastic Net**이 있다.

- **L1 정규화**: 불필요한 변수의 계수를 0으로 만들어 변수 선택 효과를 제공한다.
- **L2 정규화**: 계수를 0에 가깝게 만들어 모델의 복잡도를 줄인다.
- **Elastic Net**: L1과 L2의 장점을 결합하여, 두 정규화의 비율을 조정할 수 있다.

> **주의사항**: 정규화 강도(`alpha` 등)는 하이퍼파라미터로, 값이 클수록 정규화 효과가 강해진다.

---

## Lasso, Ridge, Elastic Net 🏋️

### Lasso (L1 정규화)

- **특징**: 일부 계수를 정확히 0으로 만들어, 중요하지 않은 변수를 제거한다.
- **적합 상황**: 변수 선택이 필요한 경우, 해석이 쉬운 모델이 필요한 경우

### Ridge (L2 정규화)

- **특징**: 모든 계수를 0에 가깝게 만들어, 변수 간 상관관계가 높을 때 유용하다.
- **적합 상황**: 모든 변수를 활용하고 싶거나, 다중공선성이 존재할 때

### Elastic Net

- **특징**: L1과 L2 정규화의 장점을 모두 활용하며, 두 정규화의 비율을 조정할 수 있다.
- **적합 상황**: 변수 선택과 계수 축소를 동시에 원할 때

```python
# 예시: Lasso, Ridge, ElasticNet 회귀 모델
from sklearn.linear_model import Lasso, Ridge, ElasticNet

lasso = Lasso(alpha=0.5)
ridge = Ridge(alpha=0.5)
elastic = ElasticNet(alpha=0.5, l1_ratio=0.7)

lasso.fit(X_train, y_train)
ridge.fit(X_train, y_train)
elastic.fit(X_train, y_train)

print("Lasso 계수:", lasso.coef_)
print("Ridge 계수:", ridge.coef_)
print("ElasticNet 계수:", elastic.coef_)
```

> **주의사항**: Lasso는 계수를 0으로 만들 수 있으나, 정보 손실이 발생할 수 있다. Ridge는 계수를 0에 가깝게만 만들며, 완전히 제거하지 않는다.

---

## 정규화 회귀 구현 🧑‍💻

정규화 회귀 모델은 `sklearn.linear_model` 모듈을 통해 쉽게 구현할 수 있다. 각 모델의 주요 하이퍼파라미터는 다음과 같다.

- `alpha`: 정규화 강도 (값이 클수록 정규화 효과가 강함)
- `l1_ratio`: Elastic Net에서 L1 정규화의 비율 (0~1)

```python
# 예시: ElasticNet 회귀 모델 구현
from sklearn.linear_model import ElasticNet

enet = ElasticNet(alpha=0.1, l1_ratio=0.6)
enet.fit(X_train, y_train)
score = enet.score(X_test, y_test)
print(f"ElasticNet 테스트 점수: {score:.2f}")
```

> **주의사항**: 정규화 계수(`alpha`)와 비율(`l1_ratio`)는 교차 검증 등으로 최적값을 찾는 것이 바람직하다.

---

## 주요 평가 지표 📏

회귀 알고리즘의 성능은 **실제값**과 **예측값**의 차이를 기반으로 평가한다. 대표적인 평가지표는 다음과 같다.

| 지표 | 설명 | 특징 |
|------|------|------|
| **RSS** | 잔차 제곱합(Residual Sum of Squares) | 오차의 제곱을 모두 더한 값, 값이 작을수록 좋음 |
| **MSE** | 평균 제곱 오차(Mean Squared Error) | RSS를 데이터 개수로 나눈 값, 이상치에 민감 |
| **MAE** | 평균 절대 오차(Mean Absolute Error) | 오차의 절대값 평균, 이상치에 덜 민감 |
| **R²** | 결정계수(R-squared) | 1에 가까울수록 모델이 데이터를 잘 설명함 |

```python
# 예시: 회귀 평가 지표 계산
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_true = np.array([10, 20, 30])
y_pred = np.array([12, 18, 33])

mse = mean_squared_error(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)

print(f"MSE: {mse:.2f}, MAE: {mae:.2f}, R^2: {r2:.2f}")
```

> **주의사항**: MSE와 RSS는 이상치에 민감하며, R²는 1에 가까울수록 좋은 모델이나, 음수가 나올 수도 있다.

---

## 평가 지표 구현 📝

회귀 평가 지표는 `sklearn.metrics` 모듈을 통해 쉽게 계산할 수 있다. RSS는 직접 구현해야 하며, MSE, MAE, R²는 내장 함수를 사용한다.

```python
# 예시: RSS 직접 계산
def rss(y_true, y_pred):
    return np.sum((y_true - y_pred) ** 2)

# 예측값 생성
y_valid = np.array([15, 25, 35])
y_pred = np.array([14, 27, 33])

print("RSS:", rss(y_valid, y_pred))
print("MSE:", mean_squared_error(y_valid, y_pred))
print("MAE:", mean_absolute_error(y_valid, y_pred))
print("R^2:", r2_score(y_valid, y_pred))
```

> **주의사항**: 평가 지표는 실제값과 예측값의 배열을 동일한 순서로 전달해야 한다.

---