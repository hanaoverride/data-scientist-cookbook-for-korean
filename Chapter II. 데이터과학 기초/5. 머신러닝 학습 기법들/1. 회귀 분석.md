# 1. 회귀 분석: 데이터 속 관계를 찾아 미래를 예측하기

## 목차
- [1. 회귀 분석: 데이터 속 관계를 찾아 미래를 예측하기](#1-회귀-분석-데이터-속-관계를-찾아-미래를-예측하기)
  - [목차](#목차)
  - [1. 회귀 분석의 본질: 관계를 함수로 모델링하기](#1-회귀-분석의-본질-관계를-함수로-모델링하기)
  - [2. 모델은 어떻게 학습하는가?: 손실 함수와 경사 하강법](#2-모델은-어떻게-학습하는가-손실-함수와-경사-하강법)
    - [손실 함수 (Loss Function): '얼마나 못했는가'를 측정하는 기준](#손실-함수-loss-function-얼마나-못했는가를-측정하는-기준)
    - [경사 하강법 (Gradient Descent): 최적의 해를 찾아가는 여정](#경사-하강법-gradient-descent-최적의-해를-찾아가는-여정)
  - [3. 선형 회귀 모델의 종류](#3-선형-회귀-모델의-종류)
    - [단순/다중 선형 회귀: 기본에 충실한 모델](#단순다중-선형-회귀-기본에-충실한-모델)
    - [다항 회귀: 데이터의 곡선 패턴을 학습하기](#다항-회귀-데이터의-곡선-패턴을-학습하기)
  - [4. 과적합과의 싸움: 정규화(Regularization)](#4-과적합과의-싸움-정규화regularization)
    - [Ridge (L2 정규화): 모든 특성을 조금씩 겸손하게 만들기](#ridge-l2-정규화-모든-특성을-조금씩-겸손하게-만들기)
    - [Lasso (L1 정규화): 불필요한 특성을 과감히 제거하기](#lasso-l1-정규화-불필요한-특성을-과감히-제거하기)
  - [5. 모델 평가: 내 예측은 얼마나 정확한가?](#5-모델-평가-내-예측은-얼마나-정확한가)

---

## 1. 회귀 분석의 본질: 관계를 함수로 모델링하기

**회귀(Regression)**  는 하나 이상의 독립 변수(X, 원인)와 연속적인 종속 변수(Y, 결과) 사이의 **관계를 수학적인 함수로 모델링**하여, 새로운 X값이 주어졌을 때 Y값을 예측하는 지도학습 기법입니다.

- **예시**: `(집의 평수, 방의 개수)`로 `집값`을 예측하기
- **목표**: 데이터 점들을 가장 잘 설명하는 하나의 선(또는 곡선)을 찾는 것

가장 간단한 **단순 선형 회귀**는 이 관계를 직선 `Y ≈ β₀ + β₁X` 으로 가정합니다. 여기서 모델의 '학습'이란, 주어진 데이터(X, Y)를 가장 잘 설명하는 최적의 **기울기(β₁)와 절편(β₀)을 찾아내는 과정**입니다.

## 2. 모델은 어떻게 학습하는가?: 손실 함수와 경사 하강법

컴퓨터는 어떻게 '최적의 선'을 찾을까요? 바로 **'손실(오차)을 최소화'** 하는 방향으로 파라미터를 점진적으로 수정해나가는 방식을 사용합니다.

### 손실 함수 (Loss Function): '얼마나 못했는가'를 측정하는 기준

**손실 함수**는 모델의 예측값과 실제값의 차이, 즉 **'오차'** 를 측정하는 함수입니다. 모델은 이 손실 함수의 값을 최소화하는 것을 목표로 학습합니다.

- **평균 제곱 오차 (MSE, Mean Squared Error)** : 가장 대표적인 회귀 손실 함수. `(실제값 - 예측값)²` 의 평균으로, 오차가 클수록 더 큰 페널티를 부여하는 효과가 있습니다.

### 경사 하강법 (Gradient Descent): 최적의 해를 찾아가는 여정

**경사 하강법**은 손실 함수를 최소화시키는 모델 파라미터(β₀, β₁)를 찾는 가장 대표적인 최적화 알고리즘입니다.

1.  아무 파라미터에서나 시작합니다.
2.  현재 위치에서 손실 함수 값이 가장 가파르게 감소하는 방향(기울기, gradient)을 계산합니다.
3.  그 방향으로 아주 조금씩 파라미터를 이동시킵니다. (이 '조금'의 크기를 **학습률(learning rate)**  이라고 합니다.)
4.  손실 함수 값이 더 이상 줄어들지 않는 지점(최솟값)에 도달할 때까지 2-3번 과정을 반복합니다.

> **핵심**: 머신러닝의 '학습'은 결국 **손실 함수라는 산의 정상에서, 경사 하강법이라는 등산 기술을 이용해 가장 낮은 계곡을 찾아 내려오는 과정**과 같습니다.

## 3. 선형 회귀 모델의 종류

### 단순/다중 선형 회귀: 기본에 충실한 모델

- **단순 선형 회귀**: 하나의 독립 변수 `X`로 `Y`를 예측. `Y ≈ β₀ + β₁X`
- **다중 선형 회귀**: 여러 개의 독립 변수 `X₁, X₂, ...`로 `Y`를 예측. `Y ≈ β₀ + β₁X₁ + β₂X₂ + ...`

### 다항 회귀: 데이터의 곡선 패턴을 학습하기

데이터의 관계가 직선이 아닌 곡선 형태일 때 사용합니다. 기존 특성을 제곱하거나 세제곱한 **새로운 특성을 추가**하여 선형 모델을 비선형 데이터에 적합시키는 기법입니다.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# 비선형 데이터 생성
X = np.arange(10).reshape(-1, 1)
y = X**2 + np.random.randn(10, 1)

# 2차항 특성 추가
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# 다항 특성으로 선형 회귀 모델 학습
model = LinearRegression().fit(X_poly, y)
```
> **주의**: 차수를 너무 높이면 훈련 데이터에만 딱 맞는 복잡한 곡선이 만들어져 **과적합(Overfitting)**  위험이 커집니다.

## 4. 과적합과의 싸움: 정규화(Regularization)

**과적합**은 모델이 훈련 데이터의 사소한 노이즈까지 모두 학습하여, 새로운 데이터에 대한 예측력이 떨어지는 현상입니다. **정규화**는 모델의 **계수(β) 크기에 페널티**를 부여하여, 모델이 너무 복잡해지지 않도록(과도하게 큰 계수 값을 갖지 않도록) 제어하는 기법입니다.

### Ridge (L2 정규화): 모든 특성을 조금씩 겸손하게 만들기

- **방법**: 손실 함수에 **모든 계수들의 제곱합**에 비례하는 페널티 항을 추가합니다.
- **효과**: 모든 계수들을 0에 가깝게 만들지만, 완전히 0으로 만들지는 않습니다. 모델의 전반적인 복잡도를 낮추고, 특성들 간 상관관계가 높을 때(다중공선성) 안정적인 성능을 보입니다.

### Lasso (L1 정규화): 불필요한 특성을 과감히 제거하기

- **방법**: 손실 함수에 **모든 계수들의 절대값 합**에 비례하는 페널티 항을 추가합니다.
- **효과**: 중요하지 않은 특성의 계수를 **정확히 0으로** 만들어 버립니다. 이로 인해 자동으로 **특성 선택(Feature Selection)**  이 이루어지는 강력한 효과가 있습니다.

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge: 모든 계수가 0에 가까워짐
ridge_model = Ridge(alpha=1.0).fit(X_train, y_train)

# Lasso: 일부 계수가 0이 됨 (특성 선택)
lasso_model = Lasso(alpha=1.0).fit(X_train, y_train)
```
> **`alpha`**: 정규화의 강도를 조절하는 하이퍼파라미터. 값이 클수록 페널티가 강해져 계수들이 더 작아집니다.

## 5. 모델 평가: 내 예측은 얼마나 정확한가?

회귀 모델의 성능은 실제값과 예측값의 오차를 기반으로 평가합니다.

- **MSE (Mean Squared Error)** : 오차 제곱의 평균. 오차가 큰 값에 더 큰 페널티를 부여하고 싶을 때 유용합니다.
- **MAE (Mean Absolute Error)** : 오차 절대값의 평균. 이상치에 덜 민감하며, 오차를 직관적으로 이해하기 쉽습니다.
- **R² (결정 계수)** : 모델이 데이터의 분산을 얼마나 잘 설명하는지를 나타냅니다. 1에 가까울수록 좋습니다. (예: R²=0.8 이면, 모델이 데이터 분산의 80%를 설명한다는 의미)