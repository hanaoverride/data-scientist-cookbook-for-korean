# 2. 분류 알고리즘: 정답을 맞추는 다양한 방법들

## 목차
- [2. 분류 알고리즘: 정답을 맞추는 다양한 방법들](#2-분류-알고리즘-정답을-맞추는-다양한-방법들)
  - [목차](#목차)
  - [1. 분류의 시작: 로지스틱 회귀 (Logistic Regression)](#1-분류의-시작-로지스틱-회귀-logistic-regression)
    - [핵심 원리: 확률을 예측하는 S자 곡선](#핵심-원리-확률을-예측하는-s자-곡선)
  - [2. 경계의 미학: 서포트 벡터 머신 (SVM)](#2-경계의-미학-서포트-벡터-머신-svm)
    - [핵심 원리: 마진을 최대로, 경계를 가장 안전하게](#핵심-원리-마진을-최대로-경계를-가장-안전하게)
  - [3. 확률 기반 추론: 나이브 베이즈 (Naive Bayes)](#3-확률-기반-추론-나이브-베이즈-naive-bayes)
    - [핵심 원리: 베이즈 정리를 이용한 가능성 비교](#핵심-원리-베이즈-정리를-이용한-가능성-비교)
  - [4. 가장 단순한 지혜: K-최근접 이웃 (KNN)](#4-가장-단순한-지혜-k-최근접-이웃-knn)
    - [핵심 원리: "친구를 보면 너를 알 수 있다"](#핵심-원리-친구를-보면-너를-알-수-있다)
  - [5. 모델 선택을 위한 가이드라인](#5-모델-선택을-위한-가이드라인)
  - [6. 올바른 평가의 중요성: 혼동 행렬과 핵심 지표](#6-올바른-평가의-중요성-혼동-행렬과-핵심-지표)

---

## 1. 분류의 시작: 로지스틱 회귀 (Logistic Regression)

- **한 줄 요약**: 선형 회귀를 분류 문제에 맞게 변형한, 가장 기본적이고 해석하기 쉬운 분류 모델.
- **이럴 때 사용하세요**:
    - 이진 분류 문제의 베이스라인 모델로 시작할 때.
    - 모델의 예측 결과를 확률로 해석하고 싶을 때.
    - 각 특성이 결과에 미치는 영향을 명확하게 설명해야 할 때.

### 핵심 원리: 확률을 예측하는 S자 곡선

선형 회귀의 예측값(`β₀ + β₁X`)은 음의 무한대에서 양의 무한대까지 어떤 값이든 가질 수 있습니다. 이를 0과 1 사이의 '확률' 값으로 바꿔주기 위해 **시그모이드(Sigmoid) 함수**라는 S자 형태의 함수를 사용합니다. 이 함수를 통과한 값은 항상 0과 1 사이에 위치하며, 이를 특정 클래스에 속할 확률로 해석합니다. (일반적으로 0.5를 기준으로 분류)

## 2. 경계의 미학: 서포트 벡터 머신 (SVM)

- **한 줄 요약**: 클래스 간의 경계를 찾는 데 집중하며, 특히 그 경계의 '안전성(마진)'을 최대화하는 모델.
- **이럴 때 사용하세요**:
    - 데이터의 특성이 매우 많을 때 (고차원 데이터).
    - 클래스 간의 구분이 비교적 명확할 때.
    - **커널 트릭**을 통해 비선형적인 데이터도 분류하고 싶을 때.

### 핵심 원리: 마진을 최대로, 경계를 가장 안전하게

SVM은 단순히 두 클래스를 나누는 선(결정 경계)을 찾는 것을 넘어, 각 클래스의 가장 가까운 데이터 포인트(**서포트 벡터**)로부터 **최대한 멀리 떨어진** 경계선을 찾으려고 노력합니다. 이 경계선과 서포트 벡터 사이의 거리를 **마진(Margin)**  이라고 하며, 마진이 클수록 새로운 데이터에 대한 예측이 더 안정적이라고 봅니다.

## 3. 확률 기반 추론: 나이브 베이즈 (Naive Bayes)

- **한 줄 요약**: 베이즈 정리에 기반하여, 각 특성이 클래스에 속할 조건부 확률을 계산하여 분류하는 모델.
- **이럴 때 사용하세요**:
    - 텍스트 분류 (스팸 메일 필터, 뉴스 기사 카테고리 분류 등).
    - 데이터가 매우 적을 때도 준수한 성능을 보임.
    - 학습 속도가 매우 빠름.

### 핵심 원리: 베이즈 정리를 이용한 가능성 비교

나이브 베이즈는 "각 특성들이 서로 독립적"이라는 '순진한(naive)' 가정을 전제로 합니다. 이 가정 덕분에, `P(클래스|데이터) ∝ P(클래스) * P(데이터|클래스)` 라는 베이즈 정리를 매우 간단하게 계산할 수 있습니다. 각 클래스에 대해 이 확률 값을 계산하고, 더 높은 확률을 가진 클래스로 예측합니다.

## 4. 가장 단순한 지혜: K-최근접 이웃 (KNN)

- **한 줄 요약**: 복잡한 학습 과정 없이, 새로운 데이터 주변의 'K'개 이웃들의 투표로 클래스를 결정하는 직관적인 모델.
- **이럴 때 사용하세요**:
    - 데이터의 분포에 대한 가정이 없을 때.
    - 모델을 빠르게 프로토타이핑하고 싶을 때.
    - 결정 경계가 매우 복잡하고 비선형적일 때.

### 핵심 원리: "친구를 보면 너를 알 수 있다"

KNN은 '학습' 단계가 따로 없습니다. 모든 훈련 데이터를 그냥 저장해 둡니다 (이를 **게으른 학습(Lazy Learning)**  이라 함). 새로운 데이터가 들어오면, 저장된 모든 훈련 데이터와의 거리를 계산하여 가장 가까운 `K`개의 이웃을 찾습니다. 그리고 그 이웃들이 가장 많이 속한 클래스로 새로운 데이터의 클래스를 예측합니다.

## 5. 모델 선택을 위한 가이드라인

| 모델 | 장점 | 단점 | 추천 상황 |
| :--- | :--- | :--- | :--- |
| **로지스틱 회귀** | 빠르고, 해석이 쉬움, 확률 예측 | 선형 경계만 학습 가능 | 베이스라인, 설명이 중요할 때 |
| **SVM** | 고차원에서 강력, 마진 최대화 | 데이터가 많으면 느림, 튜닝이 까다로움 | 이미지 분류, 복잡한 경계 |
| **나이브 베이즈** | 매우 빠름, 데이터가 적어도 잘 동작 | 특성 독립성 가정이 현실과 다를 수 있음 | 텍스트 분류, 스팸 필터 |
| **KNN** | 구현이 쉽고 직관적, 비선형에 강함 | 데이터가 많으면 매우 느림, 스케일링 필수 | 프로토타이핑, 복잡한 패턴 |

## 6. 올바른 평가의 중요성: 혼동 행렬과 핵심 지표

불균형 데이터에서는 정확도(Accuracy)가 모델 성능을 왜곡할 수 있으므로, 다음 지표들을 반드시 함께 확인해야 합니다.

- **정밀도 (Precision)** : **"스팸이라고 예측한 메일 중, 진짜 스팸은 몇 개인가?"**  (FP를 줄이는 게 중요할 때)
- **재현율 (Recall)** : **"실제 암 환자 중, 모델이 암이라고 진단한 사람은 몇 명인가?"**  (FN을 줄이는 게 중요할 때)
- **F1-Score**: 정밀도와 재현율의 균형 잡힌 평균.
- **AUC (Area Under ROC Curve)** : 모델의 전반적인 분류 성능을 나타내는 지표. 1에 가까울수록 좋습니다.
