# 2. 분류 알고리즘

## 목차 📑

### 1. 분류 개념과 로지스틱 회귀
- [1.1 분류와 회귀의 차이](#분류와-회귀의-차이-) 🔍
- [1.2 로지스틱 회귀 개념](#로지스틱-회귀-개념-) 📈
- [1.3 로지스틱 회귀 구현](#로지스틱-회귀-구현-) 🛠️

### 2. Support Vector Machine(SVM)
- [2.1 SVM 개념](#svm-개념-) 🧭
- [2.2 SVM의 결정 경계와 마진](#svm의-결정-경계와-마진-) 🚧
- [2.3 SVM 구현](#svm-구현-) 🛠️

### 3. 나이브 베이즈 분류
- [3.1 나이브 베이즈 개념](#나이브-베이즈-개념-) 📬
- [3.2 나이브 베이즈 구현](#나이브-베이즈-구현-) 🛠️

### 4. KNN (K-Nearest Neighbor)
- [4.1 KNN 개념](#knn-개념-) 👥
- [4.2 KNN 구현](#knn-구현-) 🛠️

### 5. 분류 알고리즘 평가 지표
- [5.1 혼동행렬과 주요 지표](#혼동행렬과-주요-지표-) 📊
- [5.2 불균형 데이터와 평가](#불균형-데이터와-평가-) ⚖️
- [5.3 정밀도, 재현율, F1, AUC](#정밀도-재현율-f1-auc-) 🏅
- [5.4 평가 지표 구현](#평가-지표-구현-) 🛠️

---

## 분류와 회귀의 차이 🔍

### 설명

**머신러닝**은 크게 **지도학습**, **비지도학습**, **강화학습** 등으로 구분된다.  
**지도학습**은 입력과 정답(레이블)이 있는 데이터를 이용해 모델을 학습시키는 방법이다.  
지도학습 내에서 **회귀(Regression)** 는 연속적인 값을 예측하는 문제에, **분류(Classification)** 는 입력이 어떤 범주(클래스)에 속하는지 예측하는 문제에 사용된다.

예를 들어, 항공편의 **지연 여부**를 예측할 때, 기상 정보(예: 풍속, 구름 양 등)를 입력으로 하여 '지연' 또는 '정상'과 같이 두 가지 범주 중 하나로 분류하는 것이 분류 문제이다.

### 예시

```python
# 회귀: 집값 예측 (연속값)
house_size = [40, 60, 80]
price = [200, 300, 400]

# 분류: 항공편 지연 여부 예측 (범주)
weather = ['맑음', '흐림', '비']
delay = ['NO', 'YES', 'NO']
```

### 주의사항

- **회귀**는 결과가 실수(연속값)로, **분류**는 결과가 범주(이산값)로 나타난다.
- 분류 문제에 회귀 알고리즘을 그대로 적용하면, 예측값이 0~1 범위를 벗어날 수 있으므로 적합하지 않다.

---

## 로지스틱 회귀 개념 📈

### 설명

**로지스틱 회귀(Logistic Regression)** 는 분류 문제에 특화된 알고리즘이다.  
출력값이 0과 1 사이의 확률로 제한되며, **시그모이드 함수(Sigmoid Function)** 를 사용하여 예측값을 확률로 변환한다.  
일반적으로 0.5를 기준으로 하여, 0.5 이상이면 Positive(예: '지연'), 미만이면 Negative(예: '정상')로 분류한다.

시그모이드 함수는 다음과 같이 정의된다:

\[
g(x) = \frac{1}{1 + e^{-x}}
\]

값이 커질수록 1에, 작아질수록 0에 가까워지는 S자형 곡선이다.

### 예시

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 임의의 예측값
z = np.array([-2, 0, 2])
prob = sigmoid(z)
print("시그모이드 결과:", prob)
# 출력: [0.119 0.5 0.881]
```

### 주의사항

- **로지스틱 회귀**는 이진 분류에 주로 사용되며, 다중 분류에는 소프트맥스 회귀 등 확장된 방법이 필요하다.
- 출력값이 확률이므로, 분류 기준(예: 0.5)을 상황에 따라 조정할 수 있다.

---

## 로지스틱 회귀 구현 🛠️

### 설명

로지스틱 회귀 모델은 다음과 같은 순서로 구현한다:

1. 데이터 분리: 학습 데이터와 테스트 데이터로 분할
2. 모델 생성: `LogisticRegression` 객체 생성
3. 모델 학습: `fit()` 메서드로 학습
4. 예측: `predict()` 메서드로 예측

### 예시

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# 임의 데이터 생성
data = {
    '풍속': [2, 4, 3, 5, 1],
    '지연여부': [0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)
X = df[['풍속']]
y = df['지연여부']

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 모델 생성 및 학습
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측
y_pred = model.predict(X_test)
print("예측 결과:", y_pred)
```

### 주의사항

- `train_test_split`의 `random_state`를 지정하면 데이터 분할이 재현 가능하다.
- 입력 데이터가 2차원 배열 형태여야 하므로, `X`는 반드시 DataFrame 또는 2차원 배열로 전달해야 한다.

---

## SVM 개념 🧭

### 설명

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 두 개 이상의 클래스를 분리하는 최적의 결정 경계(Decision Boundary)를 찾는 분류 알고리즘이다.  
SVM은 **마진(Margin)** 을 최대화하는 경계를 찾으며, 결정 경계와 가장 가까운 데이터 포인트를 **서포트 벡터(Support Vector)** 라고 한다.

SVM은 선형 분류뿐만 아니라, 커널 트릭(Kernel Trick)을 이용해 비선형 분류도 가능하다.

### 예시

```python
import numpy as np
import matplotlib.pyplot as plt

# 임의 데이터 생성
X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]])
y = [0, 0, 0, 1, 1, 1]

plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title("SVM 데이터 예시")
plt.xlabel("특성1")
plt.ylabel("특성2")
plt.show()
```

### 주의사항

- SVM은 고차원 데이터에서도 효과적이나, 데이터가 매우 많거나 차원이 너무 높을 경우 학습 속도가 느려질 수 있다.
- 이상치(Outlier)에 민감할 수 있으므로, **Soft Margin**을 통해 유연하게 조정할 수 있다.

---

## SVM의 결정 경계와 마진 🚧

### 설명

SVM은 **결정 경계**와 **마진** 개념이 핵심이다.  
- **결정 경계**: 데이터를 분리하는 선 또는 초평면
- **마진**: 결정 경계와 서포트 벡터 사이의 거리

마진을 최대화하면, 모델의 일반화 성능이 향상된다.  
이상치 허용 여부에 따라 **Hard Margin**(엄격 분리)과 **Soft Margin**(약간의 오차 허용)으로 나뉜다.

### 예시

```python
from sklearn.svm import SVC

# 임의 데이터
X = [[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]]
y = [0, 0, 0, 1, 1, 1]

# 모델 생성 및 학습
svc = SVC(kernel='linear')
svc.fit(X, y)
print("서포트 벡터:", svc.support_vectors_)
```

### 주의사항

- **Hard Margin**은 이상치에 매우 민감하므로, 실제 데이터에서는 **Soft Margin**을 사용하는 것이 일반적이다.
- 커널 함수(`kernel` 파라미터)를 조정하여 비선형 분류도 가능하다.

---

## SVM 구현 🛠️

### 설명

SVM 분류기는 `sklearn.svm.SVC` 클래스를 사용하여 구현한다.  
데이터 분리, 모델 생성, 학습, 예측의 절차는 로지스틱 회귀와 유사하다.

### 예시

```python
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# 임의 데이터
X = [[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]]
y = [0, 0, 0, 1, 1, 1]

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# 모델 생성 및 학습
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# 예측
y_pred = svm_model.predict(X_test)
print("SVM 예측 결과:", y_pred)
```

### 주의사항

- `kernel` 파라미터를 `'linear'`, `'rbf'`, `'poly'` 등으로 변경하여 다양한 분류 문제에 적용할 수 있다.
- SVM은 분류와 회귀 모두에 사용할 수 있으나, 분류에는 `SVC`, 회귀에는 `SVR` 클래스를 사용한다.

---

## 나이브 베이즈 개념 📬

### 설명

**나이브 베이즈(Naive Bayes)** 는 **베이즈 정리**를 기반으로 한 확률적 분류 알고리즘이다.  
각 특성(Feature)이 서로 독립적이라고 가정하며, 조건부 확률을 계산하여 분류를 수행한다.  
스팸 메일 분류, 문서 분류 등에서 자주 사용된다.

베이즈 정리는 다음과 같다:

\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]

여기서, A는 분류하려는 클래스, B는 관측된 데이터(특성)이다.

### 예시

```python
# 예시: 날씨에 따른 우산 소지 여부 예측
날씨 = ['맑음', '흐림', '비']
우산 = ['NO', 'NO', 'YES']

# 베이즈 정리 적용 예시(수치 생략)
# P(우산=YES | 날씨=비) = P(날씨=비 | 우산=YES) * P(우산=YES) / P(날씨=비)
```

### 주의사항

- 특성 간 독립성 가정이 실제 데이터와 다를 경우, 예측 성능이 저하될 수 있다.
- 학습 데이터에 없는 조합이 등장하면 확률이 0이 되어 예측이 불가능하므로, **라플라스 스무딩** 등의 기법을 사용할 수 있다.

---

## 나이브 베이즈 구현 🛠️

### 설명

나이브 베이즈 분류기는 `sklearn.naive_bayes`의 `GaussianNB` 클래스를 주로 사용한다.  
특성이 연속형이고 정규분포를 따른다고 가정할 때 적합하다.

### 예시

```python
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

# 임의 데이터
X = [[1, 2], [2, 1], [3, 3], [4, 5], [5, 4]]
y = [0, 0, 1, 1, 1]

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)

# 모델 생성 및 학습
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# 예측
y_pred = nb_model.predict(X_test)
print("나이브 베이즈 예측 결과:", y_pred)
```

### 주의사항

- `GaussianNB`는 연속형 특성에 적합하며, 범주형 특성에는 `CategoricalNB` 또는 `MultinomialNB`를 사용할 수 있다.
- 데이터에 결측치가 있으면 오류가 발생하므로, 사전에 결측치 처리가 필요하다.

---

## KNN 개념 👥

### 설명

**K-최근접 이웃(K-Nearest Neighbor, KNN)** 알고리즘은 새로운 데이터가 주어졌을 때, 가장 가까운 K개의 이웃 데이터를 찾아 다수결로 분류를 결정한다.  
K 값은 하이퍼파라미터로, 일반적으로 홀수로 설정하여 동점 상황을 방지한다.

KNN은 직관적이고 구현이 간단하며, 데이터 분포에 대한 사전 가정이 필요 없다.

### 예시

```python
from collections import Counter

# 임의의 데이터 포인트와 레이블
data_points = [[1, 2], [2, 3], [3, 1], [6, 5], [7, 7], [8, 6]]
labels = [0, 0, 0, 1, 1, 1]

# 새로운 데이터
new_point = [4, 4]

# 거리 계산 및 K=3 최근접 이웃 찾기
distances = [((x[0] - new_point[0])**2 + (x[1] - new_point[1])**2)**0.5 for x in data_points]
k = 3
nearest = sorted(zip(distances, labels))[:k]
predicted_label = Counter([label for _, label in nearest]).most_common(1)[0][0]
print("예측된 클래스:", predicted_label)
```

### 주의사항

- K 값이 너무 작으면 노이즈에 민감해지고, 너무 크면 경계가 모호해질 수 있다.
- 데이터가 많거나 차원이 높을 때 계산량이 급격히 증가한다.

---

## KNN 구현 🛠️

### 설명

KNN 분류기는 `sklearn.neighbors`의 `KNeighborsClassifier` 클래스를 사용한다.  
데이터 분리, 모델 생성, 학습, 예측의 절차는 다른 분류 알고리즘과 동일하다.

### 예시

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# 임의 데이터
X = [[1, 2], [2, 3], [3, 1], [6, 5], [7, 7], [8, 6]]
y = [0, 0, 0, 1, 1, 1]

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=5)

# 모델 생성 및 학습
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# 예측
y_pred = knn.predict(X_test)
print("KNN 예측 결과:", y_pred)
```

### 주의사항

- `n_neighbors`는 K 값으로, 데이터의 특성과 분포에 따라 적절히 조정해야 한다.
- 입력 특성의 스케일이 다를 경우, 거리 계산에 영향을 주므로 **정규화** 또는 **표준화**가 필요하다.

---

## 혼동행렬과 주요 지표 📊

### 설명

**혼동행렬(Confusion Matrix)** 은 분류 모델의 예측 결과와 실제 값을 비교하여, 성능을 평가하는 표이다.  
주요 용어는 다음과 같다:

| 용어 | 설명 |
|------|------|
| **True Positive (TP)** | 실제 Positive를 Positive로 예측 |
| **True Negative (TN)** | 실제 Negative를 Negative로 예측 |
| **False Positive (FP)** | 실제 Negative를 Positive로 예측 (Type-1 Error) |
| **False Negative (FN)** | 실제 Positive를 Negative로 예측 (Type-2 Error) |

이 행렬을 바탕으로 다양한 평가 지표를 계산할 수 있다.

### 예시

```python
from sklearn.metrics import confusion_matrix

# 실제값과 예측값
y_true = [1, 0, 1, 1, 0, 0, 1]
y_pred = [1, 0, 0, 1, 0, 1, 1]

cm = confusion_matrix(y_true, y_pred)
print("혼동행렬:\n", cm)
```

### 주의사항

- Positive/Negative 클래스의 정의는 문제에 따라 다르므로, 해석 시 주의해야 한다.
- 클래스 불균형이 심할 경우, 혼동행렬만으로 모델의 성능을 정확히 판단하기 어렵다.

---

## 불균형 데이터와 평가 ⚖️

### 설명

**정확도(Accuracy)** 는 전체 예측 중 맞춘 비율로, 가장 기본적인 평가 지표이다.  
그러나 클래스가 불균형한 경우(예: 90%가 정상, 10%가 이상)에는 모든 데이터를 다수 클래스(정상)로 예측해도 높은 정확도가 나올 수 있다.

따라서, 불균형 데이터에서는 **정밀도(Precision)**, **재현율(Recall)**, **F1 Score**, **AUC** 등 다양한 지표를 함께 고려해야 한다.

### 예시

```python
# 예시: 1000개 중 950개가 정상, 50개가 이상
# 모두 정상으로 예측하면 정확도는 95%이지만, 실제로는 이상 탐지가 불가능
```

### 주의사항

- 불균형 데이터에서는 정확도만으로 모델을 평가하면 오해가 발생할 수 있다.
- 관심 있는 클래스(예: 이상, 질병 등)에 대한 **재현율**을 높이는 것이 중요할 수 있다.

---

## 정밀도, 재현율, F1, AUC 🏅

### 설명

- **정밀도(Precision)**: Positive로 예측한 것 중 실제 Positive 비율  
  \[
  Precision = \frac{TP}{TP + FP}
  \]
- **재현율(Recall)**: 실제 Positive 중에서 Positive로 예측한 비율  
  \[
  Recall = \frac{TP}{TP + FN}
  \]
- **F1 Score**: 정밀도와 재현율의 조화 평균  
  \[
  F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
  \]
- **AUC(Area Under Curve)**: ROC 커브 아래 면적으로, 1에 가까울수록 성능이 우수함

### 예시

```python
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

y_true = [1, 0, 1, 1, 0, 1, 0, 0]
y_pred = [1, 0, 0, 1, 0, 1, 1, 0]
y_prob = [0.9, 0.2, 0.4, 0.8, 0.1, 0.7, 0.6, 0.3]  # 예측 확률

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_prob)

print(f"정밀도: {precision:.2f}, 재현율: {recall:.2f}, F1: {f1:.2f}, AUC: {auc:.2f}")
```

### 주의사항

- **정밀도**는 FP(거짓 양성)를 줄이는 것이 중요한 상황에서, **재현율**은 FN(거짓 음성)을 줄이는 것이 중요한 상황에서 중시된다.
- **F1 Score**는 데이터 불균형 상황에서 모델의 종합 성능을 평가할 때 유용하다.
- **AUC**는 모델의 분류 임계값에 관계없이 전체적인 성능을 평가할 수 있다.

---

## 평가 지표 구현 🛠️

### 설명

`sklearn.metrics` 모듈을 활용하여 다양한 분류 평가 지표를 손쉽게 계산할 수 있다.

### 예시

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 임의의 검증 데이터
y_valid = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 0, 1, 0, 1, 1]

# 평가 지표 계산
accuracy = accuracy_score(y_valid, y_pred)
precision = precision_score(y_valid, y_pred)
recall = recall_score(y_valid, y_pred)
f1 = f1_score(y_valid, y_pred)

print("정확도:", accuracy)
print("정밀도:", precision)
print("재현율:", recall)
print("F1 Score:", f1)
```

### 주의사항

- 평가 지표의 해석은 문제의 특성(예: 질병 진단, 스팸 필터 등)에 따라 달라진다.
- `roc_auc_score`는 예측 확률값이 필요하므로, `predict_proba` 또는 `decision_function`의 결과를 입력해야 한다.
