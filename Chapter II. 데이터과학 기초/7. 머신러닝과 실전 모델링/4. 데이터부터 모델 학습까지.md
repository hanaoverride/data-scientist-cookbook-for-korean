# 4. 데이터부터 모델 학습까지

## 목차 📑

### 1. 선형과 비선형 데이터
- [1.1 선형 데이터](#선형-데이터-) 📏
- [1.2 비선형 데이터](#비선형-데이터-) 🔀
- [1.3 비선형 데이터의 선형화](#비선형-데이터의-선형화-) 🔄

### 2. 퍼셉트론
- [2.1 퍼셉트론의 개념](#퍼셉트론의-개념-) 🧠
- [2.2 퍼셉트론 논리 게이트 구현](#퍼셉트론-논리-게이트-구현-) 🔌

### 3. 다층 퍼셉트론(MLP) 모델
- [3.1 MLP 구조와 특징](#mlp-구조와-특징-) 🏗️
- [3.2 대표적 비선형 모델](#대표적-비선형-모델-) 🌳
- [3.3 활성화 함수](#활성화-함수-) ⚡
- [3.4 경사하강법과 기울기 소실](#경사하강법과-기울기-소실-) 📉
- [3.5 MLP 모델 구성 예시](#mlp-모델-구성-예시-) 🖼️

### 4. 원-핫 인코딩
- [4.1 분류 문제의 에러 정의](#분류-문제의-에러-정의-) ❓
- [4.2 원-핫 인코딩 개념](#원-핫-인코딩-개념-) 🟩
- [4.3 원-핫 인코딩 예시](#원-핫-인코딩-예시-) 🔢

### 5. 소프트맥스
- [5.1 소프트맥스의 필요성](#소프트맥스의-필요성-) 🎯
- [5.2 소프트맥스 함수](#소프트맥스-함수-) 🧮
- [5.3 소프트맥스 적용 후](#소프트맥스-적용-후-) 📊

### 6. 옵티마이저
- [6.1 옵티마이저 개요](#옵티마이저-개요-) 🛠️
- [6.2 주요 옵티마이저 종류](#주요-옵티마이저-종류-) ⚙️
- [6.3 인공신경망 학습 과정](#인공신경망-학습-과정-) 🔁

---

## 선형 데이터 📏

### 설명

**선형 데이터**란 독립변수와 종속변수 사이에 **직선적 관계**가 존재하는 데이터를 의미한다. 이러한 데이터는 독립변수들의 **선형 결합**으로 종속변수를 표현할 수 있으며, 수식으로는 `Y = β0 + β1X1 + β2X2 + ... + βnXn` 형태로 나타난다. 일반적으로 한 개의 독립변수와 종속변수를 그래프로 시각화하면 **직선**으로 표현된다. 비선형성이 미미할 경우에도 선형성을 가정할 수 있으며, 이 경우 예측 모델의 복잡도가 크게 줄어든다.

### 예시

```python
# 선형 데이터 예시: 집 크기와 가격의 관계
import numpy as np
import matplotlib.pyplot as plt

size = np.array([30, 40, 50, 60, 70])
price = 2.5 * size + 10  # 선형 관계

plt.scatter(size, price)
plt.plot(size, price, color='red')
plt.xlabel('집 크기(평)')
plt.ylabel('가격(백만원)')
plt.title('집 크기와 가격의 선형 관계')
plt.show()
```

### 주의사항

- **선형성 가정**이 항상 옳은 것은 아니며, 데이터에 따라 적합하지 않을 수 있다.
- **이상치**가 존재하면 선형 모델의 성능이 저하될 수 있다.

---

## 비선형 데이터 🔀

### 설명

**비선형 데이터**는 독립변수와 종속변수 사이의 관계가 **직선**이 아닌 **곡선**이나 복잡한 형태로 나타나는 데이터를 의미한다. 실제로 자연현상이나 공학적 데이터의 대부분은 비선형성을 띠는 경우가 많으며, 특히 **고차원 데이터**에서 이러한 경향이 두드러진다. 비선형 데이터는 일부 구간에서만 선형 근사화가 가능하다.

### 예시

```python
# 비선형 데이터 예시: 온도와 반응 속도의 관계
import numpy as np
import matplotlib.pyplot as plt

temperature = np.linspace(10, 100, 50)
reaction_rate = np.exp(0.05 * temperature)  # 지수 함수 관계

plt.plot(temperature, reaction_rate)
plt.xlabel('온도(℃)')
plt.ylabel('반응 속도')
plt.title('온도와 반응 속도의 비선형 관계')
plt.show()
```

### 주의사항

- **비선형 데이터**는 단순 선형 모델로는 적절히 설명할 수 없으므로, 복잡한 모델이나 **비선형 변환**이 필요하다.
- 일부 구간에서만 선형 근사가 가능하므로, 전체 데이터에 선형 모델을 적용하면 오차가 커질 수 있다.

---

## 비선형 데이터의 선형화 🔄

### 설명

비선형 데이터는 **일부 구간**에서만 선형 근사가 가능하다. 예를 들어, 차량의 타이어 모델에서 힘과 슬립각 사이의 관계는 전체적으로 비선형이지만, **매우 작은 슬립각** 구간에서는 선형적으로 근사할 수 있다. 이러한 방법을 통해 복잡한 비선형 문제를 단순화할 수 있다.

### 예시

```python
# 비선형 데이터의 일부 구간 선형 근사
import numpy as np
import matplotlib.pyplot as plt

angle = np.linspace(-10, 10, 100)
force = np.tanh(0.2 * angle) * 100  # 비선형 함수

plt.plot(angle, force, label='비선형')
plt.plot(angle[40:60], force[40:60], color='red', label='선형 근사 구간')
plt.xlabel('슬립각(도)')
plt.ylabel('타이어 힘')
plt.legend()
plt.title('비선형 데이터의 선형 근사')
plt.show()
```

### 주의사항

- **선형화**는 전체 데이터가 아닌, **특정 구간**에서만 적용해야 한다.
- 선형 근사 구간을 잘못 선택하면 모델의 신뢰도가 떨어질 수 있다.

---

## 퍼셉트론의 개념 🧠

### 설명

**퍼셉트론**은 인공신경망의 가장 기본적인 단위로, **입력값**과 **가중치**의 곱을 모두 더한 뒤 **편향**을 더하고, **활성화 함수**를 통해 최종 출력을 결정한다. 퍼셉트론은 단순한 논리 회로(AND, OR 등)를 구현할 수 있으며, 입력과 출력이 모두 이진값인 경우가 많다.

### 예시

```python
# 퍼셉트론 기본 구조 예시
def perceptron(inputs, weights, bias):
    total = sum(i * w for i, w in zip(inputs, weights)) + bias
    return 1 if total > 0 else 0

# 입력: [1, 0], 가중치: [0.7, 0.5], 편향: -0.6
output = perceptron([1, 0], [0.7, 0.5], -0.6)
print(f"퍼셉트론 출력: {output}")  # 출력: 1
```

### 주의사항

- 퍼셉트론은 **선형 분리**가 가능한 문제만 해결할 수 있다.
- XOR과 같은 **비선형 문제**는 단일 퍼셉트론으로 해결할 수 없다.

---

## 퍼셉트론 논리 게이트 구현 🔌

### 설명

퍼셉트론을 활용하면 **AND**, **OR**과 같은 논리 게이트를 구현할 수 있다. 입력값과 가중치, 편향을 적절히 설정하면 원하는 논리 연산을 수행할 수 있다. 하지만 **XOR** 게이트는 단일 퍼셉트론으로 구현할 수 없으며, 다층 구조가 필요하다.

### 예시

```python
# AND 게이트 예시
def and_gate(x1, x2):
    return 1 if (x1 + x2) > 1.5 else 0

print(and_gate(1, 1))  # 출력: 1
print(and_gate(0, 1))  # 출력: 0

# OR 게이트 예시
def or_gate(x1, x2):
    return 1 if (x1 + x2) > 0.5 else 0

print(or_gate(0, 1))  # 출력: 1
print(or_gate(0, 0))  # 출력: 0
```

### 주의사항

- **XOR** 게이트는 단일 퍼셉트론으로 구현할 수 없으며, 반드시 **다층 퍼셉트론**이 필요하다.
- 논리 게이트 구현 시 **가중치**와 **편향** 값에 따라 결과가 달라질 수 있으므로 주의해야 한다.

---

## MLP 구조와 특징 🏗️

### 설명

**다층 퍼셉트론(MLP, Multi-Layer Perceptron)** 은 여러 개의 퍼셉트론을 **층**으로 쌓아 구성한 인공신경망이다. 입력층, 하나 이상의 **은닉층**, 그리고 출력층으로 이루어진다. 각 층은 **가중치**, **편향**, **활성화 함수**로 구성되며, 활성화 함수를 제외하면 선형 연산만 수행한다. 은닉층이 추가됨으로써 **비선형 문제**도 해결할 수 있다.

### 예시

```python
# MLP 구조 예시 (2-3-1 구조)
import numpy as np

def relu(x):
    return np.maximum(0, x)

# 입력층(2), 은닉층(3), 출력층(1)
X = np.array([0.5, 0.8])
W1 = np.array([[0.2, -0.3, 0.5], [0.7, 0.1, -0.2]])
b1 = np.array([0.1, 0.2, 0.3])
hidden = relu(np.dot(X, W1) + b1)

W2 = np.array([[0.4], [-0.6], [0.2]])
b2 = np.array([0.05])
output = relu(np.dot(hidden, W2) + b2)
print(f"MLP 출력: {output}")
```

### 주의사항

- **은닉층**이 없는 경우, MLP는 단순 퍼셉트론과 동일하다.
- 은닉층의 개수와 노드 수에 따라 모델의 **표현력**이 달라진다.

---

## 대표적 비선형 모델 🌳

### 설명

비선형 문제를 해결하기 위해 다양한 **비선형 모델**이 사용된다. 대표적으로 **랜덤 포레스트**, **조건부 추론 트리**, **XGBoost**, **LightGBM**, **CatBoost**와 같은 결정 트리 기반 모델이 있으며, **다층 퍼셉트론(MLP)**, **합성곱 신경망(CNN)**, **순환 신경망(RNN)**, **스파이크 신경망(SNN)** 등 다양한 **인공신경망(ANN)** 구조가 존재한다.

### 예시

```python
# 비선형 모델 예시: 랜덤 포레스트 분류
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=100, n_features=4, n_classes=2)
model = RandomForestClassifier(n_estimators=10)
model.fit(X, y)
print(f"예측 결과: {model.predict([X[0]])}")
```

### 주의사항

- 각 모델은 **데이터 특성**과 **문제 유형**에 따라 적합성이 다르므로, 적절한 모델 선택이 중요하다.
- **신경망 모델**은 대량의 데이터와 연산 자원이 필요할 수 있다.

---

## 활성화 함수 ⚡

### 설명

**활성화 함수**는 인공신경망에서 **비선형성**을 부여하는 역할을 한다. 대표적으로 **시그모이드(Sigmoid)**, **하이퍼볼릭 탄젠트(Tanh)**, **ReLU(Rectified Linear Unit)** 등이 있다. 시그모이드와 Tanh는 출력값이 각각 (0, 1), (-1, 1) 범위로 제한되며, **기울기 소실(Gradient Vanishing)** 문제가 발생할 수 있다. ReLU는 0 이상의 입력에 대해 선형적으로 동작하여 기울기 소실 문제를 완화한다.

### 예시

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)
sigmoid = 1 / (1 + np.exp(-x))
tanh = np.tanh(x)
relu = np.maximum(0, x)

plt.plot(x, sigmoid, label='Sigmoid')
plt.plot(x, tanh, label='Tanh')
plt.plot(x, relu, label='ReLU')
plt.legend()
plt.title('활성화 함수 비교')
plt.show()
```

### 주의사항

- **시그모이드**와 **Tanh**는 깊은 신경망에서 **기울기 소실** 문제가 발생할 수 있다.
- **ReLU**는 입력이 0 이하일 때 출력이 0이 되어, 일부 뉴런이 비활성화될 수 있다.

---

## 경사하강법과 기울기 소실 📉

### 설명

**경사하강법(Gradient Descent)** 은 인공신경망의 **최적화**를 위해 사용되는 대표적인 방법이다. 손실 함수의 값을 최소화하기 위해, 각 파라미터에 대해 **기울기(gradient)** 를 계산하여 역전파(backpropagation)로 업데이트한다. 하지만, 활성화 함수의 기울기가 매우 작아지면 **기울기 소실(Gradient Vanishing)** 현상이 발생하여, 깊은 신경망의 학습이 어려워질 수 있다.

### 예시

```python
# 경사하강법 예시: 단순 2차 함수 최적화
def loss(x):
    return (x - 3) ** 2

x = 0.0
learning_rate = 0.1
for i in range(20):
    grad = 2 * (x - 3)
    x -= learning_rate * grad
print(f"최적화된 x 값: {x:.2f}")
```

### 주의사항

- **기울기 소실**은 주로 시그모이드, Tanh 함수에서 발생하며, 깊은 신경망에서 심각해진다.
- **ReLU**와 같은 함수는 이 문제를 완화할 수 있다.

---

## MLP 모델 구성 예시 🖼️

### 설명

**MLP 모델**은 입력층, 하나 이상의 은닉층, 출력층으로 구성된다. 각 층은 여러 개의 노드로 이루어지며, 입력 데이터가 순차적으로 전달된다. 출력층의 활성화 함수와 노드 수는 문제 유형(분류/회귀)에 따라 달라진다.

### 예시

```python
# MLP 모델 구성 예시 (Keras 사용)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(16, activation='relu', input_shape=(8,)),  # 입력층(8개 특성), 은닉층(16개 노드)
    Dense(8, activation='relu'),                     # 은닉층(8개 노드)
    Dense(3, activation='softmax')                   # 출력층(3개 클래스 분류)
])
model.summary()
```

### 주의사항

- **출력층**의 활성화 함수와 노드 수는 문제에 맞게 설정해야 한다.
- 입력 데이터의 **차원**과 모델의 입력층 노드 수가 일치해야 한다.

---

## 분류 문제의 에러 정의 ❓

### 설명

**회귀 문제**에서는 정답과 예측값의 차이로 에러를 계산할 수 있지만, **분류 문제**에서는 단순한 뺄셈으로 에러를 정의할 수 없다. 예를 들어, 정답이 1번인 문제에서 2번과 5번을 예측한 경우, 어느 쪽이 더 틀렸다고 말할 수 없다. 따라서 분류 문제에서는 **원-핫 인코딩**을 활용하여 정답을 표현하고, 손실 함수로 **크로스 엔트로피** 등을 사용한다.

### 예시

```python
# 분류 문제에서 에러 계산 예시
import numpy as np

true_label = [0, 1, 0]  # 정답: 클래스 1
predicted = [0.2, 0.7, 0.1]  # 예측 확률

# 크로스 엔트로피 손실 계산
loss = -np.sum(np.array(true_label) * np.log(np.array(predicted) + 1e-8))
print(f"크로스 엔트로피 손실: {loss:.4f}")
```

### 주의사항

- **분류 문제**에서는 정답과 예측값의 단순 차이로 손실을 계산하지 않는다.
- **원-핫 인코딩**과 **확률적 출력**이 필수적이다.

---

## 원-핫 인코딩 개념 🟩

### 설명

**원-핫 인코딩(One-Hot Encoding)** 은 분류 문제에서 각 클래스를 **이진 벡터**로 표현하는 방법이다. 예측하고자 하는 클래스의 개수만큼 벡터를 만들고, 해당 클래스 위치에만 1, 나머지는 0을 할당한다. 이는 범주형 데이터를 신경망에서 처리할 수 있도록 변환하는 대표적인 방법이다.

### 예시

```python
# 원-핫 인코딩 예시: 과일 분류 (사과, 바나나, 오렌지)
fruits = ['apple', 'banana', 'orange']
labels = ['banana', 'apple', 'orange', 'banana']

def one_hot_encode(label, classes):
    return [1 if label == c else 0 for c in classes]

encoded = [one_hot_encode(l, fruits) for l in labels]
print(encoded)
# 출력: [[0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0]]
```

### 주의사항

- 클래스 개수가 많아질수록 **벡터 차원**이 커진다.
- **순서 정보**가 없는 범주형 데이터에만 적용해야 한다.

---

## 원-핫 인코딩 예시 🔢

### 설명

예를 들어, 손글씨 숫자 분류(MNIST) 문제에서는 0~9까지 총 10개의 클래스가 존재한다. 각 숫자 이미지는 정답 레이블을 **원-핫 인코딩**으로 변환하여 신경망의 출력층과 일대일 대응시킨다.

### 예시

```python
# MNIST 숫자 7의 원-핫 인코딩 예시
digit = 7
num_classes = 10
one_hot = [1 if i == digit else 0 for i in range(num_classes)]
print(one_hot)
# 출력: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
```

### 주의사항

- **출력층 노드 수**는 원-핫 인코딩 벡터의 크기와 반드시 일치해야 한다.
- **정답 데이터**와 **예측 결과**의 비교는 원-핫 인코딩 형태로 이루어진다.

---

## 소프트맥스의 필요성 🎯

### 설명

신경망의 출력층에서 **원-핫 인코딩**과 일치하는 값을 얻기 위해서는, 각 클래스별로 **확률**을 출력해야 한다. 단순히 출력값과 원-핫 인코딩 벡터의 차이를 계산하는 것은 의미가 없으므로, 출력값을 **확률 분포**로 변환하는 과정이 필요하다. 이를 위해 **소프트맥스(Softmax)** 함수가 사용된다.

### 예시

```python
# 소프트맥스 함수 적용 예시
import numpy as np

logits = np.array([1.2, 2.3, 0.8])
exp_logits = np.exp(logits)
softmax = exp_logits / np.sum(exp_logits)
print(f"소프트맥스 결과: {softmax}")
# 출력: [0.2447, 0.6642, 0.0911]
```

### 주의사항

- 소프트맥스 적용 전의 출력값을 **로짓(logit)**이라고 부른다.
- 소프트맥스는 **출력층의 활성화 함수**로 사용된다.

---

## 소프트맥스 함수 🧮

### 설명

**소프트맥스(Softmax)** 함수는 출력값을 0~1 사이의 값으로 변환하며, 전체 합이 1이 되도록 한다. 이는 각 클래스에 속할 **확률**로 해석할 수 있다. 소프트맥스는 미분이 가능하여, 신경망의 학습에 적합하다.

### 예시

```python
# 소프트맥스 함수 구현
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # 안정성 향상
    return exp_x / np.sum(exp_x)

scores = [2.0, 1.0, 0.1]
probs = softmax(scores)
print(f"클래스별 확률: {probs}")
# 출력: [0.659, 0.242, 0.099]
```

### 주의사항

- **Hardmax**는 가장 큰 값만 1, 나머지는 0으로 만드는 방식이나, 미분이 불가능하다.
- 소프트맥스는 **확률적 해석**이 가능하므로 분류 문제에 적합하다.

---

## 소프트맥스 적용 후 📊

### 설명

소프트맥스 적용 후, 출력값은 각 클래스에 속할 **확률**을 의미한다. 예측이 완벽할 경우, 정답 클래스의 확률이 1에 가까워지고, 손실값은 0에 수렴한다. 소프트맥스는 출력층의 **활성화 함수**로 자주 사용된다.

### 예시

```python
# 소프트맥스 적용 후 예시
logits = np.array([0.3, 2.1, 1.2])
probs = softmax(logits)
print(f"소프트맥스 확률: {probs}")
# 출력: [0.120, 0.659, 0.221]
```

### 주의사항

- 소프트맥스 출력값은 항상 0~1 사이이며, 합이 1이 된다.
- **정답 클래스**의 확률이 높을수록 예측이 정확하다고 판단한다.

---

## 옵티마이저 개요 🛠️

### 설명

**옵티마이저(Optimizer)** 는 인공신경망의 **손실 함수**를 최소화하기 위해 파라미터(가중치, 편향 등)를 반복적으로 업데이트하는 알고리즘이다. 학습 데이터 전체를 여러 번 반복(에폭, Epoch)하여 최적의 파라미터를 찾는다. 옵티마이저의 종류와 설정에 따라 학습 성능이 크게 달라질 수 있다.

### 예시

```python
# 옵티마이저 설정 예시 (Keras)
from tensorflow.keras.optimizers import Adam

optimizer = Adam(learning_rate=0.001)
# 모델 컴파일 시 optimizer 인자로 사용
```

### 주의사항

- **옵티마이저** 선택과 **학습률** 설정은 모델의 성능에 큰 영향을 미친다.
- 잘못된 설정은 학습 실패나 과적합을 초래할 수 있다.

---

## 주요 옵티마이저 종류 ⚙️

### 설명

대표적인 옵티마이저로는 **SGD(Stochastic Gradient Descent)**, **Momentum**, **AdaGrad**, **RMSProp**, **Adam** 등이 있다.

- **SGD**: 전체 데이터가 아닌 일부 미니배치로 기울기를 계산하여 빠르게 학습하지만, 불안정할 수 있다.
- **Momentum**: 이전 업데이트의 방향을 반영하여, 지역 최소값을 극복하고 학습 안정성을 높인다.
- **AdaGrad/RMSProp**: 각 파라미터별로 학습률을 다르게 적용하여, 평평한 구간에서는 학습을 가속하고, 급격한 구간에서는 감속한다.
- **Adam**: Momentum과 RMSProp의 장점을 결합한 방식으로, 대부분의 최신 신경망에서 기본값으로 사용된다.

### 예시

```python
# 다양한 옵티마이저 사용 예시 (PyTorch)
import torch.optim as optim

model_params = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer_sgd = optim.SGD(model_params, lr=0.01)
optimizer_adam = optim.Adam(model_params, lr=0.001)
```

### 주의사항

- **Adam**은 대부분의 경우 기본값으로 사용해도 무방하지만, 데이터와 모델에 따라 다른 옵티마이저가 더 적합할 수 있다.
- **학습률**은 반드시 실험적으로 조정해야 한다.

---

## 인공신경망 학습 과정 🔁

### 설명

인공신경망의 학습은 다음과 같은 과정을 반복한다.

1. **배치 데이터 샘플링**: 전체 데이터에서 일부를 선택
2. **순전파(Forward Propagation)**: 입력 데이터를 신경망에 통과시켜 예측값 계산
3. **손실 함수 계산**: 예측값과 정답의 차이(손실) 계산
4. **역전파(Backward Propagation)**: 손실을 기준으로 각 파라미터의 기울기 계산
5. **옵티마이저로 파라미터 업데이트**: 기울기를 이용해 파라미터 조정
6. **반복**: 모든 데이터를 사용할 때까지(1 에폭), 성능이 개선되지 않을 때까지(N 에폭)

### 예시

```python
# 인공신경망 학습 과정 의사코드
for epoch in range(num_epochs):
    for batch in data_loader:
        outputs = model(batch['input'])
        loss = loss_fn(outputs, batch['target'])
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 주의사항

- **에폭**이 너무 적으면 학습이 부족하고, 너무 많으면 과적합이 발생할 수 있다.
- **검증 데이터**를 활용하여 모델의 일반화 성능을 반드시 확인해야 한다.