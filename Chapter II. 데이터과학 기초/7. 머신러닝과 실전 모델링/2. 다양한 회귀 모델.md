# 2. 선형성을 넘어서: 고급 회귀 모델들

## 목차
- [1. 선형 회귀의 한계](#1-선형-회귀의-한계)
- [2. 결정 트리 회귀 (Decision Tree Regressor)](#2-결정-트리-회귀-decision-tree-regressor)
  - [핵심 원리: 데이터를 구간으로 나누어 예측하기](#핵심-원리-데이터를-구간으로-나누어-예측하기)
- [3. 앙상블 회귀 모델: 집단 지성의 힘](#3-앙상블-회귀-모델-집단-지성의-힘)
  - [랜덤 포레스트 회귀 (Random Forest Regressor)](#랜덤-포레스트-회귀-random-forest-regressor)
  - [그래디언트 부스팅 회귀 (Gradient Boosting Regressor)](#그래디언트-부스팅-회귀-gradient-boosting-regressor)
- [4. 서포트 벡터 회귀 (Support Vector Regression, SVR)](#4-서포트-벡터-회귀-support-vector-regression-svr)
  - [핵심 원리: 오차를 허용하는 똑똑한 도로](#핵심-원리-오차를-허용하는-똑똑한-도로)
- [5. 어떤 회귀 모델을 언제 사용해야 할까?](#5-어떤-회귀-모델을-언제-사용해야-할까)

---

## 1. 선형 회귀의 한계

선형 회귀는 데이터의 관계가 **선형(linear)**  이라고 가정합니다. 하지만 아래와 같은 데이터는 직선 하나로 설명하기 어렵습니다. 이런 비선형 관계를 모델링하기 위해 우리는 더 유연한 모델이 필요합니다.

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.linspace(-5, 5, 100)
y = X**2 + np.random.randn(100) * 2

plt.scatter(X, y, s=10)
plt.title("선형 회귀로는 설명하기 어려운 비선형 데이터")
plt.show()
```

## 2. 결정 트리 회귀 (Decision Tree Regressor)

- **한 줄 요약**: 데이터를 여러 개의 단순한 '구간'으로 나눈 뒤, 각 구간의 평균값으로 예측하는 모델.
- **이럴 때 사용하세요**:
    - 데이터의 패턴이 복잡하고 비선형적일 때.
    - 모델의 예측 과정을 직관적으로 해석하고 싶을 때.

### 핵심 원리: 데이터를 구간으로 나누어 예측하기

결정 트리 회귀는 마치 '스무고개'처럼, 데이터의 특성을 기준으로 질문을 던져 데이터를 반복적으로 분할합니다.
1.  "x가 5보다 작은가?" -> Yes / No
2.  "Yes 그룹에서, y가 10보다 작은가?" -> Yes / No
3.  ...

이 과정을 반복하여, 더 이상 나눌 수 없는 최종 구간(Leaf Node)에 도달하면, 그 구간에 속한 데이터들의 **평균값**을 해당 구간의 예측값으로 사용합니다. 결과적으로 모델은 계단 형태의 예측선을 만들어냅니다.

> **단점**: 훈련 데이터에 과적합(Overfitting)되기 매우 쉽습니다. 트리의 깊이가 깊어지면 데이터 하나하나에 맞는 매우 복잡한 모델이 만들어져, 새로운 데이터에 대한 예측력이 떨어집니다.

## 3. 앙상블 회귀 모델: 집단 지성의 힘

결정 트리의 가장 큰 단점인 '과적합' 문제를 해결하기 위해, 여러 개의 결정 트리를 함께 사용하는 **앙상블(Ensemble)**  기법이 등장했습니다.

### 랜덤 포레스트 회귀 (Random Forest Regressor)

- **핵심 원리**: 수백 개의 **서로 다른 모양의 결정 트리**를 만들고, 그들의 예측값을 **평균**냅니다.
- **어떻게 다른 모양으로?**:
    1.  **데이터 샘플링(배깅)** : 각 트리는 원본 데이터에서 무작위로 복원 추출한 데이터로 학습합니다.
    2.  **특성 샘플링**: 각 트리의 각 분기점에서는, 전체 특성 중 일부만 무작위로 선택하여 최적의 분할을 찾습니다.
- **효과**: 개별 트리는 과적합될 수 있지만, 수많은 트리들의 예측을 평균내면 오류가 서로 상쇄되면서 매우 안정적이고 정확한 예측을 만들어냅니다.

### 그래디언트 부스팅 회귀 (Gradient Boosting Regressor)

- **핵심 원리**: 간단한 결정 트리를 순차적으로 만들어가며, **이전 트리의 실수(오차)를 다음 트리가 보완**하도록 학습합니다.
- **학습 과정**:
    1.  첫 번째 트리가 대략적인 예측을 합니다.
    2.  두 번째 트리는 첫 번째 트리가 남긴 '오차'를 예측하도록 학습합니다.
    3.  세 번째 트리는 앞선 두 트리가 합쳐서 남긴 '오차'를 예측하도록 학습합니다.
    4.  이 과정을 반복하여, 점진적으로 전체 모델의 오차를 줄여나갑니다.
- **효과**: 매우 정확한 예측 모델을 만들 수 있습니다. XGBoost, LightGBM 등은 모두 이 그래디언트 부스팅에 기반한 알고리즘입니다.

## 4. 서포트 벡터 회귀 (Support Vector Regression, SVR)

- **한 줄 요약**: 예측선 주변으로 일정 폭의 '도로(margin)'를 설정하고, 그 도로 안에 최대한 많은 데이터가 들어가도록 하는 모델.
- **이럴 때 사용하세요**:
    - 데이터에 이상치(outlier)가 많을 때.
    - 데이터의 특성이 매우 많을 때 (고차원).

### 핵심 원리: 오차를 허용하는 똑똑한 도로

일반적인 회귀 모델은 모든 데이터와의 오차를 최소화하려 하지만, SVR은 다릅니다.
- **마진(Margin, 도로 폭)** : `epsilon(ε)`이라는 하이퍼파라미터로 도로의 폭을 정합니다.
- **오차 허용**: 도로 폭(`ε`) 안에 들어오는 데이터들의 오차는 **무시**합니다.
- **페널티**: 도로를 벗어나는 데이터에 대해서만 페널티를 부과하여, 그 오차를 최소화하도록 예측선을 조정합니다.

> **효과**: 마진 이내의 작은 오차나 노이즈에 민감하게 반응하지 않기 때문에, 이상치가 많은 데이터에서도 비교적 안정적인 예측선을 찾을 수 있습니다.

## 5. 어떤 회귀 모델을 언제 사용해야 할까?

| 모델 | 장점 | 단점 | 추천 상황 |
| :--- | :--- | :--- | :--- |
| **선형 회귀** | 빠르고, 해석이 매우 쉬움 | 비선형 관계 학습 불가 | 베이스라인, 설명이 중요할 때 |
| **결정 트리** | 해석 용이, 비선형 학습 가능 | 과적합 위험이 매우 높음 | 단독 사용은 비추천, 앙상블의 기반 |
| **랜덤 포레스트** | 높은 정확도, 과적합에 강함 | 모델 내부 해석이 어려움 | 대부분의 문제에서 안정적인 고성능 |
| **그래디언트 부스팅** | 매우 높은 정확도 | 튜닝이 까다롭고 과적합 위험 | 최고의 성능을 원할 때 (대회 등) |
| **SVR** | 이상치에 강건, 고차원에서 효과적 | 데이터가 많으면 매우 느림 | 노이즈가 많거나 특성이 많은 데이터 |
