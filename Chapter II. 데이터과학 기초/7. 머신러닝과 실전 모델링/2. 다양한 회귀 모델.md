# 2. 다양한 회귀 모델

## 목차 📑

### 1. 선형 회귀
- [1.1 선형 회귀란](#선형-회귀란-) 📐
- [1.2 선형 회귀의 가정](#선형-회귀의-가정-) 📝
- [1.3 선형 회귀의 원리](#선형-회귀의-원리-) 🔍
- [1.4 선형 회귀 모델 학습](#선형-회귀-모델-학습-) 🏋️

### 2. 다중 선형 회귀
- [2.1 다중 선형 회귀 개념](#다중-선형-회귀-개념-) 📊
- [2.2 다중 출력 회귀](#다중-출력-회귀-) 🧮

### 3. 성능 평가 지표
- [3.1 주요 회귀 평가 지표](#주요-회귀-평가-지표-) 📏

### 4. 로지스틱 회귀 모델
- [4.1 로지스틱 회귀 개요](#로지스틱-회귀-개요-) 🔑
- [4.2 시그모이드 함수](#시그모이드-함수-) 📈
- [4.3 로지스틱 회귀 학습 과정](#로지스틱-회귀-학습-과정-) 🔄
- [4.4 다중 분류 확장](#다중-분류-확장-) 🌳

---

## 선형 회귀란 📐

### 설명

**선형 회귀**는 **독립변수**와 **종속변수** 사이의 **선형 관계**를 모델링하는 대표적인 통계 기법이다. 독립변수는 보통 **X**로, 종속변수는 **y**로 표기한다. 선형 회귀의 목적은 입력값(X)이 주어졌을 때 결과값(y)을 가장 잘 예측하는 **직선**을 찾는 것이다. 이때 직선의 방정식은 `y = β0 + β1x` 형태로 표현된다.

### 예시

```python
# 임의의 데이터 생성 및 단순 선형 회귀 예시
import numpy as np
from sklearn.linear_model import LinearRegression

# 데이터 생성
input_data = np.array([2, 4, 6, 8, 10]).reshape(-1, 1)
output_data = np.array([5, 9, 13, 17, 21])

# 모델 학습
model = LinearRegression()
model.fit(input_data, output_data)

# 예측
predicted = model.predict(np.array([[12]]))
print(f"12 입력 시 예측값: {predicted[0]:.2f}")
```
**출력 예시**:
```
12 입력 시 예측값: 25.00
```

### 주의사항

- **선형 회귀**는 입력과 출력 사이에 **선형성**이 존재해야 효과적으로 동작한다.
- 비선형 관계에서는 예측력이 급격히 저하될 수 있다.

---

## 선형 회귀의 가정 📝

### 설명

선형 회귀가 올바르게 작동하려면 다음과 같은 **가정**들이 충족되어야 한다.

1. **선형성**: 독립변수와 종속변수 사이의 관계가 선형이어야 한다.
2. **정규성**: 잔차(실제값과 예측값의 차이)가 정규분포를 따라야 한다.
3. **등분산성**: 잔차의 분산이 입력값에 따라 일정해야 한다.
4. **독립성**: 각 독립변수는 서로 독립이어야 하며, 다중공선성이 없어야 한다.

### 예시

```python
# 등분산성, 정규성, 독립성 확인 예시 (간단한 시각화)
import matplotlib.pyplot as plt
import numpy as np

# 임의의 잔차 생성
residuals = np.random.normal(0, 1, 100)
plt.hist(residuals, bins=15)
plt.title("잔차의 분포 (정규성 확인)")
plt.show()
```

### 주의사항

- **등분산성**이 위배되면 모델의 신뢰도가 떨어질 수 있다.
- **독립성**이 보장되지 않으면 회귀계수의 해석이 왜곡될 수 있다.

---

## 선형 회귀의 원리 🔍

### 설명

선형 회귀의 핵심은 **독립변수(X)** 와 **종속변수(y)** 사이의 관계를 가장 잘 설명하는 **직선**을 찾는 것이다. 이때 직선의 기울기와 절편을 각각 **회귀계수**라 하며, 이 값들은 **잔차(Residuals)** 의 제곱합이 최소가 되도록 결정된다.

### 예시

```python
# 임의의 데이터에 대한 회귀선 시각화
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([3, 5, 7, 9, 11])
model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

plt.scatter(X, y, color='blue', label='실제 데이터')
plt.plot(X, y_pred, color='red', label='회귀선')
plt.legend()
plt.title("선형 회귀의 원리")
plt.show()
```

### 주의사항

- **잔차**는 예측값과 실제값의 차이로, 이 값이 작을수록 모델의 예측력이 높다.
- 모든 잔차의 제곱합을 **SSE**(Sum of Squared Errors)라 하며, 이 값을 최소화하는 것이 목표이다.

---

## 선형 회귀 모델 학습 🏋️

### 설명

선형 회귀 모델은 **잔차**를 기반으로 성능을 평가하며, 대표적인 **손실 함수**로는 **MSE**(Mean Squared Error)가 사용된다. MSE는 잔차의 제곱 평균으로 계산되며, 이 값을 최소화하는 방향으로 **최적화**가 진행된다. 최적화 방법으로는 **경사 하강법**이나 **최소제곱법**이 주로 사용된다.

### 예시

```python
# 경사 하강법을 이용한 선형 회귀 계수 추정 (간단 예시)
import numpy as np

X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])
w = 0.0  # 초기 기울기
b = 0.0  # 초기 절편
learning_rate = 0.01

for epoch in range(1000):
    y_pred = w * X + b
    error = y_pred - y
    w_grad = (2 / len(X)) * np.dot(error, X)
    b_grad = (2 / len(X)) * np.sum(error)
    w -= learning_rate * w_grad
    b -= learning_rate * b_grad

print(f"학습된 기울기: {w:.2f}, 절편: {b:.2f}")
```
**출력 예시**:
```
학습된 기울기: 2.00, 절편: 0.00
```

### 주의사항

- **MSE**는 볼록 함수이므로, 국소 최솟값이 전역 최솟값과 일치한다.
- **학습이 완료된 후**에는 회귀계수를 고정하여 예측만 수행하며, 추가 학습은 진행하지 않는다.

---

## 다중 선형 회귀 개념 📊

### 설명

**다중 선형 회귀**는 2개 이상의 **독립변수**가 1개의 **종속변수**에 영향을 미치는 경우를 다룬다. 이때 회귀식은 `y = β0 + β1x1 + β2x2 + ... + βn xn` 형태로 확장된다. 또한, 독립변수와 종속변수의 개수에 따라 **단변수 회귀**, **다변수 회귀**, **다중 출력 회귀**로 분류할 수 있다.

### 예시

```python
# 다중 선형 회귀 예시
import numpy as np
from sklearn.linear_model import LinearRegression

# 데이터 생성
X = np.array([
    [1, 2],
    [2, 1],
    [3, 4],
    [4, 3],
    [5, 5]
])
y = np.array([7, 6, 15, 14, 20])

# 모델 학습
model = LinearRegression().fit(X, y)
prediction = model.predict(np.array([[6, 2]]))
print(f"[6, 2] 입력 시 예측값: {prediction[0]:.2f}")
```
**출력 예시**:
```
[6, 2] 입력 시 예측값: 17.60
```

### 주의사항

- **다중공선성**(독립변수 간 상관관계)이 높으면 회귀계수의 해석이 어려워질 수 있다.
- 독립변수의 수가 많아질수록 과적합 위험이 증가하므로, 변수 선택이 중요하다.

---

## 다중 출력 회귀 🧮

### 설명

**다중 출력 회귀**는 여러 개의 **독립변수**가 여러 개의 **종속변수**에 동시에 영향을 미치는 경우를 의미한다. 이때 각 종속변수마다 별도의 회귀식을 구성하며, 총 `n × m`개의 회귀계수를 추정하게 된다.

### 예시

```python
# 다중 출력 선형 회귀 예시
import numpy as np
from sklearn.linear_model import LinearRegression

# 입력 데이터 (5개 샘플, 3개 독립변수)
X = np.array([
    [1, 2, 3],
    [2, 3, 4],
    [3, 4, 5],
    [4, 5, 6],
    [5, 6, 7]
])
# 두 개의 종속변수
Y = np.array([
    [10, 20],
    [14, 25],
    [18, 30],
    [22, 35],
    [26, 40]
])

model = LinearRegression().fit(X, Y)
pred = model.predict(np.array([[6, 7, 8]]))
print(f"입력 [6, 7, 8] 예측값: {pred[0]}")
```
**출력 예시**:
```
입력 [6, 7, 8] 예측값: [30. 45.]
```

### 주의사항

- **다중 출력 회귀**에서는 각 종속변수별로 예측 성능을 별도로 평가해야 한다.
- 데이터의 크기가 작을 경우, 모델이 불안정해질 수 있다.

---

## 주요 회귀 평가 지표 📏

### 설명

회귀 모델의 성능을 평가하기 위해 다양한 **지표**가 사용된다. 대표적으로 **MSE**, **MAE**, **RMSE**, **R²(결정계수)** 가 있다.

- **MSE(Mean Squared Error)**: 오차의 제곱 평균. 큰 오차에 더 큰 가중치를 부여하며, 이상치에 민감하다.
- **MAE(Mean Absolute Error)**: 오차의 절대값 평균. 모든 오차에 동일한 가중치를 부여하며, 이상치에 강건하다.
- **RMSE(Root Mean Squared Error)**: MSE의 제곱근. 단위가 종속변수와 동일하며, MSE보다 이상치에 덜 민감하다.
- **R²(결정계수)**: 모델이 종속변수의 변동성을 얼마나 설명하는지 나타내는 지표로, 1에 가까울수록 설명력이 높다.

### 예시

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

y_true = np.array([10, 20, 30, 40, 50])
y_pred = np.array([12, 18, 33, 37, 52])

mse = mean_squared_error(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)

print(f"MSE: {mse:.2f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}")
```
**출력 예시**:
```
MSE: 4.80, MAE: 2.40, RMSE: 2.19, R²: 0.98
```

### 주의사항

- **MSE**와 **RMSE**는 이상치에 민감하므로 데이터 분포에 따라 적절한 지표를 선택해야 한다.
- **R²** 값이 1에 가까울수록 모델의 설명력이 높지만, 과적합 여부도 함께 고려해야 한다.

---

## 로지스틱 회귀 개요 🔑

### 설명

**로지스틱 회귀**는 **범주형(카테고리형) 데이터**를 예측하는 데 사용되는 회귀 기법이다. 입력 변수의 단순한 선형 조합만으로는 분류가 어려운 경우, **로지스틱 함수**(시그모이드 함수)를 적용하여 0과 1 사이의 확률값을 산출하고, 이를 기준으로 분류를 수행한다.

### 예시

```python
# 로지스틱 회귀를 이용한 이진 분류 예시
from sklearn.linear_model import LogisticRegression
import numpy as np

X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([0, 0, 0, 1, 1, 1])  # 0: 음성, 1: 양성

model = LogisticRegression()
model.fit(X, y)
prob = model.predict_proba([[3.5]])[0, 1]
print(f"입력 3.5의 양성 클래스 확률: {prob:.2f}")
```
**출력 예시**:
```
입력 3.5의 양성 클래스 확률: 0.50
```

### 주의사항

- **로지스틱 회귀**는 선형 결정 경계를 가지므로, 복잡한 패턴을 가진 데이터에는 한계가 있다.
- 분류 임계값(기본 0.5)을 조정하여 민감도와 특이도를 조절할 수 있다.

---

## 시그모이드 함수 📈

### 설명

**시그모이드 함수**는 실수 입력을 받아 0과 1 사이의 값을 출력하는 **비선형 함수**이다. 로지스틱 회귀에서 예측값을 확률로 변환하는 데 사용된다. 함수식은 다음과 같다:

\[
f(x) = \frac{1}{1 + e^{-x}}
\]

### 예시

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)
sigmoid = 1 / (1 + np.exp(-x))

plt.plot(x, sigmoid)
plt.title("시그모이드 함수")
plt.xlabel("입력값")
plt.ylabel("출력값")
plt.show()
```

### 주의사항

- 입력값이 0에 가까울수록 출력이 0.5에 근접한다.
- 임계값을 기준으로 이진 분류가 가능하다.

---

## 로지스틱 회귀 학습 과정 🔄

### 설명

로지스틱 회귀의 학습 과정은 다음과 같다.

1. **선형 모델 계산**: 입력 변수의 선형 조합을 계산한다.
2. **시그모이드 함수 통과**: 선형 조합 결과를 시그모이드 함수에 입력하여 확률값을 얻는다.
3. **로그 손실 함수 계산**: 예측값과 실제값의 차이를 **로그 손실 함수**(binary cross-entropy)로 측정한다.
4. **최적화 진행**: 손실을 최소화하도록 모델 파라미터를 업데이트한다.
5. **검증**: 테스트 데이터로 성능을 평가한다.

### 예시

```python
# 로지스틱 회귀 학습 및 예측 예시
from sklearn.linear_model import LogisticRegression
import numpy as np

X = np.array([[0], [1], [2], [3], [4], [5]])
y = np.array([0, 0, 0, 1, 1, 1])

model = LogisticRegression()
model.fit(X, y)
pred = model.predict([[2.5]])
print(f"2.5 입력 시 예측 클래스: {pred[0]}")
```
**출력 예시**:
```
2.5 입력 시 예측 클래스: 1
```

### 주의사항

- **로그 손실 함수**는 예측값이 실제값과 가까울수록 값이 작아진다.
- 검증 데이터에서 성능이 충분히 확보될 때까지 학습을 반복한다.

---

## 다중 분류 확장 🌳

### 설명

**로지스틱 회귀**는 본래 이진 분류에 적합하지만, 여러 개의 이진 분류기를 **트리 구조**로 결합하거나 **One-vs-Rest** 방식 등으로 **다중 분류** 문제에도 적용할 수 있다. 그러나 선형 결정 경계만을 사용하므로, 복잡한 데이터에서는 성능에 한계가 있다.

### 예시

```python
# 다중 클래스 로지스틱 회귀 예시
from sklearn.linear_model import LogisticRegression
import numpy as np

X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([0, 1, 2, 0, 1, 2])  # 세 개의 클래스

model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(X, y)
pred = model.predict([[3.7]])
print(f"3.7 입력 시 예측 클래스: {pred[0]}")
```
**출력 예시**:
```
3.7 입력 시 예측 클래스: 2
```

### 주의사항

- **다중 분류**에서는 각 클래스별로 확률을 산출하며, 가장 높은 확률의 클래스를 선택한다.
- **로지스틱 회귀**는 복잡한 비선형 경계가 필요한 데이터에는 적합하지 않다.