# 5. 모델 성능 최적화: 하이퍼파라미터 튜닝의 기술

## 목차
- [1. 하이퍼파라미터란 무엇인가? (파라미터 vs. 하이퍼파라미터)](#1-하이퍼파라미터란-무엇인가-파라미터-vs-하이퍼파라미터)
- [2. 가장 중요한 하이퍼파라미터: 학습률 (Learning Rate)](#2-가장-중요한-하이퍼파라미터-학습률-learning-rate)
  - [학습률 스케줄링: 똑똑하게 학습률 조절하기](#학습률-스케줄링-똑똑하게-학습률-조절하기)
- [3. 탐색 전략 1: 그리드 탐색과 랜덤 탐색](#3-탐색-전략-1-그리드-탐색과-랜덤-탐색)
  - [그리드 탐색 (Grid Search)](#그리드-탐색-grid-search)
  - [랜덤 탐색 (Random Search)](#랜덤-탐색-random-search)
- [4. 탐색 전략 2: 베이지안 최적화 (Bayesian Optimization)](#4-탐색-전략-2-베이지안-최적화-bayesian-optimization)
- [5. 실전 튜닝 워크플로우: Keras Tuner 활용하기](#5-실전-튜닝-워크플로우-keras-tuner-활용하기)

---

## 1. 하이퍼파라미터란 무엇인가? (파라미터 vs. 하이퍼파라미터)

- **파라미터 (Parameters)** : **모델이 데이터로부터 스스로 학습하는 변수**입니다. (예: 신경망의 가중치 `w`와 편향 `b`)
- **하이퍼파라미터 (Hyperparameters)** : **모델이 학습하기 전에 사용자가 직접 설정해주는 값**입니다. 모델의 구조나 학습 방식을 결정하며, 최적의 하이퍼파라미터를 찾는 과정을 '튜닝'이라고 합니다.
    - 예: 학습률, 옵티마이저 종류, 은닉층의 개수, 드롭아웃 비율 등

## 2. 가장 중요한 하이퍼파라미터: 학습률 (Learning Rate)

**학습률**은 경사 하강법에서 모델의 가중치를 한 번에 얼마나 업데이트할지를 결정하는, **가장 중요하고 민감한** 하이퍼파라미터입니다.
- **학습률이 너무 크면**: 최적점을 지나쳐 버리며 손실 값이 발산(진동)할 수 있습니다.
- **학습률이 너무 작으면**: 학습 속도가 매우 느리고, 최적점에 도달하기 전에 학습이 끝나거나 지역 최솟값(local minima)에 빠질 수 있습니다.

### 학습률 스케줄링: 똑똑하게 학습률 조절하기

학습 내내 동일한 학습률을 사용하는 것보다, 학습 진행 상황에 따라 학습률을 동적으로 조절하는 **학습률 스케줄링(Learning Rate Scheduling)**  이 더 효과적입니다.
- **전략**: 학습 초기에는 비교적 큰 학습률로 빠르게 최적점 근처로 이동하고, 학습이 진행됨에 따라 학습률을 점차 줄여나가며 최적점에 세밀하게 수렴시킵니다.
- **종류**: Step Decay, Cosine Annealing, ReduceLROnPlateau 등 다양한 스케줄러가 있습니다.

## 3. 탐색 전략 1: 그리드 탐색과 랜덤 탐색

최적의 하이퍼파라미터 조합을 어떻게 찾을까요?

### 그리드 탐색 (Grid Search)

- **방법**: 사용자가 지정한 하이퍼파라미터 값들의 **모든 가능한 조합**을 격자(grid)처럼 만들어, 하나씩 모두 시도해보는 방식입니다.
- **장점**: 모든 조합을 탐색하므로 최적의 조합을 찾을 가능성이 높습니다.
- **단점**: 탐색해야 할 조합의 수가 기하급수적으로 늘어나, 시간이 매우 오래 걸립니다.

### 랜덤 탐색 (Random Search)

- **방법**: 사용자가 지정한 범위 내에서 하이퍼파라미터 값들을 **무작위로 샘플링**하여, 정해진 횟수만큼만 시도해보는 방식입니다.
- **장점**: 그리드 탐색보다 훨씬 적은 시도로 좋은 성능의 조합을 찾을 수 있습니다. 중요하지 않은 하이퍼파라미터에 시간을 낭비할 가능성이 줄어듭니다.
- **단점**: 무작위 탐색이므로 최적의 조합을 놓칠 수 있습니다.

> **실전에서는?**: 일반적으로 **랜덤 탐색이 그리드 탐색보다 더 효율적**이라고 알려져 있습니다. 먼저 랜덤 탐색으로 좋은 성능을 보이는 하이퍼파라미터의 대략적인 범위를 찾고, 그 주변에서 다시 그리드 탐색을 수행하는 전략이 효과적입니다.

## 4. 탐색 전략 2: 베이지안 최적화 (Bayesian Optimization)

- **방법**: **이전 탐색 결과를 바탕으로, 다음 탐색할 지점을 더 똑똑하게 결정**하는 방식입니다.
- **핵심 원리**:
    1.  지금까지의 (하이퍼파라미터, 성능) 결과들을 바탕으로, 아직 탐색하지 않은 영역의 성능을 예측하는 확률 모델(대리 모델, Surrogate Model)을 만듭니다.
    2.  이 모델을 사용하여 '현재까지의 최고 성능을 넘어설 가능성이 가장 높은' 지점(획득 함수, Acquisition Function 최대 지점)을 다음 탐색 지점으로 선택합니다.
- **장점**: 그리드/랜덤 탐색보다 훨씬 적은 횟수의 시도로 최적의 하이퍼파라미터를 찾을 수 있어 매우 효율적입니다.
- **단점**: 구현이 상대적으로 복잡합니다. (Hyperopt, Optuna, Scikit-optimize 같은 라이브러리 사용)

## 5. 실전 튜닝 워크플로우: Keras Tuner 활용하기

**Keras Tuner**는 텐서플로우 Keras 모델의 하이퍼파라미터를 쉽게 튜닝할 수 있도록 도와주는 라이브러리입니다.

```python
# !pip install keras-tuner -q
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt

# 1. 튜닝할 하이퍼파라미터를 지정하여 모델 빌드 함수 정의
def build_model(hp):
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28, 28)))

    # 은닉층의 유닛 개수를 32~512 사이에서 32 간격으로 탐색
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    model.add(keras.layers.Dense(units=hp_units, activation='relu'))
    model.add(keras.layers.Dense(10, activation='softmax'))

    # 학습률을 0.01, 0.001, 0.0001 중에서 선택
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 2. 튜너(Tuner) 설정: 랜덤 탐색 수행
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy', # 검증 정확도를 기준으로 평가
    max_trials=10, # 10번의 조합을 시도
    directory='my_dir',
    project_name='intro_to_kt')

# (데이터 로드 및 전처리 과정은 생략)
# fashion_mnist = keras.datasets.fashion_mnist
# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
# x_train, y_train, x_test, y_test ...

# 3. 하이퍼파라미터 탐색 시작
# tuner.search(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

# 4. 최적의 하이퍼파라미터 확인
# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
# print(f"최적의 은닉층 유닛 수: {best_hps.get('units')}")
# print(f"최적의 학습률: {best_hps.get('learning_rate')}")
```
