# 5. 운영 가능한 LLM 앱 만들기: 보안, 비용, 신뢰성

## 목차
- [1. 보안: 내 집의 열쇠를 안전하게 보관하기 (API 키 관리)](#1-보안-내-집의-열쇠를-안전하게-보관하기-api-키-관리)
  - [왜 .env 파일을 사용해야 하는가?](#왜-env-파일을-사용해야-하는가)
  - [실전 코드: .env로 API 키 관리하기](#실전-코드-env로-api-키-관리하기)
- [2. 비용: 돈이 새는 곳을 막아라 (토큰 사용량 모니터링)](#2-비용-돈이-새는-곳을-막아라-토큰-사용량-모니터링)
  - [토큰 기반 과금 체계의 이해](#토큰-기반-과금-체계의-이해)
  - [실전 코드: 사용량 모니터링 클래스 구현하기](#실전-코드-사용량-모니터링-클래스-구현하기)
- [3. 신뢰성: 우리 앱이 나쁜 일에 쓰이지 않도록 (오남용 방지)](#3-신뢰성-우리-앱이-나쁜-일에-쓰이지-않도록-오남용-방지)
  - [콘텐츠 안전 필터링 (Content Safety Filtering)](#콘텐츠-안전-필터링-content-safety-filtering)


---

## 1. 보안: 내 집의 열쇠를 안전하게 보관하기 (API 키 관리)

LLM API 키는 당신의 서비스와 계정에 접근할 수 있는 '마스터 키'입니다. 이 키가 외부에 유출되면, 해커가 당신의 계정으로 막대한 비용을 발생시키거나 악의적인 용도로 API를 사용할 수 있습니다.

### 왜 .env 파일을 사용해야 하는가?

- **소스 코드와 민감정보의 분리**: API 키 같은 민감정보를 코드에 직접 하드코딩하는 것은 최악의 보안 습관입니다. 코드가 Git 등을 통해 공개되면 API 키가 그대로 노출됩니다.
- **`.env` 파일**: 프로젝트의 루트 디렉토리에 `.env`라는 파일을 만들고, 그 안에 민감정보를 `KEY=VALUE` 형태로 저장합니다.
- **`.gitignore`**: `.env` 파일은 절대로 Git 저장소에 올라가면 안 됩니다. `.gitignore` 파일에 `.env`를 추가하여, 실수로라도 커밋되는 것을 방지해야 합니다.

### 실전 코드: .env로 API 키 관리하기

1.  **`python-dotenv` 라이브러리 설치**
    ```bash
    pip install python-dotenv
    ```

2.  **.env 파일 생성**
    ```
    # .env
    OPENAI_API_KEY="sk-..."
    SERPAPI_API_KEY="..."
    ```

3.  **.gitignore 파일에 추가**
    ```
    # .gitignore
    .env
    ```

4.  **파이썬 코드에서 사용**
    ```python
    import os
    from dotenv import load_dotenv

    # .env 파일에서 환경 변수를 로드
    load_dotenv()

    # 환경 변수에서 API 키를 안전하게 불러오기
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        raise ValueError("OPENAI_API_KEY가 .env 파일에 설정되지 않았습니다.")

    # 이제 api_key 변수를 사용하여 API 호출
    # client = openai.OpenAI(api_key=api_key)
    ```

## 2. 비용: 돈이 새는 곳을 막아라 (토큰 사용량 모니터링)

상용 LLM 서비스는 대부분 API 호출 시 사용된 **토큰(Token)**  의 양에 따라 비용을 청구합니다. 토큰은 모델이 텍스트를 처리하는 기본 단위입니다.

### 토큰 기반 과금 체계의 이해

- **입력/출력 토큰 분리 과금**: 대부분의 모델은 프롬프트에 사용된 '입력 토큰'과, 모델이 생성한 답변의 '출력 토큰'에 대해 서로 다른 요금을 적용합니다.
- **모델별 요금 차이**: GPT-4o, Claude 3 Opus 같은 고성능 모델일수록 토큰당 비용이 비쌉니다.
- **비용 계산**: `총 비용 = (입력 토큰 수 * 토큰당 입력 요금) + (출력 토큰 수 * 토큰당 출력 요금)`

### 실전 코드: 사용량 모니터링 클래스 구현하기

애플리케이션에 API 호출 비용을 실시간으로 추적하고 관리하는 기능을 추가해 봅시다.

```python
import tiktoken # OpenAI의 토큰 계산 라이브러리

class CostMonitor:
    def __init__(self, model_name="gpt-4o"):
        self.total_cost_usd = 0.0
        self.model_pricing = {
            "gpt-4o": {"input": 5.00, "output": 15.00},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60}
        }
        if model_name not in self.model_pricing:
            raise ValueError("지원하지 않는 모델입니다.")
        self.pricing = self.model_pricing[model_name]
        self.tokenizer = tiktoken.encoding_for_model(model_name)

    def calculate_cost(self, prompt, completion):
        """프롬프트와 답변 내용으로 비용을 계산하고 누적합니다."""
        prompt_tokens = len(self.tokenizer.encode(prompt))
        completion_tokens = len(self.tokenizer.encode(completion))

        cost = ((prompt_tokens / 1_000_000) * self.pricing["input"] +
                (completion_tokens / 1_000_000) * self.pricing["output"])
        
        self.total_cost_usd += cost
        return cost

    def get_total_cost_str(self):
        return f"누적 사용 요금: ${self.total_cost_usd:.6f}"

# --- 사용 예시 ---
monitor = CostMonitor(model_name="gpt-4o-mini")

# 첫 번째 API 호출
prompt1 = "안녕하세요?"
completion1 = "네, 안녕하세요! 무엇을 도와드릴까요?"
cost1 = monitor.calculate_cost(prompt1, completion1)
print(f"호출 1 비용: ${cost1:.6f}")
print(monitor.get_total_cost_str())

# 두 번째 API 호출
prompt2 = "LangChain에 대해 설명해주세요."
completion2 = "LangChain은 LLM을 활용한 애플리케이션 개발을 돕는 프레임워크입니다..."
cost2 = monitor.calculate_cost(prompt2, completion2)
print(f"호출 2 비용: ${cost2:.6f}")
print(monitor.get_total_cost_str())
```

## 3. 신뢰성: 우리 앱이 나쁜 일에 쓰이지 않도록 (오남용 방지)

LLM은 유해하거나, 편향되거나, 위험한 콘텐츠를 생성할 수 있습니다. 실제 서비스에서는 이러한 오남용을 방지하기 위한 안전장치가 필수적입니다.

### 콘텐츠 안전 필터링 (Content Safety Filtering)

대부분의 상용 LLM API는 자체적으로 유해 콘텐츠(증오 발언, 성적 콘텐츠, 자기 상해, 폭력 등)를 감지하고 차단하는 안전 필터를 내장하고 있습니다.

- **OpenAI Moderation API**: 텍스트가 OpenAI의 사용 정책을 위반하는지 여부를 분류해주는 별도의 무료 API를 제공합니다. LLM에 프롬프트를 보내기 전에, 이 Moderation API를 먼저 호출하여 사용자 입력의 안전성을 검사하는 파이프라인을 구축할 수 있습니다.
- **응답 필터링**: LLM의 답변이 생성된 후에도, 자체적으로 정의한 키워드 필터나 또 다른 분류 모델을 통해 유해성을 검사하고 사용자에게 제공하기 전에 차단할 수 있습니다.
