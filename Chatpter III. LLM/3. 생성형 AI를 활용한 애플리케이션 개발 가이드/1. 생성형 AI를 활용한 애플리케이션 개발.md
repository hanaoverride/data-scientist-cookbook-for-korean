# 1. 생성형 AI 애플리케이션 개발 실전: RAG와 에이전트

## 목차
- [1. LLM 애플리케이션 개발의 핵심 과제](#1-llm-애플리케이션-개발의-핵심-과제)
- [2. 패턴 1: RAG (검색 증강 생성) - LLM에게 최신 지식과 사실을 주입하기](#2-패턴-1-rag-검색-증강-생성---llm에게-최신-지식과-사실을-주입하기)
  - [RAG 아키텍처 설계](#rag-아키텍처-설계)
  - [RAG 구현의 핵심: 벡터 데이터베이스와 임베딩](#rag-구현의-핵심-벡터-데이터베이스와-임베딩)
- [3. 패턴 2: 에이전트 (Agent) - LLM에게 행동할 수 있는 도구를 쥐여주기](#3-패턴-2-에이전트-agent---llm에게-행동할-수-있는-도구를-쥐여주기)
  - [에이전트 아키텍처 설계 (ReAct 프레임워크)](#에이전트-아키텍처-설계-react-프레임워크)
- [4. LLM 애플리케이션의 결과 평가하기](#4-llm-애플리케이션의-결과-평가하기)
  - [LLM을 평가자로 활용하기 (LLM-as-a-Judge)](#llm을-평가자로-활용하기-llm-as-a-judge)

---

## 1. LLM 애플리케이션 개발의 핵심 과제

LLM을 활용한 애플리케이션을 개발할 때 우리는 다음과 같은 LLM의 근본적인 한계를 반드시 해결해야 합니다.
- **할루시네이션 (Hallucination)** : 어떻게 하면 사실에 기반한 답변만 생성하게 할까?
- **지식의 한계 (Knowledge Cut-off)** : 어떻게 하면 최신 정보나 우리 회사 내부 문서를 참고하여 답변하게 할까?
- **행동의 한계 (Action-taking)** : 어떻게 하면 단순히 텍스트만 생성하는 것을 넘어, 외부 시스템을 호출하거나 계산을 수행하게 할까?

이 문제들을 해결하는 핵심적인 아키텍처가 바로 RAG와 에이전트입니다.

## 2. 패턴 1: RAG (검색 증강 생성) - LLM에게 최신 지식과 사실을 주입하기

**RAG(Retrieval-Augmented Generation)**  는 LLM이 답변을 생성하기 전에, **외부의 신뢰할 수 있는 지식 소스(Knowledge Base)에서 관련 정보를 먼저 검색**하고, 검색된 내용을 참고하여 답변을 생성하는 방식입니다.

### RAG 아키텍처 설계

```mermaid
graph TD
    A[사용자 질문] --> B{1. 정보 검색기 (Retriever)};
    C[벡터 DB<br>(사내 문서, 최신 뉴스 등)] --> B;
    B -- "관련 문서 청크" --> D{2. 프롬프트 재구성};
    A -- "원본 질문" --> D;
    D --> E[3. LLM 생성기 (Generator)];
    E --> F[최종 답변];
```

1.  **정보 검색 (Retrieval)** : 사용자의 질문과 가장 관련성이 높은 문서를 **벡터 데이터베이스**에서 찾습니다.
2.  **정보 증강 (Augmentation)** : 검색된 문서 내용(맥락)과 원본 질문을 조합하여 LLM에게 전달할 프롬프트를 만듭니다.
3.  **생성 (Generation)** : LLM은 주어진 맥락 정보를 바탕으로, 사실에 근거한 답변을 생성합니다.

### RAG 구현의 핵심: 벡터 데이터베이스와 임베딩

- **임베딩(Embedding)** : 문서를 LLM이 이해할 수 있는 숫자 벡터로 변환하는 과정입니다.
- **벡터 데이터베이스(Vector Database)** : 이 임베딩 벡터들을 저장하고, 질문 벡터와 가장 유사한(가까운) 문서 벡터를 매우 빠르게 찾아주는 특수한 데이터베이스입니다. (예: FAISS, ChromaDB, Pinecone)

```python
# LangChain을 이용한 RAG의 개념적 구현
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain.chains import RetrievalQA

# 1. 문서 로드 및 벡터 DB 생성 (실제로는 여러 문서를 로드)
documents = ["RAG는 검색 증강 생성을 의미합니다.", "LangChain은 LLM 앱 개발 프레임워크입니다."]
# text_splitter = ... (문서를 청크로 나누는 과정)
# db = FAISS.from_documents(splitted_docs, OpenAIEmbeddings())

# 2. RAG 체인 생성
# retriever = db.as_retriever()
# qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)

# 3. 질문하기
# question = "RAG가 무엇인가요?"
# answer = qa_chain.run(question) # LLM은 documents 내용을 참고하여 답변
```

## 3. 패턴 2: 에이전트 (Agent) - LLM에게 행동할 수 있는 도구를 쥐여주기

**에이전트**는 LLM이 단순한 텍스트 생성을 넘어, **외부 도구(Tools)를 사용하여 스스로 행동**하고 문제를 해결하도록 만드는 아키텍처입니다.

### 에이전트 아키텍처 설계 (ReAct 프레임워크)

에이전트는 **ReAct(Reason + Act)**  라는 프레임워크를 기반으로, **[생각 -> 행동 -> 관찰]** 사이클을 반복하며 작업을 수행합니다.

1.  **도구(Tools) 정의**: 에이전트가 사용할 수 있는 도구 목록을 정의합니다. 각 도구는 특정 기능을 수행합니다. (예: `google_search`, `python_repl`, `database_query`, `calculator`)
2.  **LLM의 계획 수립 (Reason)** : 사용자의 목표를 달성하기 위해, 어떤 도구를 어떤 순서로 사용해야 할지 계획을 세웁니다.
3.  **도구 실행 (Act)** : 계획에 따라 도구를 실행하고 그 결과를 얻습니다.
4.  **결과 관찰 (Observe)** : 도구 실행 결과를 바탕으로, 목표가 달성되었는지 확인하고 다음 계획을 세웁니다.

```python
# LangChain을 이용한 에이전트의 개념적 구현
from langchain.agents import load_tools, initialize_agent, AgentType
from langchain_openai import OpenAI

# 1. 도구 정의
llm = OpenAI(temperature=0)
tools = load_tools(["serpapi", "llm-math"], llm=llm) # 검색과 계산기 도구

# 2. 에이전트 초기화
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# 3. 에이전트 실행
# LLM이 스스로 검색과 계산을 수행하여 답을 찾음
agent.run("어제 나스닥 지수에 1.5를 곱하면 얼마야?")
```

## 4. LLM 애플리케이션의 결과 평가하기

RAG나 에이전트의 답변은 정해진 정답이 없는 경우가 많아 평가하기 까다롭습니다.

### LLM을 평가자로 활용하기 (LLM-as-a-Judge)

최근에는 **더 강력한 상위 LLM(예: GPT-4)을 '평가자'로 활용**하여, 개발 중인 애플리케이션의 답변 품질을 평가하는 방법이 널리 쓰입니다.

- **평가 프롬프트 예시**:
    ```
    [지시]
    당신은 답변의 품질을 평가하는 공정한 평가자입니다. 아래 [질문]과 [생성된 답변]을 보고, 답변이 [평가 기준]에 얼마나 부합하는지 1점에서 5점 사이로 평가하고 그 이유를 설명해주세요.

    [질문]
    RAG가 무엇인가요?

    [생성된 답변]
    RAG는 검색을 통해 정보를 얻는 것입니다.

    [평가 기준]
    - 정확성: 답변이 사실에 기반하고 있는가?
    - 완전성: 질문의 핵심적인 내용을 모두 포함하고 있는가?
    ```
