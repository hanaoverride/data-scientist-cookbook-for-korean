# 2. μ¤ν”μ†μ¤μ™€ μƒμ© LLM

## λ©μ°¨ π“‘

### 1. LLM μ ν• λΉ„κµ π¤–
- [1.1 μ¤ν”μ†μ¤ LLM](#μ¤ν”μ†μ¤-llm)
- [1.2 μƒμ© LLM](#μƒμ©-llm)

### 2. LLM μ„±λ¥ λ¦¬λ”λ³΄λ“ π†
- [2.1 μ£Όμ” λ²¤μΉλ§ν¬ λ° λ¦¬λ”λ³΄λ“](#μ£Όμ”-λ²¤μΉλ§ν¬-λ°-λ¦¬λ”λ³΄λ“)


## LLM μ ν• λΉ„κµ π¤–

### μ¤ν”μ†μ¤ LLM

#### μ„¤λ…

**μ¤ν”μ†μ¤ LLM**μ€ λ¨λΈμ νλΌλ―Έν„°κ°€ κ³µκ°λμ–΄ λ„κµ¬λ‚ μ¶”κ°€ ν•™μµμ΄λ‚ μ¶”λ΅ μ— ν™μ©ν•  μ μλ‹¤. λ€ν‘μ μΌλ΅ **LLaMA, MPT, Alpaca, Vicuna, Falcon** λ“±μ΄ μλ‹¤. μΌλ°μ μΌλ΅ λ¬΄λ£λ΅ μ‚¬μ©ν•  μ μμ§€λ§, μΈνΌλ°μ¤ νμ΄ν”„λΌμΈμ„ μ§μ ‘ κµ¬μ¶•ν•΄μ•Ό ν•λ©°, μƒμ—…μ  μ΄μ© μ‹ λΌμ΄μ„ μ¤ ν™•μΈμ΄ ν•„μ”ν•λ‹¤.

#### μμ‹

```python
# μ¤ν”μ†μ¤ LLM μμ‹ (HuggingFace Transformers ν™μ©)
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mosaicml/mpt-7b")
model = AutoModelForCausalLM.from_pretrained("mosaicml/mpt-7b")

input_text = "κ°€μ„ μ €λ… μ‚°μ±…μ— μ–΄μΈλ¦¬λ” μ·μ°¨λ¦Όμ„ μ¶”μ²ν•΄μ¤."
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

#### μ£Όμμ‚¬ν•­

- **μƒμ—…μ  μ‚¬μ©** μ „μ—λ” λ°λ“μ‹ λΌμ΄μ„ μ¤ μ΅°ν•­μ„ ν™•μΈν•΄μ•Ό ν•λ‹¤.
- μ§μ ‘ μΈνΌλ°μ¤ μ„λ²„λ¥Ό μ΄μν•  κ²½μ°, **μ„λ²„ λΉ„μ©**μ΄ λ°μƒν•  μ μλ‹¤.

---

### μƒμ© LLM

#### μ„¤λ…

**μƒμ© LLM**μ€ λ¨λΈ νλΌλ―Έν„°λ¥Ό κ³µκ°ν•μ§€ μ•κ³ , **API μ—”λ“ν¬μΈνΈ**λ¥Ό ν†µν•΄ μ ‘κ·Όν•λ‹¤. λ€ν‘μ μΌλ΅ **ChatGPT(OpenAI), Claude(Anthropic), Bard(Google)** λ“±μ΄ μλ‹¤. μ‚¬μ©λ‰μ— λ”°λΌ λΉ„μ©μ΄ λ°μƒν•μ§€λ§, μΈν”„λΌ κ΄€λ¦¬κ°€ ν•„μ” μ—†κ³ , ν΄λΌμ°λ“ μΈν”„λΌμ μ΄μ μ„ λ„λ¦΄ μ μλ‹¤. μΌλ°μ μΌλ΅ μ¤ν”μ†μ¤ λ¨λΈλ³΄λ‹¤ μ„±λ¥μ΄ μ°μν•λ‹¤.

#### μμ‹

```python
# μƒμ© LLM API μμ‹ (OpenAI)
import openai

def get_ai_suggestion(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "λ‹Ήμ‹ μ€ ν¨μ… μ½”λ”” μ „λ¬Έκ°€μ…λ‹λ‹¤."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

result = get_ai_suggestion("λ΄„ μ†ν’μ— μ–΄μΈλ¦¬λ” μ·μ°¨λ¦Όμ„ μ¶”μ²ν•΄μ¤.")
print(result)
```

#### μ£Όμμ‚¬ν•­

- **API μ‚¬μ©λ‰**μ— λ”°λΌ λΉ„μ©μ΄ λ°μƒν•λ―€λ΅, μμ‚°μ„ κ³ λ ¤ν•μ—¬ μ‚¬μ©ν•΄μ•Ό ν•λ‹¤.
- κ° μ„λΉ„μ¤μ **Context Length**(μ…λ ¥/μ¶λ ¥ ν† ν° ν•λ„)μ™€ κ°€κ²© μ •μ±…μ„ ν™•μΈν•΄μ•Ό ν•λ‹¤.

---

## LLM μ„±λ¥ λ¦¬λ”λ³΄λ“ π†

### μ£Όμ” λ²¤μΉλ§ν¬ λ° λ¦¬λ”λ³΄λ“

#### μ„¤λ…

LLMμ μ„±λ¥μ€ λ‹¤μ–‘ν• **λ²¤μΉλ§ν¬**(μ: Chatbot Arena, MT Bench, MMLU λ“±)λ¥Ό ν†µν•΄ ν‰κ°€λλ‹¤. λ€ν‘μ μΈ λ¦¬λ”λ³΄λ“λ΅λ” HuggingFace Open LLM Leaderboard, LMSYS Chatbot Arena λ“±μ΄ μλ‹¤. μ£Όμ” ν‰κ°€μ§€ν‘λ” **Elo μ μ, λ©€ν‹°νƒμ¤ν¬ μ •ν™•λ„, λ€ν™” ν’μ§** λ“±μ΄λ‹¤.

| λ¨λΈλ…                | Arena Elo | MMLU (%) | SWE-bench (%) | λΌμ΄μ„ μ¤      |
|----------------------|-----------|----------|---------------|--------------|
| Gemini 2.5 Pro       | 1463      | 89.8     | 63.8          | Proprietary  |
| OpenAI o3-2025-04-16 | 1449      | 84.0     | 49.3          | Proprietary  |
| GPT-4o (2025-03-26)  | 1441      | -        | -             | Proprietary  |
| GPT-4.5 Preview      | 1436      | -        | -             | Proprietary  |
| Claude Opus 4        | 1417      | 88.8     | 79.4          | Proprietary  |
| Gemini 2.5 Flash     | 1415      | -        | -             | Proprietary  |
| DeepSeek R1          | 1413      | 71.5     | 49.2          | Apache 2.0   |
| GPT-4.1 (2025-04-14) | 1411      | 79.2     | 52.0          | Proprietary  |
| Grok 3 Preview       | 1408      | 80.2     | -             | Proprietary  |
| Claude Sonnet 4      | 1382      | 86.5     | 80.2          | Proprietary  |


#### μμ‹

```python
# λ¦¬λ”λ³΄λ“ μ •λ³΄ μ΅°ν μμ‹ (μ›Ή ν¬λ΅¤λ§)
import requests
from bs4 import BeautifulSoup

url = "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
# μ‹¤μ  λ°μ΄ν„° νμ‹± λ΅μ§μ€ μƒλµ
```

#### μ£Όμμ‚¬ν•­

- λ²¤μΉλ§ν¬ μ μλ” **μ—…λ°μ΄νΈ μ‹μ **κ³Ό **ν‰κ°€ κΈ°μ¤€**μ— λ”°λΌ λ‹¬λΌμ§ μ μλ‹¤.
- μ‹¤μ  μ• ν”λ¦¬μΌ€μ΄μ… λ©μ μ— λ§λ” **μ„±λ¥ μ§€ν‘**λ¥Ό μ°μ„ μ μΌλ΅ κ³ λ ¤ν•΄μ•Ό ν•λ‹¤.

