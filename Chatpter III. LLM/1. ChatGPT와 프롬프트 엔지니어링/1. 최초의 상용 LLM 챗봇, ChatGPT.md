# 1. ChatGPT의 심장: 트랜스포머와 어텐션 메커니즘

## 목차
- [1. ChatGPT 이전의 시대: RNN의 한계](#1-chatgpt-이전의-시대-rnn의-한계)
- [2. 혁명의 시작: 트랜스포머 (Transformer)](#2-혁명의-시작-트랜스포머-transformer)
- [3. 트랜스포머의 핵심 엔진: 어텐션 (Attention)](#3-트랜스포머의-핵심-엔진-어텐션-attention)
  - [어텐션의 직관적 의미: "중요한 단어에 집중하기"](#어텐션의-직관적-의미-중요한-단어에-집중하기)
  - [셀프 어텐션 (Self-Attention): 문장 스스로 의미를 파악하다](#셀프-어텐션-self-attention-문장-스스로-의미를-파악하다)
- [4. GPT는 어떻게 학습하고 문장을 생성하는가?](#4-gpt는-어떻게-학습하고-문장을-생성하는가)
  - [사전 학습 (Pre-training): 세상의 모든 텍스트를 읽다](#사전-학습-pre-training-세상의-모든-텍스트를-읽다)
  - [파인 튜닝 (Fine-tuning): 특정 작업의 전문가가 되다](#파인-튜닝-fine-tuning-특정-작업의-전문가가-되다)
  - [자기회귀 (Auto-regression): 다음 단어 예측하기](#자기회귀-auto-regression-다음-단어-예측하기)
- [5. ChatGPT는 무엇이 다른가?: RLHF의 힘](#5-chatgpt는-무엇이-다른가-rlhf의-힘)

---

## 1. ChatGPT 이전의 시대: RNN의 한계

과거의 언어 모델은 주로 **RNN(순환 신경망)**  에 기반했습니다. RNN은 단어를 순서대로 하나씩 처리하며 과거의 정보를 '기억'하려 했습니다. 하지만 치명적인 단점이 있었습니다.

- **장기 의존성 문제**: 문장이 길어지면, 문장 앞부분의 중요한 정보를 쉽게 잊어버렸습니다.
- **병렬 처리의 어려움**: 단어를 하나씩 순차적으로 처리해야 하므로, 대규모 데이터 학습에 매우 비효율적이었습니다.

## 2. 혁명의 시작: 트랜스포머 (Transformer)

2017년, 구글의 논문 "Attention Is All You Need"에서 발표된 **트랜스포머**는 이 모든 것을 바꿨습니다.
- **핵심 아이디어**: "단어를 순서대로 처리할 필요 없이, 문장 전체를 한 번에 보고 단어들 간의 관계를 파악하면 되지 않을까?"
- **결과**: RNN의 순환 구조를 완전히 버리고, **어텐션(Attention)**  이라는 메커니즘만으로 문장을 이해하는 새로운 구조를 제시했습니다. 이는 **병렬 처리**를 가능하게 하여 학습 속도를 획기적으로 높였고, **장기 의존성 문제**를 해결하여 언어 모델의 성능을 극적으로 끌어올렸습니다.

## 3. 트랜스포머의 핵심 엔진: 어텐션 (Attention)

### 어텐션의 직관적 의미: "중요한 단어에 집중하기"

우리가 문장을 해석할 때 모든 단어를 동일한 비중으로 보지 않는 것처럼, 어텐션은 모델이 특정 단어를 예측하거나 번역할 때, **입력 문장에서 어떤 단어에 더 '집중'해야 하는지**를 알려주는 메커니즘입니다.

- **예시 (번역)** : "I am a student"를 "나는 학생이다"로 번역할 때, 'student'를 번역하는 시점에서는 입력 문장의 'student'라는 단어에 가장 높은 '어텐션 점수'를 부여합니다.

### 셀프 어텐션 (Self-Attention): 문장 스스로 의미를 파악하다

트랜스포머의 핵심은 **셀프 어텐션**입니다. 이는 문장 내의 단어들이 **서로에게** 어텐션을 부여하여, 단어들 간의 문법적, 의미적 관계를 파악하는 과정입니다.

- **문장**: "그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤했기 때문이다."
- **'그것(it)'이 가리키는 대상은?**: 사람은 '그것'이 '동물'을 가리킨다는 것을 쉽게 알 수 있습니다.
- **셀프 어텐션의 역할**: 모델은 셀프 어텐션을 통해 '그것'이라는 단어를 처리할 때, 문장 내의 다른 단어들 중 '동물'이라는 단어에 가장 높은 어텐션 점수를 부여합니다. 이 과정을 통해 모델은 '그것'의 문맥적 의미가 '동물'과 강하게 연결되어 있음을 학습합니다.

> **핵심**: 셀프 어텐션은 문장 내 모든 단어 쌍에 대한 관계의 강도를 계산하여, 문장의 내부 구조를 깊이 있게 이해하는 것을 가능하게 합니다.

## 4. GPT는 어떻게 학습하고 문장을 생성하는가?

GPT(Generative Pre-trained Transformer)는 트랜스포머의 **디코더(Decoder)**  구조를 활용하여 문장 생성에 특화된 모델입니다.

### 사전 학습 (Pre-training): 세상의 모든 텍스트를 읽다

- **방법**: 인터넷의 수많은 책, 기사, 웹사이트 등 방대한 텍스트 데이터를 사용하여 **자기지도학습(Self-supervised Learning)**  을 수행합니다.
- **학습 목표**: 문장의 일부를 가리고, 가려진 단어가 무엇인지 맞추는 문제를 반복해서 풉니다. (예: "나는 [MASK]에 간다" -> [MASK] = '학교')
- **결과**: 이 과정을 통해 모델은 단어의 의미, 문법, 문맥, 그리고 세상의 다양한 지식을 스스로 학습하게 됩니다.

### 파인 튜닝 (Fine-tuning): 특정 작업의 전문가가 되다

사전 학습된 거대 모델을, 번역, 요약, 질의응답 등 특정 작업에 맞는 소규모의 정답 데이터셋으로 추가 학습시켜 해당 분야의 '전문가'로 만듭니다.

### 자기회귀 (Auto-regression): 다음 단어 예측하기

GPT가 문장을 생성하는 원리는 **자기회귀** 방식입니다.
1.  "옛날 옛적에" 라는 시작 단어들이 주어집니다.
2.  모델은 이 단어들을 바탕으로, 다음에 올 가장 확률 높은 단어, 예를 들어 '한'을 예측합니다.
3.  이제 "옛날 옛적에 한"을 새로운 입력으로 삼아, 그 다음 단어 '소녀가'를 예측합니다.
4.  이 과정을 반복하여 전체 문장을 생성합니다.

## 5. ChatGPT는 무엇이 다른가?: RLHF의 힘

GPT-3와 같은 기존 모델은 때때로 부정확하거나, 유해하거나, 사용자의 의도와 무관한 답변을 생성하는 문제가 있었습니다. ChatGPT는 이 문제를 해결하기 위해 **인간 피드백 기반 강화학습(RLHF, Reinforcement Learning from Human Feedback)**  이라는 특별한 파인 튜닝을 거칩니다.

1.  **1단계 (SFT 모델)** : 사람이 직접 작성한 고품질의 질문-답변 쌍으로 모델을 파인 튜닝합니다.
2.  **2단계 (보상 모델 학습)** : 하나의 질문에 대해 모델이 여러 개의 답변을 생성하면, 사람이 각 답변에 대한 선호도 순위를 매깁니다. 이 데이터를 사용하여, '좋은 답변'에 높은 점수를 주는 별도의 **보상 모델(Reward Model)**  을 학습시킵니다.
3.  **3단계 (강화학습)** : ChatGPT 모델이 새로운 답변을 생성할 때마다, 이 보상 모델이 점수를 매깁니다. ChatGPT는 이 '점수(보상)'를 최대로 받는 방향으로 자신의 정책을 업데이트합니다.

> **핵심**: RLHF를 통해 ChatGPT는 단순히 문법적으로 옳은 문장을 넘어, **인간이 선호하는, 더 유용하고 안전한 방향**으로 답변을 생성하도록 훈련됩니다.