# 3. 거대 언어 모델(LLM)은 어떻게 지금의 모습이 되었나?

## 목차
- [1. 언어 모델의 진화: 문제 해결의 역사](#1-언어-모델의-진화-문제-해결의-역사)
  - [1. 통계적 언어 모델 (SLM): 단어 빈도에 의존하다](#1-통계적-언어-모델-slm-단어-빈도에-의존하다)
  - [2. 신경망 언어 모델 (NLM): 단어의 '의미'를 배우다](#2-신경망-언어-모델-nlm-단어의-의미를-배우다)
  - [3. 사전 학습 언어 모델 (PLM): 세상을 미리 공부하다](#3-사전-학습-언어-모델-plm-세상을-미리-공부하다)
- [2. LLM의 게임 체인저: In-context Learning](#2-llm의-게임-체인저-in-context-learning)
  - [Zero-shot, One-shot, Few-shot Learning](#zero-shot-one-shot-few-shot-learning)
- [3. LLM 활용의 두 가지 길: 파인튜닝 vs. 프롬프트 엔지니어링](#3-llm-활용의-두-가지-길-파인튜닝-vs-프롬프트-엔지니어링)
- [4. LLM의 주요 활용 분야](#4-llm의-주요-활용-분야)

---

## 1. 언어 모델의 진화: 문제 해결의 역사

### 1. 통계적 언어 모델 (SLM): 단어 빈도에 의존하다

- **작동 방식**: "I am a" 다음에 "student"가 올 확률을, 훈련 데이터에서 "I am a student"라는 시퀀스가 얼마나 자주 등장했는지를 세어서 계산합니다.
- **한계 (희소성 문제, Sparsity Problem)** : 훈련 데이터에 한 번도 등장하지 않은 문장은 확률이 0이 되어 절대 생성할 수 없습니다. (예: "I am a pineapple")

### 2. 신경망 언어 모델 (NLM): 단어의 '의미'를 배우다

- **해결책**: 단어를 '의미'를 담은 벡터(워드 임베딩)로 표현하여, 단어 간 유사성을 학습합니다.
- **작동 방식**: Word2Vec, RNN, LSTM 등의 신경망을 사용하여, 문맥 속에서 단어의 의미를 파악하고 다음 단어를 예측합니다. "I am a student"를 본 적 없어도, 'student'와 비슷한 의미의 'teacher'가 그 자리에 올 수 있음을 학습합니다.
- **새로운 한계**: 각 특정 작업(번역, 감성 분석 등)을 수행하려면, 해당 작업에 맞는 대규모의 정답 데이터를 처음부터 새로 모아 모델을 학습시켜야 했습니다. 이는 매우 비효율적이었습니다.

### 3. 사전 학습 언어 모델 (PLM): 세상을 미리 공부하다

- **혁신적인 아이디어**: "인터넷의 방대한 텍스트로 언어 자체에 대한 일반적인 지식(문법, 상식 등)을 미리 학습시켜두고(사전 학습), 그 다음에 특정 작업에 맞게 약간만 추가 학습(파인튜닝)하면 어떨까?"
- **작동 방식 (GPT, BERT 등)** :
    1.  **사전 학습 (Pre-training)** : 인터넷의 모든 텍스트를 읽으며 "문장의 빈칸 맞추기" 같은 자기지도학습 문제를 풉니다. 이 과정을 통해 범용적인 언어 능력을 갖춘 '기초 모델'이 탄생합니다.
    2.  **파인튜닝 (Fine-tuning)** : 이 기초 모델을, 우리가 풀고 싶은 특정 문제(예: 영화 리뷰 감성 분석)의 소규모 정답 데이터로 추가 학습시켜 '전문가 모델'로 만듭니다.
- **결과**: **거대 언어 모델(LLM, Large Language Model)**  의 시대가 열렸습니다. 파라미터의 수를 수천억 개까지 늘린 LLM은 사전 학습만으로도 놀라운 언어 능력을 보여주기 시작했습니다.

## 2. LLM의 게임 체인저: In-context Learning

LLM의 규모가 커지면서, 파인튜닝 없이도 새로운 작업을 수행하는 놀라운 능력이 발견되었습니다. 이를 **In-context Learning (문맥 내 학습)**  이라고 합니다.

- **핵심**: LLM은 방대한 사전 학습을 통해 이미 세상의 수많은 '문제 유형'과 '패턴'을 학습했습니다. 따라서 우리가 프롬프트에 몇 개의 예시만 보여주면, "아, 지금 나에게 이런 종류의 작업을 원하는구나!"라고 스스로 패턴을 파악하고, 추가적인 가중치 업데이트(학습) 없이도 해당 작업을 수행할 수 있습니다.

### Zero-shot, One-shot, Few-shot Learning

- **Zero-shot**: 예시 없이, 작업에 대한 설명만으로 결과를 요청.
  > "Translate English to French. sea otter =>"
- **One-shot**: 하나의 예시를 제공.
  > "Translate English to French. sea otter => loutre de mer. cheese =>"
- **Few-shot**: 여러 개의 예시를 제공. (가장 효과적)
  > "Translate English to French. sea otter => loutre de mer. cheese => fromage. dog =>"

## 3. LLM 활용의 두 가지 길: 파인튜닝 vs. 프롬프트 엔지니어링

LLM을 특정 목적에 맞게 사용하는 방법은 크게 두 가지입니다.

| 구분 | 파인튜닝 (Fine-tuning) | 프롬프트 엔지니어링 (Prompt Engineering) |
| :--- | :--- | :--- |
| **개념** | 모델의 **가중치를 직접 업데이트**하여 특정 작업에 대한 전문가로 만듦 | 모델의 가중치는 그대로 두고, **입력(프롬프트)을 잘 설계**하여 원하는 결과를 유도 |
| **비용** | GPU 등 많은 컴퓨팅 자원과 시간이 필요 | 거의 비용이 들지 않음 |
| **데이터** | 해당 작업에 맞는 수백~수천 개의 정답 데이터 필요 | 예시 데이터가 거의 필요 없음 (Few-shot) |
| **적합한 상황** | 특정 도메인(법률, 의료)에 고도로 특화된 모델이 필요할 때 | 빠르고 다양한 작업을 실험하거나, 범용적인 작업이 필요할 때 |

## 4. LLM의 주요 활용 분야

- **대화형 AI (챗봇)** : ChatGPT, Gemini 등
- **코드 생성 및 보조**: GitHub Copilot 등
- **업무 자동화**: 회의록 요약, 이메일 초안 작성, 보고서 생성 (Notion AI 등)
- **창작 활동**: 소설, 시, 대본, 가사 등 창의적인 텍스트 생성
