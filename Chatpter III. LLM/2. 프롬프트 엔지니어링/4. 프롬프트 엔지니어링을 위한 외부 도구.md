# 4. LLM의 한계 극복하기: LangChain으로 외부 세계와 연결

## 목차
- [4. LLM의 한계 극복하기: LangChain으로 외부 세계와 연결](#4-llm의-한계-극복하기-langchain으로-외부-세계와-연결)
  - [목차](#목차)
  - [1. LLM의 근본적인 한계](#1-llm의-근본적인-한계)
  - [2. 한계 극복의 열쇠: LangChain 프레임워크](#2-한계-극복의-열쇠-langchain-프레임워크)
    - [LangChain의 핵심 철학: LLM을 부품처럼 조립하기](#langchain의-핵심-철학-llm을-부품처럼-조립하기)
  - [3. 실전 1: LLM에게 최신 정보 알려주기 (검색 에이전트)](#3-실전-1-llm에게-최신-정보-알려주기-검색-에이전트)
  - [4. 실전 2: LLM에게 파일 읽고 요약시키기 (문서 분석)](#4-실전-2-llm에게-파일-읽고-요약시키기-문서-분석)
  - [5. 실전 3: LLM에게 계산 능력 부여하기 (계산기 에이전트)](#5-실전-3-llm에게-계산-능력-부여하기-계산기-에이전트)
  - [6. 미래를 향한 표준: Model Context Protocol (MCP)](#6-미래를-향한-표준-model-context-protocol-mcp)
    - [MCP의 핵심 구조](#mcp의-핵심-구조)
    - [개념적 작동 예시](#개념적-작동-예시)

---

## 1. LLM의 근본적인 한계

LLM은 그 자체로 매우 강력하지만, 다음과 같은 명백한 한계를 가집니다.
- **최신성 부족**: 특정 시점까지의 데이터로만 학습되어, 최신 정보를 알지 못합니다.
- **사실 왜곡 (할루시네이션)** : 학습 데이터에 기반하여 그럴듯한 거짓말을 만들어낼 수 있습니다.
- **외부 세계와의 단절**: 웹 검색, 데이터베이스 조회, 다른 프로그램 실행 등 외부 세계와 상호작용할 수 없습니다.
- **긴 맥락 처리의 어려움**: 한 번에 처리할 수 있는 입력 토큰의 양에 제한이 있어, 매우 긴 문서를 한 번에 이해하기 어렵습니다.

## 2. 한계 극복의 열쇠: LangChain 프레임워크

**LangChain**은 이러한 LLM의 한계를 극복하기 위해, LLM을 **외부 데이터 소스 및 다른 프로그램(도구)**  과 쉽게 연결하고 조합할 수 있도록 도와주는 오픈소스 프레임워크입니다.

### LangChain의 핵심 철학: LLM을 부품처럼 조립하기

LangChain은 LLM을 단독으로 사용하는 것이 아니라, 다양한 기능(검색, 계산, DB 조회 등)을 가진 '부품'들과 **'체인(Chain)'** 또는 **'에이전트(Agent)'** 라는 형태로 조립하여 더 복잡하고 강력한 애플리케이션을 만들 수 있게 합니다.

- **체인 (Chain)** : LLM과 다른 구성 요소를 순차적으로 연결하여 특정 작업을 수행합니다. (예: 문서 로드 -> 텍스트 분할 -> 요약 -> 출력)
- **에이전트 (Agent)** : LLM이 스스로 **[생각 -> 행동 -> 관찰]** 사이클을 돌며, 어떤 도구를 사용해야 할지 직접 결정하고 작업을 수행합니다. (ReAct 프레임워크 기반)

## 3. 실전 1: LLM에게 최신 정보 알려주기 (검색 에이전트)

LLM에게 웹 검색 능력을 부여하여, 최신 정보에 기반한 답변을 생성하게 만들어 봅시다.

```python
# !pip install langchain langchain-openai google-search-results
import os
from langchain_openai import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

# API 키 설정 (실제 키를 입력해야 합니다)
# os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"
# os.environ["SERPAPI_API_KEY"] = "YOUR_SERPAPI_API_KEY"

# LLM과 도구 로드
llm = OpenAI(temperature=0)
tools = load_tools(["serpapi"], llm=llm)

# 에이전트 초기화
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# 에이전트 실행
question = "2024년 UEFA 챔피언스리그 우승팀은 어디야?"
agent.run(question)
```
> **작동 원리**: 에이전트는 질문을 받고, `serpapi`(구글 검색 도구)를 사용해야겠다고 '생각'합니다. 검색 결과를 '관찰'한 뒤, 그 정보를 바탕으로 최종 답변을 생성합니다.

## 4. 실전 2: LLM에게 파일 읽고 요약시키기 (문서 분석)

LLM의 토큰 제한을 극복하고 긴 문서를 요약하기 위해, LangChain의 **요약 체인(Summarization Chain)**  을 사용합니다.

```python
# !pip install pypdf
from langchain_openai import OpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import PyPDFLoader

# PDF 파일 로드 및 분할
loader = PyPDFLoader("example.pdf") # 요약할 PDF 파일
docs = loader.load_and_split()

# LLM 및 요약 체인 초기화
llm = OpenAI(temperature=0)
# map_reduce: 문서를 여러 청크로 나누어 각각 요약한 뒤, 그 요약본들을 다시 합쳐 최종 요약
chain = load_summarize_chain(llm, chain_type="map_reduce")

# 요약 실행
summary = chain.run(docs)
print(summary)
```

## 5. 실전 3: LLM에게 계산 능력 부여하기 (계산기 에이전트)

LLM은 복잡한 수학 계산에 약합니다. 검색 능력과 함께 계산기 도구를 부여하여, 사실 확인과 계산을 모두 수행하는 에이전트를 만들어 봅시다.

```python
# LLM, 검색 도구, 계산기 도구 로드
llm = OpenAI(temperature=0)
tools = load_tools(["serpapi", "llm-math"], llm=llm)

# 에이전트 초기화
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# 에이전트 실행
question = "테슬라의 현재 주가를 52주 최고가로 나눈 값은 얼마야?"
agent.run(question)
```
> **작동 원리**: 에이전트는 먼저 `serpapi`로 '테슬라 현재 주가'와 '52주 최고가'를 검색합니다. 검색된 두 숫자를 '관찰'한 뒤, 이 두 숫자를 나누기 위해 `llm-math`(계산기) 도구를 사용해야겠다고 '생각'하고, 최종 계산 결과를 답변으로 내놓습니다.

---

## 6. 미래를 향한 표준: Model Context Protocol (MCP)


> `LangChain`과 같은 프레임워크는 LLM의 한계를 극복하는 강력한 '해결책'입니다. 하지만 이는 특정 언어(Python)와 프레임워크에 종속적이죠. 만약 어떤 LLM이든, 어떤 도구든, 어떤 플랫폼에서든 마치 USB-C 포트처럼 그냥 꽂기만 하면 작동하게 할 수는 없을까요? 이 질문에 대한 답이 바로 **Model Context Protocol (MCP)** 입니다.

MCP는 LLM, 도구, 데이터 소스 간의 상호작용을 위한 **개방형 표준 프로토콜**입니다. 현재 LLM 제조사마다, 도구 개발사마다 제각각인 연결 방식을 통일하여, AI 생태계의 파편화를 해결하는 것을 목표로 합니다.

- **`LangChain` vs. `MCP`**:
    - `LangChain`이 특정 문제를 해결하기 위한 '애플리케이션 프레임워크'라면,
    - `MCP`는 웹의 HTTP처럼, 서로 다른 시스템이 소통하기 위한 근본적인 '통신 규약'에 가깝습니다.

### MCP의 핵심 구조

MCP는 클라이언트-서버 모델을 기반으로 작동합니다.

1.  **MCP 서버**: 외부 도구(API, 데이터베이스 등)를 감싸고, 자신이 제공할 수 있는 기능 목록을 표준화된 방식으로 외부에 알립니다. 이 기능들은 크게 세 가지로 나뉩니다.
    - **Tools**: LLM이 실행할 수 있는 함수 (예: `get_current_weather(city: str)`)
    - **Resources**: LLM이 참조할 수 있는 읽기 전용 데이터 (예: `company_product_database.csv`)
    - **Prompts**: 재사용 가능한 프롬프트 템플릿 (예: `summarize_text(text: str)`)

2.  **MCP 클라이언트 (LLM 에이전트)**: MCP 서버에 어떤 기능들이 있는지 동적으로 파악하고, 정해진 규약에 따라 해당 기능을 요청하여 결과를 받아옵니다.

### 개념적 작동 예시

LLM 에이전트가 '서울 날씨'를 알고 싶을 때, MCP를 사용하면 다음과 같이 상호작용합니다.

1.  **[클라이언트 -> 서버] 기능 탐색**: "날씨 관련 `Tool`이 있습니까?"
2.  **[서버 -> 클라이언트] 기능 명세 응답**: "`get_current_weather(city: str)`라는 `Tool`이 있습니다. `city`는 문자열이어야 합니다."
3.  **[클라이언트 -> 서버] 기능 실행 요청**: `{"tool": "get_current_weather", "parameters": {"city": "서울"}}`
4.  **[서버 -> 클라이언트] 결과 반환**: `{"result": {"temperature": 25, "condition": "맑음"}}`

이처럼, LLM은 날씨 API의 상세한 사용법을 미리 알 필요 없이, MCP라는 공통 언어를 통해 동적으로 상호작용할 수 있습니다.

> **Insight:**
>
> MCP의 등장은 프롬프트 엔지니어링이 '어떻게 질문할까'의 문제를 넘어, '어떻게 시스템을 안정적으로 연결할까'라는 **AI 아키텍처 설계의 영역**으로 확장되고 있음을 보여주는 중요한 신호입니다. 이는 LLM을 단순한 챗봇이 아닌, 예측 가능하고 재사용 가능한 '소프트웨어 컴포넌트'로 취급하려는 움직임입니다. MCP가 널리 채택된다면, 우리는 훨씬 더 모듈화되고 확장성 높은 AI 애플리케이션을 구축할 수 있게 될 것입니다.